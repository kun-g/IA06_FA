web-scraper-order,web-scraper-start-url,citation,bib,left,dl_link,dl_link-href,name,extra
"1573179748-705","https://amturing.acm.org/award_winners/goldwasser_8627889.cfm","Along with Silvio Micali, for transformative work that laid the complexity-theoretic foundations for the science of cryptography, and in the process pioneered new methods for efficient verification of mathematical proofs in complexity theory.","<p>Shafi Goldwasser has made fundamental contributions to cryptography, computational complexity, computational number theory and probabilistic algorithms.&nbsp; Her career includes many landmark papers which have initiated entire subfields of computer science.&nbsp; These include creating the theoretical foundations of modern cryptography, the introduction of zero-knowledge interactive proofs, the introduction of multi-prover proofs (later known as probabilistically checkable proofs), discovering the connection between probabilistically checkable proofs and the intractability of approximation problems, showing how to use the theory of elliptic curves to distinguish primes from composites, and launching combinatorial property testing.</p>
<p>Shafi was born in 1959 in New York City. Her parents were Israeli, and her joint American/Israeli citizenship presaged the two countries that would play such an important role in her research. Her family returned to Israel where Shafi attended grade school in Tel Aviv. In high school she was especially interested in physics, mathematics and literature. After her schooling she returned to the U.S. and became an undergraduate in the mathematics department at Carnegie Mellon University. Soon, however, she became interested in programming (which she had never done before) and computer science. One computer science course that she especially remembers, taught by Jon Bentley, was an algorithms and discrete math course that she loved. She also worked on the CM* project at CMU, a 50-processor multiprocessor system. Shafi next had a summer internship at the RAND Corporation in Los Angeles. She loved living on Venice Beach, but her seduction by California wasn’t complete until she drove up the coast road one weekend and entered Berkeley for the first time.</p>
<p>Shafi enrolled in graduate school in Computer Science at the University of California, Berkeley, without knowing what she wanted to study. Her master's work was with Michael Powell and David Patterson, studying the optimal instruction set for the RISC architecture. But she soon met a group of enthusiastic young theoretical computer scientists – including Eric Bach, Faith Ellen, Mike Luby, Jeff Shallit, Vijay Vazirani and her Turing Award co-recipient <a href=""/award_winners/micali_0557920.cfm"">Silvio Micali</a> – and she began to see that her interests lay in theoretical areas. What closed the deal for her was a number theory course by another Turing Award recipient, <a href=""/award_winners/blum_4659082.cfm"">Manuel Blum</a>. Subjects such as primality testing, quadratic residues, quadratic non-residues, RSA, and coin-tossing really excited her. She happily attended the first Crypto conference in Santa Barbara and met the three authors of the RSA cryptographic system, themselves later Turing Award recipients: <a href=""/award_winners/rivest_1403005.cfm"">Ron Rivest</a>,&nbsp;<a href=""/award_winners/shamir_2327856.cfm"">Adi Shamir</a> and <a href=""/award_winners/adleman_7308544.cfm"">Len Adleman</a>.</p>
<p>The first problem Shafi began working on with Micali was how to hide partial information in “mental poker”. Their solution [<a href=""/bib/goldwasser_8627889.cfm#bib_1"">1</a>] was an essentially perfect way of encrypting a single bit (against a computationally limited adversary), and they invented a “hybrid” technique to show that independently encrypting individual bits causes the whole message to be secure. In their example, encryption security was <em>provably</em> based on a <a href=""https://en.wikipedia.org/wiki/Goldwasser%E2%80%93Micali_cryptosystem"" target=""_blank"">quadratic residuosity assumption</a>. They were the first to give a rigorous definition of <a href=""https://en.wikipedia.org/wiki/Semantic_security"">semantic security</a> for a public-key encryption system, and showed that it was equivalent to a number of other intuitive formulations of security. Julius Caesar may have used cryptography, but now we were finally beginning to understand it.</p>
<p>Upon graduating from Berkeley in 1984, Shafi went to the Massachusetts Institute of Technology, first as a postdoc, and then as a faculty member. She became the RSA Professor of Electrical Engineering and Computer Science in 1997. In 1992 she began a parallel career as a Professor of Computer Science and Applied Mathematics at the Weizmann Institute of Science in Israel. Shafi, with her husband and computer scientist Nir Shavit and their two sons, somehow divide their time between the two institutes, spending about three years at a time in each country.</p>
<p>It was an exciting time when Shafi came to M.I.T. She joined a group with similar research interests: Micali had arrived, and Benny Chor, Oded Goldreich, Ron Rivest and Mike Sipser were there. With Goldreich and Micali [<a href=""/bib/goldwasser_8627889.cfm#bib_4"">4</a>], Shafi investigated whether the notion of a <a href=""https://en.wikipedia.org/wiki/Pseudorandom_number_generator"" target=""_blank"">pseudorandom number generator</a> could be generalized so that one could generate <em>exponentially</em> many bits (or equivalently, a function) pseudorandomly. What would it even mean to do this? This definition was in itself important, and it is why we understand today what it means for a <a href=""https://en.wikipedia.org/wiki/Block_cipher"" target=""_blank"">block cipher</a> such as <a href=""https://en.wikipedia.org/wiki/Advanced_Encryption_Standard"" target=""_blank"">AES</a> to be secure. They also showed how to provably transform a pseudorandom number generator into a pseudorandom function generator. These ideas had applications to the (then) new field of Learning Theory, providing examples of things that cannot be learned.</p>
<p>Shafi, with Micali (and later Rackoff) [<a href=""https://amturing.acm.org/bib/goldwasser_8627889.cfm#bib_6"">6</a>], had been thinking for a while about expanding the traditional notion of “proof” to an interactive process in which&nbsp; a ""prover"" can convince a probabilistic ""verifier"" of the correctness of&nbsp; a mathematical proposition with overwhelming probability if and only if the proposition is correct. They called this interactive process an ""interactive proof"" (a name suggested by Mike Sipser). They wondered if one could &nbsp;prove some non-trivial statement (for example, membership of a string in a hard language) without giving away any knowledge whatsoever about why it was true. They defined that the verifier receives no knowledge&nbsp; from the prover if the verifier&nbsp; could simulate on his own the probability distribution that he obtains in interacting with the prover.The idea that “no knowledge” means simulatability was a very important contribution. They also gave the first example of these “zero knowledge interactive proofs” using quadratic residuosity. This paper won the first&nbsp;<a href=""https://en.wikipedia.org/wiki/G%C3%B6del_Prize"" target=""_blank"">ACM SIGACT Gödel Prize</a>. This zero-knowledge work led to a huge research program in the community that continues to this day, including results showing that (subject to an&nbsp;<a href=""https://amturing.acm.org/info/goldwasser_8627889.cfm"" target=""_blank"">assumption</a>&nbsp;such as the existence of one-way functions) a group of distrusting parties can compute a function of all their inputs without learning any knowledge about other people’s inputs beyond that which follows from the value of the function.</p>
<p>Sharing the Gödel Prize was a paper by László Babai and Shlomo Moran that gave a different notion of interactive proof, where the randomness of the verifier is only from <a href=""https://en.wikipedia.org/wiki/Interactive_proof_system"" target=""_blank"">public coins</a>. An example in Shafi's paper on zero knowledge clearly seemed to require private coins, but Shafi and Michael Sipser [<a href=""/bib/goldwasser_8627889.cfm#bib_7"">7</a>] later proved that the two notions are equivalent. This involved using public coins to do interactive proofs showing lower bounds on the sizes of sets.</p>
<p>Around this time, Shafi returned to her love of number theory. After hearing a talk by René Schoof about counting the number of points on elliptic curves, she and Joe Kilian [<a href=""/bib/goldwasser_8627889.cfm#bib_3"">3</a>] showed that for most primes, it is possible to use elliptic curves to construct a normal, non-interactive proof that the number is indeed prime. This meant that when a “prime” is chosen for a cryptographic algorithm such as <a href=""https://en.wikipedia.org/wiki/RSA_%28algorithm%29"" target=""_blank"">RSA</a>, one can be absolutely certain that the number really is prime. (It was only much later that we learned of a polynomial-time algorithm for primality testing.)</p>
<p>Shafi then started asking a number of questions concerning what kinds of security can be achieved without computational complexity assumptions. This led to a model for multi-party computation where, instead of an assumption, one changes the physical model so that every pair of parties has a secure channel between them. Shafi, with Michael Ben-Or and Avi Wigderson, showed [<a href=""/bib/goldwasser_8627889.cfm#bib_8"">8</a>] that with sufficiently many honest parties, function evaluation in this setting can be done securely. The construction uses a form of algebraic “verifiable secret sharing”, a variant on an idea first proposed [<a href=""/bib/goldwasser_8627889.cfm#bib_2"">2</a>] by Goldwasser, Baruch Awerbuch, Benny Chor, and Micali.</p>
<p>Another outcome of this research was a variant of interactive proofs where the prover is replaced by two or more provers who cannot talk with each other. Shafi, with Ben-Or, Kilian and Wigderson, showed [<a href=""/bib/goldwasser_8627889.cfm#bib_9"">9</a>] that two provers are sufficient, and that all of NP can be proven with zero knowledge in this model without any assumptions. This inspired many extremely important results by other people. We can't explain all that development here, so we will cut to Shafi's next big contribution.</p>
<p>Shafi, with Feige, Lovasz, Safra and Szegedy, by examining the power of multi-prover proofs, discovered [<a href=""/bib/goldwasser_8627889.cfm#bib_10"">10</a>] that the existence of these proofs (with certain parameters) implies a hardness of approximation result for certain NP-complete languages. Specifically, they showed that if the size of a maximum clique in a graph can be approximated within a constant factor, then all of NP can be accepted in nearly polynomial time. This result inspired decades of results about PCPs (probabilistically checkable proofs, an alternative characterization of multi-prover proofs) and hardness of approximation. This paper earned Shafi her second Gödel Prize, shared with two papers that prove nearly optimal parameters for PCPs. One of the most important contributors to this area is Johan Håstad, who years earlier had been the very first of Shafi's many amazing graduate students.</p>
<p>With Mihir Bellare, Carsten Lund and Alexander Russell, Shafi produced [<a href=""/bib/goldwasser_8627889.cfm#bib_11"">11</a>] one of the first works showing how to fine-tune some of the PCP parameters, leading to improved results on hardness of approximation. The theme of approximation enters her work in a number of other ways as well. One computational problem, which quantum computers have not to date been able to attack and on which public-key cryptography can be based, is approximating the shortest vector size in an integer lattice.&nbsp; Shafi and Goldreich [<a href=""/bib/goldwasser_8627889.cfm#bib_12"">12</a>] showed an especially succinct interactive proof for this approximation problem, thus demonstrating it is unlikely to be NP-hard.</p>
<p>On the algorithmic side, with Goldreich and Dana Ron, Shafi introduced the subject of “property testing” for combinatorial properties [<a href=""/bib/goldwasser_8627889.cfm#bib_13"">13</a>].&nbsp; Given an object (such as a graph) for which either a given property holds or the object is far from any other object for which the property holds, we want to (probabilistically) determine which is the case by examining the object in only a small&nbsp; number of locations. In [<a href=""https://amturing.acm.org/bib/goldwasser_8627889.cfm#bib_13"">13</a>] property-testers are devised which need to examine only a constant number of edges in a graph for several NP-complete properties such as 3-coloring, max-cut, and other graph partition problems..</p>
<p>Interactive proofs also play a major role in her recent research about how a user can delegate computation to a very fast but untrusted “cloud” computer. This is one of the most important research areas in cryptography today. Shafi, with Yael Tauman Kalai and Guy Rothblum, introduced [<a href=""/bib/goldwasser_8627889.cfm#bib_15"">15</a>] one practical formulation of this question, and showed how to efficiently delegate the computation of small-depth functions.</p>
<p>Shafi has recently explored different models for how to achieve “code obfuscation”. For example, with Tauman Kalai and Rothblum she proposed [<a href=""https://amturing.acm.org/bib/goldwasser_8627889.cfm#bib_16"">16</a>] the model of ""one-time program"" which obfuscates a program so that it can be executed only for a prescribed number of executions, assuming a special kind of universal secure hardware.&nbsp; In recent work [<a href=""https://amturing.acm.org/bib/goldwasser_8627889.cfm#bib_18"">18</a>] with Tauman Kalai, Vinod Vaikuntanathan, Raluca Ada Popa, and Nickolai Zeldovich on “functional encryption,” Shafi introduced yet another new paradigm for general function obfuscation called “token-based obfuscation.""</p>
<p>Another recent area of research [<a href=""/bib/goldwasser_8627889.cfm#bib_17"">17</a>] is protection against “side-channel attacks”, where an adversary is able to get information (for example, by measuring processor power consumption) that is not part of the stream of bits specified by a protocol. Shafi, with Adi Akavia and Vaikuntanathan, had the first results showing how to do public-key encryption in a way that remains secure even if the secret memory containing the secret key is partially leaked. This was the beginning of an intensive research effort by the cryptographic community to define and achieve leakage resilience for cryptographic primitives and protocols.</p>
<p>Two other interesting facts about Shafi: Since her husband Nir Shavit has also won a Gödel Prize, her household total of three may be a record. And recently, Shafi has become a fan and practitioner of “<a href=""https://en.wikipedia.org/wiki/Playback_Theatre"">Playback Theater</a>”, an improvisational interactive group experience.</p>
<p>&nbsp;</p>
<p align=""right""><span class=""callout"">Author: Charles Rackoff</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/goldwasser_8627889.cfm""><img src=""/images/lg_aw/8627889.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Shafi Goldwasser ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/goldwasser_8627889.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6>BIRTH:</h6>
<p>1959, New York City</p>
<h6>EDUCATION:</h6>
<p>B.S., Department of Mathematics, Carnegie Mellon University (1979); M.S., Department of Electrical Engineering and Computer Science, University of California at Berkeley (1981); Ph.D., Department of Electrical Engineering and Computer Science, University of California at Berkeley (1984).</p>
<h6>EXPERIENCE:</h6>
<p>Bantrel Postdoctoral Fellowship, Massachusetts Institute of Technology (1983); Assistant Professor, Massachusetts Institute of Technology (1983-1987); Associate Professor, Massachusetts Institute of Technology (1987-1992); Professor of Electrical Engineering and Computer Science, Massachusetts Institute of Technology (1992-present); Professor of Computer Science and Applied Mathematics, Weizmann Institute of Science (1993-present); Co-Leader of the Cryptography and Information Security Group, Massachusetts Institute of Technology (1995-present); RSA Professor of Electrical Engineering and Computer Science, Massachusetts Institute of Technology (1997-present).</p>
<h6>HONORS AND AWARDS:</h6>
<p>IBM Young Faculty Development Award (1983-1985); NSF Presidential Young Investigator Award (1987-1992); NSF Award for Women in Science (1991-1996); Co-winner, SIGACT Gödel Prize (1993); ACM Grace Murray Hopper Award (1996); RSA Award in Mathematics for Outstanding Mathematical Contributions to Cryptography (1998); Weizmann Institute Levenson Prize in Mathematics (1999); Co-winner, SIGACT Gödel Prize (2001); Fellow, American Academy of Arts and Science (2001); Fellow, National Academy of Sciences (2004); Fellow, National Academy of Engineering (2005); Distinguished Alumnus Award in Computer Science and Engineering, University of California, Berkeley (2006); &nbsp;Athena Lecturer, Association for Computing Machinery’s Committee on Women in Computing (2008); Franklin Institute Benjamin Franklin Medal in Computer and Cognitive Science (2010); IEEE Emanuel R. Piore Award (2011);&nbsp;<span style=""line-height: 20.8px;"">Fellow,&nbsp;</span>IACR<span style=""line-height: 20.8px;"">&nbsp;(2012).</span></p>
<h6><a href=""https://awards.acm.org/press_releases/turing_award_2012.pdf"" target=""_blank"">PRESS RELEASE</a></h6>","","https://dl.acm.org/author_page.cfm?id=81100237195","Shafi Goldwasser","<li class=""bibliography""><a href=""/bib/goldwasser_8627889.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/goldwasser_8627889.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/goldwasser_8627889.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/goldwasser_8627889.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/goldwasser_8627889.cfm""><span></span>Video Interview</a></li>"
"1573179624-696","https://amturing.acm.org/award_winners/bachman_9385610.cfm","For his outstanding contributions to database technology.","<p>By creating the Integrated Data Store (IDS), and advocating forcefully for the concepts behind it, Charles W. Bachman was very influential in the creation of the data base management system as we know it today. During a long and varied career he ran a chemical plant, created cost capital accounting systems, headed an early data processing group, pioneered the application of computers to manufacturing control, led efforts to standardize database and computer communication concepts, won the highest honor in computer science and founded a publicly traded company.</p>
<p>Bachman told me [<a href=""/bib/bachman_9385610.cfm#bib_7"">7</a>] that when he thought about career options as a boy, “I was never interested in anything else. I was always going to be an engineer.” The Second World War temporarily intervened, but in 1950 his dream came true when engineering degrees from Michigan State and the University of Pennsylvania earned him a job with Dow Chemical. An interest in management and cost accounting methods exposed him to punched card machines, and in 1957 he became the founding head of Dow’s corporate data processing department. His work there, and his exposure to other computer center leaders through IBM’s <a href=""https://en.wikipedia.org/wiki/SHARE_%28computing%29"" target=""_blank"">SHARE</a> user group, changed the course of his career.</p>
<p>Business problems led Dow to cancel its computer order prior to delivery. Frustrated, Bachman left Dow in 1960 to work for General Electric, then the textbook example of a well-managed, diversified technology company. He was part of a team responsible for experimenting with new approaches such as operations research, simulation, forecasting, and automation, for possible application across the firm’s many different business units. During the 1950s hundreds of American businesses had rushed to order computers. There was a lot of hype about potential benefits, but getting the machines to do anything useful was much harder than expected. They often ended up being used only to automate narrow clerical tasks like payroll or billing. By 1960 management experts realized that to justify the huge personnel and hardware costs of computerization, companies would need to use computers to tie together business processes such as sales, accounting, and inventory so that managers would have access to integrated, up-to-date information. This was the great dream of corporate computing in the 1960s.</p>
<p>Various firms tried to build such “totally integrated management information systems”, but the hardware and software of the era made that difficult. Each business process ran separately, with its own data files stored on magnetic tape. A small change to one program might mean rewriting related programs across the company. But business needs change constantly, so integration never got very far.</p>
<p>At General Electric Bachman was working on just such as “integrated systems” project, and produced the Manufacturing Information and Control System (MIACS) for a GE manufacturing plant in Philadelphia. Using one of the first available disk drives, his team solved the fundamental problems that defeated so many others.</p>
<p>The crucial invention, operational by 1963, was Bachman’s Integrated Data Store or IDS. IDS maintained a single set of shared files on disk, together with the tools to structure and maintain them. Programs responsible for particular tasks, such as billing or inventory updates, retrieved and updated these files by sending requests to IDS. IDS provided application programmers with a set of powerful commands to manipulate data, an early expression of what would soon be called a Data Manipulation Language.</p>
<p>This made programmers much more productive, because they did not have to grapple with the daunting complexity of working with the “random access” disk storage devices. It also meant that the files could be restructured, moved or expanded without rewriting all the programs that accessed them. IDS maintained a separate data dictionary, tracking information on the different kinds of records in the system and their relationships—for example between customers and the orders they had placed. This was a crucial step towards the integration of different kinds of data, which in turn was vital to the integration of business processes and the establishment of the computer as a managerial tool. Only the boldness of Bachman’s IDS design, and the remarkable efficiency with which he squeezed IDS and the MIACS applications into a computer with the equivalent of 40 Kbytes of memory, made this possible. This tight coupling meant that IDS and the Problem Controller, a transaction-oriented operating system produced by the same team, almost entirely replaced General Electric’s earlier rudimentary system software. It was years before any other program matched the power and flexibility of IDS.</p>
<p>By the end of the 1960s the “data base management system,” as programs such as IDS were being called, was one of the most important areas of business computing research and development. When the packaged mainframe software industry boomed during the 1970s, data base management systems were its most important product category. Bachman played an important role in this process, as an early chair and active member of the Database Task Group established by the computer trade association <a href=""https://en.wikipedia.org/wiki/CODASYL"" target=""_blank"">CODASYL</a> (best known as the creator of the COBOL language for business programming) to standardize concepts, terminology and technologies in this area. His design for IDS, and formulation of the underlying concept of the network data model, were the most important influences on the group’s final work.</p>
<p>In 1973 Bachman became the eighth person to win the ACM Turing Award. At that time computer science was a young discipline, and its leaders were struggling to establish it as a respectable academic field with its own areas of theory, rather than as just a technical tool needed to support the work of real scientists such as physicists. So the awards tended to go to brilliant theorists working in prestigious universities. Bachman was the first Turing Award winner without a Ph.D., the first to be trained in engineering rather than science, the first to win for the application of computers to business administration, the first to win for a specific piece of software, and the first who would spend his whole career in industry. Bachman does not believe the award had much impact on his subsequent career progress, although it caused him to develop an interest in Alan Turing’s life and even to visit <a href=""/photo/bachman_9385610.cfm#ph_1"">Sara Turing</a>, Alan Turing’s elderly mother, then in an English nursing home.</p>
<p>His award acceptance lecture, “The Programmer as Navigator,” was an influential declaration of a new world in which complex data structures provided the framework for corporate computing systems, around whose topographies individual application programs would navigate. The award cemented Bachman’s position within the industry as a leading expert on Database Management Systems (DBMS) and the most respected advocate for the network data model and the systems influenced by the CODASYL approach that were rapidly gaining market share in the mainframe world during the mid-1970s.</p>
<p>As such, he stood in opposition to the ideas of <a href=""/award_winners/codd_1000892.cfm"">Edgar F. (“Ted”) Codd</a>, a mathematically inclined IBM research scientist whose relational model for database manipulation had attracted a growing band of supporters and was beginning to legitimize database systems as a theoretically respectable research field within computer science. A debate between the two and their supporters, held at an ACM workshop in 1974, is remembered among database researchers as a milestone in the development of their field. Bachman stood for engineering pragmatism and proven high performance technology, while Codd personified scientific rigor and elegant but unproven theory. Their debate was inconclusive, which was perhaps inevitable given that no practical relational systems had yet been produced.</p>
<p>Bachman’s influence on today’s data base management systems is unmistakable, even though data base management systems based on Codd’s relational approach, such as Oracle, DB2, and SQL Server, had eclipsed CODASYL systems by the end of the 1990s. Modern relational systems continue to follow the basic template for the data base management system invented by Bachman and his colleagues in CODASYL: a complex piece of software managing data storage, enforcing access restrictions, providing interfaces for both application programs and ad-hoc queries, and providing different views on the same data to different users. Although the rigid connections between different kinds of records that Bachman favored are not part of the relational model itself, pragmatism has pushed modern systems to support them in the form of <a href=""https://en.wikipedia.org/wiki/Integrity_constraints"" target=""_blank"">referential integrity constraints</a>.</p>
<p>Today’s database designers rely on graphical tools to depict the relationships between different tables as a web of connections. These “data structure diagrams” are the direct descendants of a technique Bachman devised [<a href=""/bib/bachman_9385610.cfm#bib_3"">3</a>] to illustrate the complex data structures required for the adaptive manufacturing control logic of MIACS.</p>
<p>Bachman helped guide two highly influential committees during the 1970s. The Study Group – Database Systems was set up in 1972 by the Systems Planning and Resources Committee (SPARC) of the American National Standards Institute (ANSI). Its distinction between conceptual, internal, and external schemas was widely adopted. As chair of the Open Systems Interconnection (OSI) subcommittee of the International Organization for Standardization (ISO) from 1978 to 1982, Bachman helped to formulate the seven layer model of computer communications that has provided the basic framework for discussion of network protocols ever since.</p>
<p>In 1983 Bachman founded his own company, Bachman Information Systems. It followed the classic path of a technology startup, winning funding from leading venture capital firms and making an initial public offering. Its products served the corporate market for data modeling and software engineering. He retired in 1996, but remained active as a consultant specializing in the design of database schemas.</p>
<p>Through all of this he retained an engineer’s zest for the elegant solution of difficult problems, and faith in the power of careful analysis and a systems approach to make the world a better place. As he wrote in a note at the end of our oral history transcript, “My work has been my play.”[<a href=""/bib/bachman_9385610.cfm#bib_7"">7</a>]</p>
<p style=""text-align: right;""><span class=""callout"">Author: Thomas Haigh</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/bachman_9385610.cfm""><img src=""/images/lg_aw/9385610.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Charles W Bachman""></a>
</div>
<h5><a href=""/photo/bachman_9385610.cfm""><img alt="""" src=""/images/misc/bhlight.jpg"" style=""float: left;""></a>&nbsp; <a href=""/photo/bachman_9385610.cfm"">Photo-Essay</a></h5>
<h5>&nbsp;</h5>

<h6><span class=""label"">BIRTH: </span></h6>
<p>December 11, 1924 in Manhattan, Kansas, U.S.A.</p>
<p><span style=""color: rgb(230, 126, 41); font-size: 10.72px; font-weight: bold;"">DEATH:</span></p>
<p>July 13, 2017</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>B.Sc., Mechanical Engineering (Michigan State, 1948); M.S. Mechanical Engineering, (University of Pennsylvania, 1950).</p>
<h6><span class=""label"">EMPLOYMENT: </span></h6>
<p>Dow Chemical Corporation, Midland, MI (1950-1955, assignments in Engineering, Finance, Manufacturing and systems work; Manager of Corporate Data Processing 1956-1959; Manager of Data Processing Research, 1960); General Electric (Integrated Systems Project in Production Control Services, NY, 1960-1964; Computer Department, Phoenix, AZ 1964-1970--titles included Manager of Software Product Planning, Manager of Applied Technology Subsection, and Manager of Data Management Software for the Advanced System Division);&nbsp; Honeywell Information Systems, Boston, MA (Chief Staff Engineer, 1970-1980); Cullinane Database Systems, Inc. (1980-83); Bachman Information Systems (President &amp; CEO 1983-1988; Chairman; 1988-1996); Cayenne Software, Inc. (President, 1996-1997); Consultant (Constellar 1997-1999; Cbr Systems, Inc.: 2002-2006; InfiniteIQ: 2009–2010).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>ACM Turing Award (1973); Distinguished Fellow of the British Computer Society (1977).</p>","","https://dl.acm.org/author_page.cfm?id=81100243333","Charles William Bachman","<li class=""bibliography""><a href=""/bib/bachman_9385610.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283928&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/bachman_9385610.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/bachman_9385610.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/bachman_9385610.cfm""><span></span>Video Interview</a></li>
<li class=""additional""><a href=""//dl.acm.org/citation.cfm?id=1141882"" target=""_blank""><span></span>Oral History Interview</a></li>"
"1573179309-676","https://amturing.acm.org/award_winners/cook_n991950.cfm","For his advancement of our understanding of the complexity of computation in a significant and profound way. His seminal paper, ""The Complexity of Theorem Proving Procedures,"" presented at the 1971 ACM SIGACT Symposium on the Theory of Computing, laid the foundations for the theory of NP-Completeness. The ensuing exploration of the boundaries and nature of NP-complete class of problems has been one of the most active and important research activities in computer science for the last decade.","<p><strong>Stephen Arthur Cook was born on December 14, 1939 in Buffalo, NY.</strong> Cook’s father worked as a chemist for a subsidiary of Union Carbide, and was also an adjunct professor at SUNY Buffalo. His mother worked as a homemaker and also as an occasional English teacher at Erie Community College. When he was ten, Cook moved with his family to Clarence, NY, which was the home of Wilson Greatbatch, the inventor of the implantable pacemaker. As a teenager, Stephen developed an interest in electronics and worked forGreatcatch, who was then experimenting with transistor-based circuitry.</p>
<p>Cook entered the University of Michigan in 1957, majoring in science engineering. He was introduced to computer programming in a freshman course taught by Bernard Galler. With a fellow student he wrote a program to test Goldbach’s conjecture that every even integer greater than two is the sum of two primes. Stephen eventually changed his major to mathematics, and received his Bachelor’s degree in 1961. He subsequently entered graduate studies in the Mathematics Department at Harvard University, where he encountered influences that would shape the direction of his future research, including Michael Rabin’s early work on computational complexity, Alan Cobham’s characterization of polynomial time computable functions, and his supervisor Hao Wang’s investigations in automated theorem proving. Cook received his Ph.D. in 1966. His thesis, titled <em>On the Minimum Computation time of Functions</em>, addresses the intrinsic computational complexity of multiplication. One contribution of the thesis was an improvement of <a href=""https://en.wikipedia.org/wiki/Toom%E2%80%93Cook_multiplication"" target=""_blank"">Andrei Toom’s multiplication algorithm</a>, which is now known as Toom-Cook multiplication. This algorithm is still a subject of study and is of practical importance in high-precision arithmetic.</p>
<p>After graduating, he joined the Mathematics Department at the University of California, Berkeley, leaving there in 1970 to take the position of Associate Professor in Computer Science at the University of Toronto. A year later, Cook presented his seminal paper, “The complexity of theorem proving procedures,” at the <em>3rd Annual ACM Symposium on Theory of Computing </em>[<a href=""/bib/cook_n991950.cfm#link_1"">1</a>]. This paper marked the introduction of the theory of NP-completeness, which henceforth occupied a central place in theoretical computer science. (Leonid Levin independently introduced NP-completeness in 1973.) It was also an early contribution to the theory of propositional proof complexity, an area in which Cook continued to do extensive research over the next 40 years.</p>
<p>The theory of NP-completeness provides a way to characterize the difficulty of computational problems with respect to the time, that is, the maximum number of computational steps required to solve a problem, as a function of input size. Cook’s paper addresses the fact that many problems which are difficult to solve in this sense have solutions which are easy to verify once they are found. In current terminology, problems which are easy to solve comprise the class P, while those which are easy to verify comprise the class NP. Cook showed that certain problems in NP, now known as NP-complete problems, are as hard to solve as any others in NP, in the sense that if any one of these problems is easy to solve, then all problems in NP are easy to solve. Cook’s paper also was the source of the celebrated and still unsolved P versus NP question, which asks whether there are indeed problems in NP which are not in P, that is, problems whose solutions are easily verified but are not easily solved. A simple example which elucidates the relationship between the classes P and NP is the popular Sudoku puzzle (see <a href=""/info/cook_n991950.cfm"">here</a>).</p>
<p>Cook’s paper had an immediate impact. In 1972 <a href=""/award_winners/karp_3256708.cfm"">Richard Karp&nbsp;</a>published a paper in which he showed that twenty-one problems in combinatorics, optimization and graph theory which were believed to be computationally difficult were indeed NP-complete. Seven years later, Michael Garey and David Johnson published the book <em>Computers and Intractability: A Guide to the Theory of NP-Completeness</em>, which included a compendium of over three hundred problems which by then had been shown to be NP-compete. It would be hard to estimate how many problems have now been proved to be NP-complete, but they certainly number in the thousands. More significantly, the use of NP-completeness as a tool to understand computational difficulty spans virtually all areas of computer science.</p>
<p>The 1970 paper (“The complexity of theorem proving procedures”) introduced concepts and techniques which have had a lasting impact in various fields of computer science. In the paper, Cook introduces a canonical NP-complete problem, the satisfiability problem for Boolean formulas (SAT). The study of the SAT problem has become a field in its own right, and the development and use of specialized programs known as SAT-solvers has become an important practical approach to problems in areas such as verification and circuit design. In order to show that other problems are NP-complete, Cook developed the method of resource-bounded reducibility. This technique is an essential tool in computational complexity theory and is the foundation of complexity-based approaches to cryptographic security.</p>
<p>The impact of the P versus NP problem has extended beyond the field of computer science. In particular, it has been recognized as a mathematical problem of fundamental significance. In the year 2000, Fields Medalist Steve Smale listed the P versus NP problem as the third in his list of eighteen unsolved problems in mathematics for the 21st century. In the same year, the Clay Mathematics Institute announced the <em>Millennium Prize Problems</em>. In the words of the Institute, the prize was proposed “to record some of the most difficult problems with which mathematicians were grappling at the turn of the second millennium.’’ The P versus NP was included as one of these seven fundamental problems.</p>
<p>Cook has also made important contributions to areas of mathematical logic related to computational complexity. The paper “The relative efficiency of propositional proof systems,”[<a href=""/bib/cook_n991950.cfm#link_2"">2</a>]&nbsp;co-authored with his student Robert Reckhow, laid the foundations of propositional proof complexity. His 1975 paper, ``Feasibly constructive proofs and the propositional calculus’’ [<a href=""/bib/cook_n991950.cfm#link_3"">3</a>]&nbsp;introduces a logical system which characterizes feasible reasoning, and demonstrates how such reasoning is related to efficiency in propositional proof systems. He has also done significant work in areas including automata theory, parallel computation, program language semantics and verification, computational algebra, and computability and complexity in higher types.</p>
<p>Professor Cook’s work has received extensive recognition. He was awarded the ACM Turing Award in 1982. He is a fellow of the Royal Society of Canada and the Royal Society of London, and is a member of the National Academy of Sciences (U.S.), the American Academy of Arts and Sciences, and the Gottingen Academy of Sciences. He has been a recipient of the Canada Council Izaak Walton Killam Memorial Prize in 1997, the CRM-Fields Prize in 1999, the Royal Society of Canada John L. Synge Award in 2006, the NSERC Award of Excellence in 2007, and the Czech Academy of Sciences Bernard Bolzano Medal in2008. He was an NSERC Steacie Fellow in 1978-79 and was awarded a Killam Research Fellowship in 1982-83.</p>
<p>In 1985, Stephen Cook was promoted to the position of University Professor at the University of Toronto, and now holds the position of Distinguished University Professor in the Computer Science and Mathematics Departments. Over the course of his career he has mentored over thirty Ph.D. students, and continues to be active in teaching, research and graduate supervision. His book, <em>Logical Foundations of Proof Complexity</em>, co-authored with Phuong Nguyen, appeared in 2010. He currently lives in Toronto with his wife Linda, and has two sons. He is an avid sailor and a long-time member of the Royal Canadian Yacht Club.</p>
<p><strong>Enduring Links of Interest</strong></p>
<p style=""margin-left: 40px;"">•&nbsp;<a href=""https://www.claymath.org/millennium/"" target=""_blank"">Clay Foundation Millenium Problem Prize </a>page on the <a href=""https://www.claymath.org/millennium/P_vs_NP/"" target=""_blank"">P vs NP problem</a><br>
• <a href=""http://purl.umn.edu/107226"" target=""_blank"">Oral history interview </a>with Stephen A. Cook at the <a href=""http://www.cbi.umn.edu/"" target=""_blank"">Charles Babbage Institute</a><br>
• <a href=""http://genealogy.math.ndsu.nodak.edu/id.php?id=14011"" target=""_blank"">Stephen A. Cook </a>at the <a href=""http://genealogy.math.ndsu.nodak.edu/id.php?id=14011"" target=""_blank"">Mathematics Genealogy Project</a></p>
<br>
<p style=""text-align: right;""><span class=""callout"">Author: Bruce Kapron</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/cook_n991950.cfm""><img src=""/images/lg_aw/N991950.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Stephen A Cook""></a>
<br><br>
<h6 class=""label""><a href=""/photo/cook_n991950.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>1939, Buffalo NY</p>
<h6 class=""label"">EDUCATION:</h6>
<p>B.Sc., University of Michigan (1961); S.M., Harvard University (1962); Ph.D., Harvard University (1966).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Assistant Professor, University of California, Berkeley (1966-1970); Associate Professor, University of Toronto (1970-1975); Professor, University of Toronto (1975-1985); University Professor, University of Toronto (from 1985).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>NSERC E.W.R. Steacie Memorial Fellowship (1977); Association for Computer Machinery Turing Award (1982); Canada Council Killam Research Fellowship (1982); Canada Council Izaak Walton Killam Memorial Prize (1997); CRM-Fields Prize (1999); Royal Society of Canada John L. Synge Award (2006); NSERC Award of Excellence (2007); Czech Academy of Sciences Bernard Bolzano Medal (2008); Fellow, Royal Society of Canada; Fellow, Royal Society of London; Fellow, Association for Computing Machinery; Member, National Academy of Sciences (U.S.); Member, American Academy of Arts and Sciences; Member, Gottingen Academy of Sciences; Gerhard Herzberg Canada Gold Medal for Science and Engineering (2013) which comes with $1 million in research funding; t<span style=""line-height: 1.3;"">he&nbsp;</span><span style=""line-height: 1.3;"">Government of Ontario&nbsp;appointed him to the&nbsp;</span><span style=""line-height: 1.3;"">Order of Ontario&nbsp;in 2013, the highest honor in&nbsp;</span><span style=""line-height: 1.3;"">Ontario.</span></p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81451600040","Stephen Arthur Cook","<li class=""bibliography""><a href=""/bib/cook_n991950.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283938&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/cook_n991950.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/cook_n991950.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/cook_n991950.cfm""><span></span>Video Interview</a></li>"
"1573179700-702","https://amturing.acm.org/award_winners/knuth_1013846.cfm","For his major contributions to the analysis of algorithms and the design
of programming languages, and in particular for his contributions to the ""art of computer programming""
through his well-known books in a continuous series by this title.","<p>
<strong>Birth and Education</strong></p>
<p>
<strong>Donald Ervin “Don” Knuth was born January 10, 1938, in Milwaukee, Wisconsin.</strong> His father was a teacher in a Lutheran high school and a church organist. Don Knuth attended Lutheran high school and, in later life, also became a church organist.</p>
<p>
In the 7th and 8th grades Knuth was very interested in the structure of English grammar. In high school he was interested in physics and was good at mathematics. Heading for college, he wondered whether to major in physics or music, but chose the former because he got a scholarship to the Case Institute of Technology in Cleveland, Ohio. In his sophomore year of college he switched his major from physics to mathematics, partly because he had trouble with science labs.</p>
<p>
During the summer between his freshman and sophomore years, Knuth worked in the statistics lab drawing graphs, key punching tabulating cards, and using a card sorter. While there, he spotted the newly installed IBM 650 computer across the hall. The 650 intrigued him, and he learned to program using it. He wrote a variety of interesting programs during his undergraduate years, including one to rate the performance of members of the basketball team he managed.</p>
<p>
Knuth was so good at mathematical studies at Case that the faculty awarded him an M.S. in mathematics when he finished his B.S. work.</p>
<p>
<br>
<strong>The Art of Computer Programming</strong></p>
<p>
While working on his PhD in mathematics at the California Institute of Technology, Knuth also did private consulting, and wrote compilers for various computers. The word got around that he knew a lot about compilers, and in January 1962, in his second year at Cal Tech, Addison-Wesley asked him to write a book on compilers. He sketched 12 chapters and signed a contract.</p>
<p>
After receiving his PhD in 1963, Knuth began working on a chapter on sorting, a topic related to some compilers. He read many technical articles, and noticed the spotty and sometimes unreliable nature of the literature in the, then new, field of computer science. He saw the need for someone to write a book which organized and reliably presented what was then known about the field. Knuth was a good writer and had an instinct for trying to organize things, so he decided to tackle it.&nbsp; He used a quantitative rather than qualitative approach, and emphasized aesthetics—the creation of programs that are beautiful. The book grew longer as he wrote it, reaching 3,000 hand-written pages (the equivalent of 2,000 printed pages) by the time he finished the first draft of the 12 chapters in June 1965.</p>
<p>
Addison-Wesley decided that the 12 chapters should be reorganized into 7 volumes, with a chapter or two per volume. The first four volumes [<a href=""/bib/knuth_1013846.cfm#link_1"">1</a>]&nbsp;were to be on basic concepts and information structures (volume 1, chapters 1 and 2), random numbers and arithmetic (volume 2, chapters 3 and 4), sorting and searching (volume 3, chapters 5 and 6), and combinatorial algorithms (volume 4, now scheduled to be divided into at least two volumes). Volumes 5-7 were to be the more compiler specific chapters (lexical scanning and parsing, context free languages, and compiler techniques).</p>
<p>
Volume 1 of <em>The Art of Computer Programming </em>was published in 1968. That same year Knuth moved from Cal Tech to Stanford University, a move motivated partly by the fact Stanford also agreed to hire his colleague <a href=""/award_winners/floyd_3720707.cfm"">Robert Floyd</a>,&nbsp;the 1978 Turing Award winner.</p>
<p>
By 1973 Knuth had published volumes 1-3 of <em>The Art of Computer Programming </em>(“TAOCP”), which did the job Knuth intended of summarizing what was then known about the topics covered in those first six chapters. TAOCP emphasized a mathematical approach for comparing algorithms to find out how good a method is. Knuth says that when he decided this approach, he suggested to his publishers renaming the book <em>The Analysis of Algorithms</em>, but they said ""We can’t; it will never sell."" Arguably, the books established analysis of algorithms as a computer science topic in its own right. Knuth has stated that developing analysis of algorithms as an academic subject is his proudest achievement.</p>
<p>
<br>
<strong>Turing Award</strong></p>
<p>
The first three volumes of TAOCP had great impact on the field and encouraged many people to build on his work. In 1974 Knuth was awarded the ACM's Alan M. Turing Award, with the citation specifically citing his book series. B. A. Galler commented [<a href=""/bib/knuth_1013846.cfm#link_7"">7</a>]:&nbsp;</p>
<p style=""margin-left: 40px;"">
The 1974 A.M. Turing Award was presented to Professor Donald E. Knuth of Stanford University for a number of major contributions to analysis of algorithms and the design of programming languages, and in particular for his most significant contributions to the ""art of computer programming"" through his series of well-known books. The collections of technique, algorithms, and relevant theory in these books have served as a focal point for developing curricula and as an organizing influence on computer science.</p>
<p>
The Turing citation also refers to ""design of programming languages"", the topic that first hooked Knuth on computers, and an area in which he was active from 1957-1971 [<a href=""/award_winners/knuth_1013846.cfm#link_d"">d</a>].</p>
<p>
<br>
<strong>TeX and METAFONT: typesetting and font design</strong></p>
<p>
Knuth is well-known for his perfectionism, and offers to pay $2.56 for each error found in TAOCP books. Finding one confers prestige on the discoverer, many of whom frame and display the hand-signed check rather than cashing it.</p>
<p>
As the topics covered by volumes 1-3 developed, Knuth put out revised editions. He was deeply disappointed when saw the typesetting from Addison-Wesley for the second edition of volume 2, because in 1973 Addison-Wesley had replaced its mechanical typesetting technology with computerized typesetting that did not reproduce the high quality of the original printings of volumes 1-3.</p>
<p>
Consequently, in 1977 Knuth began developing a new typesetting system to enable high quality computerized typesetting, in particular for TAOCP. This system was announced in his 1978 American Mathematical Society Gibbs Lecture entitled ""<a href=""https://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.bams/1183544082"" target=""_blank"">Mathematical Typography</a>"" and published in the <em>Bulletin (New Series) of the American Mathematical Society</em>, volume 1, 1979, pp. 337-372. Knuth had two goals for his system:</p>
<p style=""margin-left: 40px;"">
(1) achieving the finest quality printed documents<br>
(2) creating a system that would be archival in the sense that it was independent of changes in printing technology to the maximum extent possible.</p>
<p>
Knuth's system, developed with help from Stanford students and colleagues, had three primary components: the TeX typesetting engine, the METAFONT font design system, and the Computer Modern set of type fonts [<a href=""/bib/knuth_1013846.cfm#link_4"">4</a>].&nbsp;&nbsp;Combined, these revolutionized digital typesetting. Knuth made his code publicly available, and it has been widely adapted by commercial typesetting systems.</p>
<p>
Knuth put hooks in his TeX engine so that others could make additions, with the condition that any resulting system be give a different name. That produced a vibrant, <a href=""https://tug.org/usergroups.html"" target=""_blank"">worldwide community </a>of users and developers for TeX and related systems like LaTeX, ConTeXt, LuaTeX. Knuth's TeX was an early success story for the free and open-source software movement.</p>
<p>
Knuth thought his typesetting work would take a year or two, but it was not until 1990 that he announced that he would make no further changes to his systems except to correct serious bugs.</p>
<br>
<p>
<strong>Structured and literate programming</strong></p>
<p>
In the early 1970s Knuth learned the ""structured programming"" methodology promoted by <a href=""/award_winners/dijkstra_1053701.cfm"">Dijkstra</a>,&nbsp;<a href=""/award_winners/hoare_4622167.cfm"">Hoare</a>,&nbsp;&nbsp;<a href=""/award_winners/dahl_6917600.cfm"">Dahl</a>,&nbsp;and others. TeX and METAFONT were the first large programs Knuth had written since 1970, and he wrote them using structured programming.</p>
<p>
The first version of TeX was written in the SAIL (Stanford Artificial Intelligence Language) programming language. People other than Knuth began to use TeX in the summer of 1978. While working to make a more portable Pascal-language version of TeX, Knuth experimented with typesetting the software itself. This led to a system he called DOC to support structured programming and documentation of the program. DOC was used in the spring of 1980 to create a highly portable version of TeX.</p>
<p>
The Tex Users Group was also established in early 1980.&nbsp; Once enough user feedback was gathered, Knuth decided that TeX and METAFONT needed to be rewritten. The original program was renamed TeX78, and the new program was called TeX82. In writing TeX82, Knuth decided DOC also needed to be rewritten and he created WEB, so called because it created a web of program and documentation that was both a compilable program and beautiful documentation. METAFONT was also rewritten in WEB. The second and fourth volumes of <a href=""/bib/knuth_1013846.cfm#link_4"">bibliographic item 4</a>&nbsp;illustrate TeX and METAFONT documented using WEB—what Knuth called literate programming.&nbsp; Later versions and additions to WEB were based on the C programming language.</p>
<br>
<p>
<strong>Other activities, TAOCP continued, and Knuth's overall approach</strong></p>
<p>
Throughout his career, Knuth has published extensively in a variety of technical areas; bibliographic item [<a href=""/bib/knuth_1013846.cfm#link_7"">7</a>]&nbsp;lists the volumes of his collected papers in <em>eight</em> different topic areas. At Stanford he supervised more than 30 PhD students. He created important courses such as <em>Analysis of Algorithms </em>[<a href=""/bib/knuth_1013846.cfm#link_2"">2</a>],&nbsp;<em>Concrete Mathematics </em>[<a href=""/bib/knuth_1013846.cfm#link_3"">3</a>],&nbsp;and the legendary <em>Programming and Problem Solving Seminar</em>.</p>
<p>
In addition to his own teaching and research, Knuth has served the computing community on professional society committees, as an invited lecturer on many occasions, and on the editorial boards of more than 30 technical journals. He is the holder or co-holder of 5 patents.</p>
<p>
Knuth considers TAOCP his masterwork, and in 1993 he retired early to spend more time writing additional volumes. He had produced revised editions of volumes 1-3 in 1978-1979. He designed a new hypothetical computer to replace the MIX computer of volumes 1-3 for the analysis of algorithms; this new computer was described in Knuth's 1999 book devoted to the topic [<a href=""/bib/knuth_1013846.cfm#link_8"">8</a>].&nbsp;</p>
<p>
Knuth began releasing volume 4A of TAOCP, on combinatorial algorithms, in fascicles ranging from 128 to 250 pages. In early 2011 the 921-page volume 4A was published, but the later parts of chapter 7 were reserved for future volumes.</p>
<p>
Donald Knuth is one of the preeminent computer scientists of our time. He has made major contribution to many areas, in effect pursuing multiple simultaneous and serial careers, any one of which would be a proud lifetime achievement for other people. He credits much of the success of his work to combining theory with practice. Knuth is the rare theoretician who writes many lines of code every day. Programming is an art he practices often.</p>
<p>
Knuth married Nancy Jill Carter in 1961. They have two children, John Martin (born 1965) and Jennifer Sierra (born 1966).</p>
<p>
Knuth has been widely interviewed, both for print and on video. The TeX Users Group maintains a fairly comprehensive <a href=""https://tug.org/interviews/#knuth"" target=""_blank"">list</a> of his interviews.</p>
<br>
<p>
<strong>Essay Sources</strong></p>
<p>
a. ""Donald Knuth,"" Interviewed by Donald J. Albers and Lynn A. Steen,&nbsp;<em>Mathematical People: Profiles and Interviews</em>, Donald J. Albers and G. L. Alexanderson, editors, Birkhauser, Boston, 1985, pp. 182-203.<br>
b. Donald Knuth, ""Robert W Floyd, In Memoriam,""&nbsp;<em>ACM SIGACT News</em>, Volume 34 Issue 4, December 2003, pp. 3-12.<br>
c. ""The Art of Computer Programming,"" June 2011 interview of Knuth by British Computer Society editor Justin Richards,<a href=""http://www.bcs.org/content/conWebDoc/40462"" target=""_blank""> http://www.bcs.org/content/conWebDoc/40462</a><br>
d. <a name=""link_d""></a>Donald Knuth, Selected Papers on Computer Languages, see the 2003 volume of bibliographic item 7.<br>
e. Donald Knuth, <em>Digital Typography</em>, see the 1999 volume of&nbsp;<a href=""/bib/knuth_1013846.cfm#link_7"">bibliographic item 7</a>.<br>
f. Donald Knuth, <em>Selected Papers on Computer Science</em>, see the 1996 volume of <a href=""/bib/knuth_1013846.cfm#link_7"">bibliographic item 7</a>.<br>
g. Donald Knuth, <em>Literate Programming</em>, see the 1992 volume of <a href=""/bib/knuth_1013846.cfm#link_7"">bibliographic item 7</a>.<br>
h. Knuth's personal website: <a href=""http://www-cs-faculty.stanford.edu/~uno/"" target=""_blank"">http://www-cs-faculty.stanford.edu/~uno/</a><br>
i. Knuth's curriculum vitae (CV) : <a href=""http://www-cs-faculty.stanford.edu/~uno/vita.pdf"" target=""_blank"">http://www-cs-faculty.stanford.edu/~uno/vita.pdf</a></p>
<p style=""text-align: right;"">
<span class=""callout"">Author: David Walden</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/knuth_1013846.cfm""><img src=""/images/lg_aw/1013846.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Donald E. Knuth""></a>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>January 10, 1938, in Milwaukee, Wisconsin.</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Graduated from Milwaukee Lutheran High School (1956); BS in mathematics from the Case Institute of Technology (1960), his undergraduate work was so distinguished that he was awarded a simultaneous MS by special vote of the faculty; Ph.D. in mathematics from the California Institute of Technology (1963).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Assistant and then Associate Professor, California Institute of Technology (1963-1968); A series of full and named professorships, Stanford University (1968-1992); Professor of the Art of Computer Programming, Emeritus, Stanford University (from 1993).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Knuth has received over 100 awards and honors. Among these are: First ACM Grace Murray Hopper Award (1971); Fellow, American Academy of Arts and Sciences (1973); Alan M. Turing Award (1974); Member, National Academy of Science (1975); Lester R. Ford Award (1975 and 1993); American Mathematical Society Gibbs Lecture (1978); National Medal of Science (1979, awarded by President Carter); Member, National Academy of Engineering (1981); IEEE Computer Society Computer Pioneer Award (1982); Franklin Medal (1988); Harvey Prize (1995); IEEE John von Neumann Medal (1995); Kyoto Prize (1996); Fellow, Computer History Museum (1998).<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100246710","Donald (""Don"") Ervin Knuth","<li class=""bibliography""><a href=""/bib/knuth_1013846.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283929&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/knuth_1013846.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/knuth_1013846.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/knuth_1013846.cfm""><span></span>Video Interview</a></li>"
"1573179197-669","https://amturing.acm.org/award_winners/simon_1031467.cfm","In joint scientific efforts extending over twenty years, initially in collaboration with J. C. Shaw at the RAND Corporation, and subsequentially with numerous faculty and student collegues at Carnegie-Mellon University, Simon and co-recipient Allen Newell made basic contributions to artificial intelligence, the psychology of human cognition, and list processing.","<p align=""left""><strong>Herbert Alexander Simon was born in Milwaukee, Wisconsin on June 15, 1916, to Edna and Arthur Simon.</strong> Simon’s father worked for the Cutler-Hammer manufacturing company helping to design control devices. His father’s work was not a direct influence on Simon, but when he later began to study feedback-controlled devices, the connection to his father was a source of pride.</p>
<p align=""left"">Simon’s family was quite ecumenical regarding religion: Simon himself was raised mostly as a Jew, though he attended a Lutheran Sunday school and later became a Unitarian, in part because there was “no room” in his personal theology for “any notion of a chosen people.” Simon’s highly analytic approach to religion was typical: he applied his own critical, rational faculties to everything—and everyone—he encountered. In particular, he applied it to the human mind and how it solved problems.</p>
<p align=""left"">The human mind was central to all of Simon’s work, whether in political science, economics, psychology, or computer science. Indeed, to Simon, computer science was psychology by other means. Simon’s remarkable contributions to computer science flowed from his desire to make the computer an effective tool for simulating human problem-solving. To him, a computer program that solved a problem in a way that humans did not (or worse, could not) was not terribly interesting, even if it solved that problem far more efficiently than humans did. Conversely a computer program that failed to solve a problem might be a great achievement, so long as it failed in the ways that humans fail.</p>
<p align=""left"">One of the most important outcomes of this approach to computer science was Simon’s development—and strong advocacy—of <a href=""https://en.wikipedia.org/wiki/Heuristic"" target=""_blank"">heuristic programming</a>. Drawing on his studies of human psychology and of organizational decision-making, Simon noted that people <strong>intend</strong> to be rational but that they rarely, if ever, have access to all the information or all the time they would need to make the optimally rational choice. Thus, Simon concluded, we do not, because we cannot, solve problems by using exhaustive, precise algorithms. Rather, we must use simpler heuristics and accept satisfactory rather than optimal results in order to make decisions or solve problems. To use a common analogy: a safecracker with unlimited time can try every combination and thus can be assured of opening the safe eventually. The safecracker who operates in the real world, however, has limited time and so begins by trying combinations based on the owner’s family birthdays, anniversaries, and the like. This heuristic does not guarantee success, but it will work often, and when it works, gives results much more quickly.</p>
<p align=""left"">Simon and his colleagues Allen Newell and J.C. Shaw employed this notion of heuristic problem-solving in the first successful AI program, the <a href=""https://en.wikipedia.org/wiki/Logic_Theorist"" target=""_blank""><em>Logic Theorist</em></a> (<em>LT</em>) of 1955-56, which was used to prove the theorems of Russell and Whitehead’s <a href=""https://en.wikipedia.org/wiki/Principia_Mathematica"" target=""_blank""><em>Principia Mathematica</em></a><em>. </em>In a wonderful ironic twist, Simon first used family members to simulate the workings of the <em>Logic Theorist </em>before it was programmed into a computer, so he had people simulate the workings of a machine designed to simulate the workings of people’s minds! Simon was so excited by <em>LT </em>that he famously announced to his undergraduate class the next semester that “Over the Christmas holiday, Allen Newell and I invented a machine that thinks.”</p>
<p align=""left"">Significantly, in addition to employing principles of heuristic problem-solving, the <em>Logic Theorist</em> was an error-controlled, feedback “machine” that compared the goal state (the statement to prove) with the current state and performed one of a small set of basic operations in order to reduce the difference between the two states. The <em>Logic Theorist</em> was a remarkable success, and Simon, Newell, and Shaw elaborated on its basic principles in creating another renowned program, The <a href=""https://en.wikipedia.org/wiki/General_Problem_Solver"" target=""_blank""><em>General Problem Solver</em></a> (<em>GPS</em>) in 1957-8. The <em>GPS</em> was not quite as universal as its name implied, but it was startlingly good at solving certain kinds of well-defined problems. Even more, <em>GPS</em>, like <em>LT, </em>appeared to solve them in much the same ways that humans did.</p>
<p align=""left"">Simon’s novel approach to the computer was, in part, a product of his education at the University of Chicago in the 1930s, to which he won admission as an undergraduate by competitive exam. He flourished in the intellectual hothouse of interwar Chicago, attending few courses but reading widely—and debating fiercely—in political science, philosophy, and mathematics. The transition to graduate study at Chicago was nearly seamless for Simon, who relished the demanding, but unstructured, nature of work there in the Department of Political Science.</p>
<p align=""left"">While at Chicago, Simon encountered the German philosopher, Rudolf Carnap, whose rigorous <a href=""https://en.wikipedia.org/wiki/Positivism"" target=""_blank"">positivism</a> meshed well with Simon’s emerging outlook. Simon also studied with the pioneering mathematical economist Henry Schultz, who introduced Simon to the burgeoning world of econometrics, to mathematical modeling, to sophisticated work on the theory of measurement, and to the <a href=""http://cowles.econ.yale.edu/"" target=""_blank"">Cowles Commission for Research in Economics</a>, which was home at the time to eleven future Nobel Prize winners in Economics, including Simon. Simon believed that these mathematical economists were developing some powerful tools and techniques for modeling human behavior, but that they had an absurdly unrealistic image of the ability of humans to make rational choices. As he put it, “we need a less God-like, and more rat-like, picture of the chooser.” <em>LT</em> and <em>GPS</em> were intended to create just such “rat-like” models of how people actually solve problems in the real world.</p>
<p align=""left"">Simon—with a series of collaborators—continued to develop programs designed to simulate the operations of the human information-processing system, ranging from programs that played chess, to the <a href=""https://en.wikipedia.org/wiki/EPAM"" target=""_blank""><em>Elementary Perceiver and Memorizer</em></a> (<em>EPAM</em>, co-created with <a href=""/award_winners/feigenbaum_4167235.cfm"">Edward Feigenbaum</a>) that simulated the processes of human sensory perception and learning, to <em>BACON</em>, which simulated the process of discovery in science. Throughout, he was a strong, even fierce, advocate of the computer program as <strong>the</strong> best formalism for psychological theories, holding that the program <u>is</u> the theory. The fullest statement of this belief was the monumental text, <em>Human Problem Solving</em> [<a href=""/bib/simon_1031467.cfm#link_7"">7</a>], authored by Simon and Newell in 1972, in which they introduced the notion of a program as a set of “production systems”, or “if-then” statements. The flip side of this coin was his insistence that computer simulation was an <strong>empirical</strong> science that taught us new and valuable things about ourselves and our world; simulation was not an exercise in elaborating tautologies.</p>
<p align=""left"">Last, but not least, Simon believed that organization and structure were critical. Indeed, what his computer simulations simulated was <strong>not</strong> the actual physical operations of neurons in the brain, but rather the structure of problem-solving processes. The computer program thus could be a structural model of the mind in action, not a model of its specific physical make-up. Two of the key conclusions he drew about the structure of our human mental processes are that they are <strong>hierarchical</strong> and that they are <strong>associative</strong>. In other words, he believed that they have a “tree structure”, with each node/leaf linked to a branch above it. Significantly, to Simon, each “leaf” could either be one thing <strong>or</strong> a set of things—a list of things, to be precise, with the elements of a list possibly being other leafs with their own lists, and sub-lists. (Think of a “to do” list that contains the item “go grocery shopping”, an item that contains its own sub-list of items to purchase.) Since items on a list could “call” items on other lists, this model of the mind could work associatively within its basic hierarchic structure, creating webs of association amongst the branches of the mind’s tree.</p>
<p align=""left"">To implement this hierarchical, associative model of the mind, Simon and Newell worked with Shaw (a programmer at RAND) to develop the first list processing language, <a href=""https://en.wikipedia.org/wiki/Information_Processing_Language"" target=""_blank"">IPL</a> (Information Processing Language). While IPL, a low-level assembly language for list processing, was largely superseded by <a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy’s</a> more powerful high-level list processing language LISP, it was a major influence on the development of later list-processing languages, including LISP itself.</p>
<p align=""left"">As befits someone fascinated by organizations, Simon was an institution-builder as well as a researcher. In the world of computer science, his most significant institutional legacy is the world-renowned School of Computer Science at Carnegie-Mellon University. Simon, Newell, and their colleague <a href=""/award_winners/perlis_0132439.cfm"">Alan Perlis</a> first created a Department of Computer Science in 1965, and they (and others) expanded it until it became its own school in 1988. In keeping with Simon’s interests in AI, simulation, software design, and human-computer interaction, the Carnegie-Mellon University School of Computer Science excels in those areas.</p>
<p align=""left"">Simon died on February 9, 2001, having received not only the ACM Turing Award (shared with Newell in 1975), but also the Nobel Prize in Economics (1978), The National Medal of Science (1986), The American Psychological Association’s Lifetime Achievement Award (1993), the American Political Science Association’s Dwight Waldo Award (1995), and the Institute for Operations Research and Management Science Von Neumann Theory Prize (1988). He was survived by his wife of 63 years, Dorothea (who died in August 2002), and their children, Katherine, Peter, and Barbara.</p>
<p align=""left"">Three useful sources of biographical information on Simon are:</p>
<ol>
<li>Mie Augier and James March, <em>Models of a Man: essays in honor of Herbert Simon</em>, Cambridge, MA: MIT Press, 2004.<span class=""callout"">A collection of essays by prominent scholars on Simon’s influence on them and their fields.</span></li>
<li>Hunter Crowther-Heyck, <em>Herbert A. Simon: the bounds of reason in modern America</em>, Baltimore, MD: Johns Hopkins University Press, 2005.<span class=""callout"">The standard intellectual biography of Simon.</span></li>
<li>Herbert Simon, <em>Models of My Life</em>, NY: Basic Books, 1991. <span class=""callout"">Simon’s own autobiography.</span></li>
</ol>
<p align=""right""><span class=""callout"">Author: Hunter Heyck</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/simon_1031467.cfm""><img src=""/images/lg_aw/1031467.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Herbert A. Simon""></a>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>June 15, 1916, Milwaukee, Wisconsin, US.</p>
<h6><span class=""label"">DEATH: </span></h6>
<p>February 9, 2001, Pittsburgh, Pennsylvania, US.</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>BS University of Chicago, Political Science (1937); PhD, University of Chicago, Political Science (1943).</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Assistant Professor of Political Science (later Associate Professor and Department Chair), Illinois Institute of Technology (1944-1949); Professor, Graduate School of Industrial Administration, Carnegie-Mellon University, (1949-1965), Computer Science and Psychology (1965-2001), Trustee for Life (1972-2001)</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>A few of Simon’s many prominent awards are:&nbsp;<span style=""line-height: 20.8px;"">Member, National Academy of Sciences (1972);&nbsp;</span><span style=""line-height: 1.3;"">ACM Turing Award (1975 - with Allen Newell);&nbsp;</span><span style=""line-height: 20.8px;"">Nobel Prize in Economics (1978);</span><span style=""line-height: 20.8px;"">&nbsp;</span><span style=""line-height: 1.3;"">National Medal of Science (1986);&nbsp;</span><span style=""line-height: 1.3;"">Harold </span>Pender<span style=""line-height: 1.3;""> Award (1987);&nbsp;</span><span style=""line-height: 20.8px;"">Institute of Operations Research and Management Science von Neumann Theory Prize (1988);</span><span style=""line-height: 20.8px;"">&nbsp;</span><span style=""line-height: 20.8px;"">APA Lifetime Achievement Award (1993);</span><span style=""line-height: 20.8px;"">&nbsp;ACM Fellow (1994);&nbsp;</span><span style=""line-height: normal;"">IJCAI Award for Research Excellence (1995);&nbsp;</span>APSA<span style=""line-height: 1.3;""> Waldo Award (1995).</span></p>","","https://dl.acm.org/author_page.cfm?id=81100527918","Herbert (""Herb"") Alexander Simon","<li class=""bibliography""><a href=""/bib/simon_1031467.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283930&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/simon_1031467.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/simon_1031467.cfm""><span></span>Additional<br> Materials</a></li>"
"1573178950-651","https://amturing.acm.org/award_winners/sifakis_1701095.cfm","","<p>
Sifakis’ work is characterized by an unusual recurrent pattern: the problem is first studied from an abstract, foundational point of view, which leads to methods and techniques for its solution, which, in turn, leads to an effective implementation that is successfully used in multiple industrial applications. In what follows his main achievements are briefly presented from a historical perspective.</p>
<p>
From 1974 to 1977 Sifakis studied <a href=""https://en.wikipedia.org/wiki/Petri_net"" target=""_blank"">Petri nets</a> and other models for concurrent systems [<a href=""/bib/sifakis_1701095.cfm#bib_1"">1</a>]. During this period Sifakis obtained original and fundamental results on the structural properties of Petri nets as well as on the performance evaluation of timed Petri nets. These results are extensively used today for scheduling <a href=""https://en.wikipedia.org/wiki/Data-flow"" target=""_blank"">data-flow</a> systems.</p>
<p>
From 1977 to1982 he switched his attention to verification of <a href=""https://en.wikipedia.org/wiki/State_transition_system"" target=""_blank"">transition systems</a>. Once again his work yielded original results on the algorithmic verification of concurrent systems based on a fix-point characterization of the modalities of a branching time <a href=""https://en.wikipedia.org/wiki/Temporal_logic"" target=""_blank"">temporal logic</a>. These results laid down the foundations of <a href=""https://en.wikipedia.org/wiki/Model_checking"" target=""_blank"">model checking</a> – the most widely used verification method in industry – and have been implemented for the first time in the CESAR model checker [<a href=""/bib/sifakis_1701095.cfm#bib_2"">2</a>].</p>
<p>
This work was expanded during 1982-1988 to deal with model checking and temporal logics for the description of concurrent system specifications [<a href=""/bib/sifakis_1701095.cfm#bib_3"">3</a>]. The development of the CESAR model checker was funded by France Telecom and was extensively used for the certification of communication protocols. He went on to development a theory and techniques for computing property-preserving abstractions, which&nbsp; have been effectively used to verify complex systems.</p>
<p>
In a later episode (1988-2000) he extended this work to deal with modeling and verification of real-time systems [<a href=""/bib/sifakis_1701095.cfm#bib_7"">7</a>, <a href=""/bib/sifakis_1701095.cfm#bib_8"">8</a>, <a href=""/bib/sifakis_1701095.cfm#bib_9"">9</a>]. This included:</p>
<ol>
<li>
the study of hybrid models – models combining discrete and continuous dynamics;</li>
<li>
the development and implementation of the KRONOS model checker, and, in collaboration with T. Henzinger, the first symbolic model checker for timed automata (work);</li>
<li>
in collaboration with O. Maler and &nbsp;<a href=""/award_winners/pnueli_4725172.cfm"">A. Pnueli</a> the development and implementation of an efficient symbolic synthesis algorithm for timed;</li>
<li>
the study of compositional modeling techniques for real-time scheduling by using priorities.</li>
</ol>
<p>
In 1993 Sifakis founded the <a href=""http://www-verimag.imag.fr/"" target=""_blank"">Verimag</a> industrial laboratory, a joint-venture between the Computer Science and Applied Mathematics laboratory (IMAG) and VERILOG SA. Verimag has been funded by Airbus and Schneider Electric to develop methods and tools based on results produced by Sifakis and his team, mainly:</p>
<ol>
<li>
The <a href=""https://en.wikipedia.org/wiki/Esterel_Technologies"" target=""_blank"">SCADE</a> synchronous programming environment based on the <a href=""https://en.wikipedia.org/wiki/Lustre_%28programming_language%29"" target=""_blank"">Lustre language</a>, used by Airbus for more than 15 years to develop safety critical systems, a de facto standard for aeronautics. SCADE has been qualified as a development tool by the FAA, EASA, and Transport Canada under DO-178B up to Level A.</li>
<li>
The ObjectGeode specification and validation tool for real-time distributed applications, which includes functional testing and verification techniques. This tool&nbsp; was commercialized by Telelogic SA until the company was acquired&nbsp; by IBM in 2008.</li>
</ol>
<p>
Since 1997, Verimag has been a public research laboratory, associated with CNRS and the University of Grenoble. It has played a prominent role in the area of embedded systems by producing cutting edge results and leading research initiatives and projects in Europe.</p>
<p>
From 1998, Sifakis has actively promoted the emergence of embedded systems and has contributed to the constitution of an international and live research community in this area. He has been the Scientific Coordinator of the <a href=""http://www.artist-embedded.org/artist/"" target=""_blank"">ARTIST</a> European Network of Excellence on Embedded System Design which has coordinated research of leading European teams in embedded systems for more than 10 years. He actively contributed to setting up the <a href=""http://www.artemisia-association.org/"" target=""_blank"">ARTEMISIA</a> industrial association for embedded systems in Europe and is a co-founder of the EmSoft Conference and Embedded Systems Week.</p>
<p>
During this later period Sifakis has also played a major role in the development and implementation of the BIP component framework for rigorous system design [<a href=""/bib/sifakis_1701095.cfm#bib_10"">10</a>,<a href=""/bib/sifakis_1701095.cfm#bib_11"">11</a>]. The implementation consists of a language and a set of tools including source-to-source transformers, a compiler and the D-Finder tool for compositional verification. BIP is unique for its expressiveness [<a href=""/bib/sifakis_1701095.cfm#bib_12"">12</a>,<a href=""/bib/sifakis_1701095.cfm#bib_13"">13</a>]. It can describe mixed hardware/software systems. It uses a small and powerful set of primitives encompassing a general concept of system architecture. BIP was successfully used in several industrial projects, in particular for the componentization of legacy software and the automatic generation of implementations for many-core platforms.</p>
<p>
Further information on Joseph Sifakis and his research can be found <a href=""http://www-verimag.imag.fr/%7Esifakis"" target=""_blank"">here</a>.</p>
<p align=""right"">
<span class=""callout"">Author: Cristian S. Calude</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/sifakis_1701095.cfm""><img src=""/images/lg_aw/1701095.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Joseph Sifakis""></a>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>December 26, 1946</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>Doctorate (1974, Electrical Engineering, National Technical University of Athens, Greece); State Doctorate (1979, Computer Science, University of Grenoble, France).</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Since 1975, CNRS and University Joseph Fourier researcher; founder of Verimag Laboratory in Grenoble (CNRS, UGA), France; Professor at EPFL, (École Polytechnique Fédérale de Lausanne), October 2011-October 2016); INRIA-Schneider endowed industrial chair (2008-2011); Coordinator of Artist Design, the European Network of Excellence for Research on Embedded Systems (2008-2012); in addition to these appointments he has had broad experience with industries such as Airbus, Astrium, Ericsson, the European Space Agency, France Telecom, Philips, ST Microelectronics, Thalès.&nbsp;</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>CNRS Silver Medal (2001), Honorary doctorates from École Polytechnique Fédérale de Lausanne, Switzerland (2009), University of Athens (2010) and the International Hellenic University (2011); Honorary Professor of the University of Patras (2008);&nbsp;Member of the Academia Europaea (2008), the French National Academy of Engineering (2008), the French Academy of Sciences (2011), the American Academy of Arts and Sciences (2015) and the National Academy of Engineering (2017); Grand Officer of the French National Order of Merit (2008); Commander of the Legion of Honor (2011); Leonardo da Vinci Medal (2012); Commander of the Order of the Phoenix, Greece (2013); Logic in Computing Science (LICS) 2012 Test-of-Time Award for the paper ""Symbolic model checking for real-time systems.""</p>","","https://dl.acm.org/author_page.cfm?id=81100396269","Joseph Sifakis","<li class=""bibliography""><a href=""/bib/sifakis_1701095.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/sifakis_1701095.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/sifakis_1701095.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179793-709","https://amturing.acm.org/award_winners/cerf_1083211.cfm","With Robert E. Kahn, for pioneering work on internetworking, including the design and implementation of the Internet's basic communications protocols, TCP/IP, and for inspired leadership in networking.","<p><strong>Vinton G. Cerf and <a href=""/award_winners/kahn_4598637.cfm"">Robert E. Kahn</a> led the design and implementation of the Transmission Control Protocol and Internet Protocol (TCP/IP) that are the basis for the current internet.</strong> They formulated fundamental design principles of networking, specified TCP/IP to meet these requirements, prototyped TCP/IP, and coordinated several early TCP/IP implementations. Since then, they have continued to provide leadership in the networking research community and in the emerging industries of the internet and telecommunications.</p>
<p><em><strong>Background</strong></em></p>
<p>Robert Kahn was born 23 December 1938, in Brooklyn, New York. He earned his B.E.E. in electrical engineering at the City College of New York in 1960 and went on to earn his M.A. (1962) and Ph.D. (1964) in electrical engineering from Princeton. He took a job at AT&amp;T Bell Labs in 1964 and then joined the electrical engineering department at MIT as an assistant professor in 1966. Wishing to do more applied research, in 1967 Kahn arranged a leave of absence from MIT in to work at the firm of Bolt, Beranek and Newman (now known as BBN), where he began developing his own ideas for computer&nbsp;networking.</p>
<p>In 1968, Lawrence Roberts of the US Department of Defense Advanced Research Projects Agency issued a request for proposals for an experimental large-scale network. The ARPANET project proposed using emerging techniques such as <a href=""/info/cerf_1083211.cfm"">packet switching&nbsp;</a>and <a href=""/info/cerf_1083211.cfm"">distributed communications</a>,&nbsp;along with an unheard-of diversity of computer hardware and operating systems. The ARPANET’s backbone was to consist of a set of long-distance telephone lines connected by packet switches. BBN’s role was to provide the hardware and software for these switches, called&nbsp;<a href=""https://en.wikipedia.org/wiki/Interface_Message_Processor"" target=""_blank"">Interface Message Processors</a> or IMPs. Kahn played a key role in system design for the proposal that ultimately won BBN the contract, and he subsequently joined the ARPANET development team led by Frank Heart.</p>
<p>In 1972, Kahn took the lead in organizing the first public demonstration of the ARPANET at the October International Computer Communication Conference in Washington, D.C. Prior to that demonstration the ARPANET had been an interesting but underused experimental system but, with Kahn’s encouragement, people at the various sites began to bring new applications online, making the network more attractive to users. This effort brought the ARPANET to maturity and dramatically introduced the network to the larger computer science world.</p>
<p>Vint Cerf was born 23 June 1943 in New Haven, CT. He suffered from a hearing impairment from an early age, and he later attributed some of his interest in computer networking to its promise as an alternative communications channel for the hearing impaired. Cerf received his B.S. in Mathematics from Stanford University in 1965, then worked for IBM for two years, where he contributed to Quicktran, a FORTRAN based time-sharing system. This whetted his interest in computer science, and he left IBM to study at the University of California, Los Angeles, where he earned his M.S. (1970) and Ph.D. (1972) in Computer Science.</p>
<p>As a graduate student at UCLA, Cerf became involved in the ARPANET through Leonard Kleinrock, an expert in queuing theory who had a contract to do performance analysis for the new network. As part of this role, UCLA was given one of the first four ARPANET nodes. Cerf became deeply involved in the ongoing discussion and development of the ARPANET host computer software (NCP) through the Network Working Group, whose informal, decentralized mode of operation would become the model for Internet protocol development and open software.</p>
<p><em><strong>Joint work on the Internet</strong></em><br>
Cerf and Kahn first met when Kahn came to UCLA in 1969 to help test the nascent ARPANET. The two formed an effective working relationship to generate test data and predict and diagnose problems in the network.</p>
<p>In late 1972, Kahn joined the<a href=""https://en.wikipedia.org/wiki/Information_Processing_Techniques_Office"" target=""_blank""> Information Processing Techniques Office</a> (part of the US Defense Advanced Research Projects Agency – usually known as “IPTO”) as a program manager and initiated projects in network security, digital speech transmission, and packet radio. In 1973, building on a previous ARPA-funded project called <a href=""https://en.wikipedia.org/wiki/ALOHAnet"" target=""_blank"">Alohanet</a>, Kahn initiated a ground-based packet-radio project, called <a href=""https://en.wikipedia.org/wiki/Packet_radio"" target=""_blank"">PRNET</a>, which started experimental operation in 1975. Kahn also began experimenting with using the Intelsat I satellite to link the Arpanet to sites in Britain and Norway (where ARPA conducted seismic monitoring to detect Soviet underground nuclear tests). In 1975 this effort grew into the Atlantic Packet Satellite Network (SATNET), an experimental transatlantic network operated in conjunction with the British Post Office and Norwegian Telecommunications Authority.</p>
<p>By 1973 Kahn was already thinking about connecting ARPA’s packet radio and satellite networks to the ARPANET, but he faced formidable challenges, since the three networks were technologically incompatible. ARPANET used point-to-point transmission, while the radio networks used broadcast; ARPANET tried to guarantee reliable transmission of packets, while PRNET did not; and SATNET had longer transmission delays because of the great distance the packets had to travel. Successfully connecting such diverse networks would require a new approach.</p>
<p>In the spring of 1973, Kahn approached Cerf with the idea of developing a system for interconnecting networks—what would eventually be called an “internet.” Kahn felt that his own knowledge of the problem of connecting dissimilar networks, combined with Cerf’s expertise in writing host software, would create a strong partnership. In addition, Kahn and Cerf demonstrated farsighted leadership by inviting networking experts from around the world to weigh in on the Internet design at a seminar in June 1973. This move not only led to more robust protocols, but also laid the groundwork for the global spread of the Internet.<br>
Cerf and Kahn outlined the resulting Internet architecture in a seminal 1974 paper, <em>A Protocol for Packet Network Intercommunication </em>[<a href=""/bib/cerf_1083211.cfm#link_2"">2</a>].&nbsp;</p>
<p>There were two key elements. First was a host protocol called the Transmission Control Protocol (TCP), which was intended to provide reliable, ordered, flow-controlled transmission of packets over the interlinked networks. Second was a set of gateways or routers that would sit between networks, passing traffic between them and handling inter-network addressing and routing. There was also a hierarchical address system, whereby packets were first sent by the gateways to a network address and then directed internally to a host address within that network. The Internet architecture was designed to make minimal demands on participating networks, to provide a seamless user experience, and to scale up gracefully, key features that would facilitate the Internet’s rapid expansion in the 1990s.</p>
<p>Cerf initially worked on developing the Internet protocols as an ARPA contractor at UCLA, then moved to IPTO as program manager for networking in 1976, staying at the agency until 1982. In 1978 he collaborated with Jon Postel and Danny Cohen, both at the University of Southern California Information Sciences Institute, to reformulate TCP as a set of two protocols: a host protocol (TCP) and a separate internetwork protocol (IP). IP would be a stripped-down protocol for passing packets within or between networks; it would run on both hosts and gateways, while the more complex TCP would run only on hosts and provide reliable end-to-end service. The new TCP/IP architecture, described in Cerf’s 1980 article “Protocols for Interconnected Packet Networks” [<a href=""/bib/cerf_1083211.cfm#linl_3"">3</a>],&nbsp;simplified the operation of internet gateways and helped increase the number and diversity of the networks connected to the Internet.</p>
<p>Cerf and Kahn oversaw the implementation of TCP and the experimental connection of ARPANET, PRNET and SATNET in 1977; this became the first incarnation of the Internet. Kahn became Director of IPTO in 1979, serving until 1985. He helped guide the changeover of ARPANET sites from the original NCP protocol to TCP/IP in 1983. Also in 1983, Kahn initiated ARPA’s Strategic Computing Initiative, a billion-dollar research program that included chip design, parallel computer architectures, and artificial intelligence.</p>
<p><em><strong>Later activities</strong></em><br>
In 1986 Kahn founded the <a href=""https://en.wikipedia.org/wiki/Corporation_for_National_Research_Initiatives"" target=""_blank"">Corporation for National Research Initiatives </a>(CNRI), where he remains Chairman, CEO, and President. CNRI is a not-for-profit organization that provides technical leadership and funds research and development of emerging information infrastructure components, such as digital object identifiers and “knowbots,” mobile software agents that operate across networked environments.</p>
<p>Cerf left ARPA in 1982 to become Vice president of Digital Information Services at <a href=""https://en.wikipedia.org/wiki/MCI_Communications"" target=""_blank"">MCI</a>, where he created MCI Mail. Cerf arranged for MCI Mail to become the first commercial email service to use the Internet in 1989. Cerf later returned to MCI as Senior Vice President from 1994-2005.</p>
<p>In 1986 Cerf joined Kahn as Vice President of CNRI. In 1991, recognizing the need for a neutral forum for Internet standards development, Cerf and Kahn founded the <a href=""https://en.wikipedia.org/wiki/Internet_Society"" target=""_blank"">Internet Society </a>(ISOC), an international nonprofit organization. ISOC provided an institutional home for the Internet Engineering Task Force, which sets technical standards for the Internet, and eventually expanded into policy and educational activities. Cerf served as President of ISOC from 1992 until 1995. He also served as chairman of the board of the Internet Corporation for Assigned Names and Numbers (ICANN), which coordinates the <a href=""/info/cerf_1083211.cfm"">domain name&nbsp;</a>system among other functions, from 2000-2007. In 2005 Cerf was hired by Google as Vice president and Chief Internet Evangelist.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Janet Abbate</span><br>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/cerf_1083211.cfm""><img src=""/images/lg_aw/1083211.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Vinton Cerf""></a>
<br><br>
<h6 class=""label""><a href=""/photo/cerf_1083211.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>Born 23 June 1943, New Haven, CT</p>
<h6 class=""label"">EDUCATION:</h6>
<p>B.S. in Mathematics, (Stanford University, 1965); M.S. in Computer Science, (University of California, Los Angeles, 1970); Ph.D. in Computer Science (University of California, Los Angeles, 1972).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>IBM Systems engineer (1965-67); Assistant Professor, Stanford University (1972–1976); Program Manager, Information Processing Techniques Office, Defense Advanced Research Projects Agency (DARPA) (1976-82); Vice president of Digital Information Services, MCI (1982–1986); Vice President, Corporation for National Research Initiatives (CNRI) (1986-1994); Senior Vice President, MCI (1994-2005); Vice president and Chief Internet Evangelist, Google, (from 2005).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Silver Medal of the International Telecommunications Union (1995); US National Medal of Technology (1997); IEEE Alexander Graham Bell Medal (1997); Marconi&nbsp;Award (1998); Fellow of the Computer History Museum (2000);Charles Stark Draper Prize (2001); ACM Turing Award (2004); US Presidential Medal of Freedom (2005); Japan Prize (2008); Harold Pender Award (2010); elected president of the ACM (2012-2014);&nbsp;<span style=""line-height: normal;"">Queen Elizabeth Prize for Engineering</span>&nbsp;(2013);&nbsp;?awarded Officer of the French Légion d'honneur (2014);&nbsp;Cerf<span style=""line-height: 1.3;""> has also been named a Fellow of the following organizations: IEEE, Association for Computing Machinery (ACM), American Academy of Arts &amp; Sciences (</span>AAAS<span style=""line-height: 1.3;"">), American Association for the Advancement of Science (</span>AAAS<span style=""line-height: 1.3;"">), British Computer Society, and National Academy of Engineering (</span>NAE<span style=""line-height: 1.3;"">).</span></p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81385601636","Vinton (“Vint”) Gray Cerf","<li class=""bibliography""><a href=""/bib/cerf_1083211.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/cerf_1083211.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/cerf_1083211.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/cerf_1083211.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179180-668","https://amturing.acm.org/award_winners/newell_3167755.cfm","In joint scientific efforts extending over twenty years, initially in collaboration with J. C. Shaw at the RAND Corporation, and subsequentially with numerous faculty and student collegues at Carnegie-Mellon University, Newell and co-recipient Herbert A. Simon made basic contributions to artificial intelligence, the psychology of human cognition, and list processing.","<p><strong>Allen Newell was born in San Francisco on March 19, 1927 to Robert R. Newell, a prominent professor of radiology at Stanford Medical School, and Jeanette La Valley Newell.</strong> While Newell did not follow his father into medicine, he admired him greatly, and he certainly inherited his father’s taste for research and his broad intellectual interests. While at Lowell High School in San Francisco, Newell met Noël McKenna; the two married in 1947, when both were 20, and remained married until Newell’s death from cancer in 1992.</p>
<p>Professionally, Newell is chiefly remembered for his important contributions to artificial intelligence research, his use of computer simulations in psychology, and his inexhaustible, infectious energy. His central goal was to understand the cognitive architecture of the human mind and how it enabled humans to solve problems. His remarkable accomplishments in computer science were all means to this end, whether they were the development (with <a href=""/award_winners/simon_1031467.cfm"">Herbert Simon</a> and <a href=""https://en.wikipedia.org/wiki/Cliff_Shaw"" target=""_blank"">J.C. Shaw</a>) of the first list-processing language (<a href=""https://en.wikipedia.org/wiki/Information_Processing_Language"" target=""_blank"">IPL</a>) and of programs designed to use heuristics in solving problems (especially the <a href=""https://en.wikipedia.org/wiki/Logic_Theorist"" target=""_blank""><em>Logic Theorist</em></a> and <a href=""https://en.wikipedia.org/wiki/General_Problem_Solver"" target=""_blank""><em>General Problem Solver</em></a>, also developed with Simon and Shaw), or advances in speech recognition and in human-computer interaction. For Newell, the goal was to make the computer into an effective tool for simulating human problem-solving. A computer program that solved a problem in a way that humans did not, or could not, was not terribly interesting to him, even if it solved that problem “better” than humans did. The hope was to develop programs that succeeded at solving problems when and how humans succeeded and failed when and how humans failed.</p>
<p>Newell’s career in science initially began differently. After a stint in the Navy, during which he assisted his father in mapping radiation distribution after the Bikini Atoll atomic tests, Newell re-enrolled at Stanford, majoring in physics. While at Stanford, he took several courses with <a href=""https://en.wikipedia.org/wiki/George_P%C3%B3lya"" target=""_blank"">George Polya</a>, one of the leading exponents of heuristic problem-solving in mathematics (explained best in Polya’s 1945 book, <em>How to Solve It</em>). The idea of heuristic problem-solving made a great impression on Newell, who realized that humans have neither the time nor the processing power necessary to solve problems using exhaustive algorithmic methods. Rather, humans must use simplified rules—heuristics—to guide selective searches for solutions.&nbsp;</p>
<p>Newell was excited by the power and elegance of mathematics, and in 1949 he left Stanford to begin graduate work in mathematics at Princeton. There Newell worked as a research assistant for <a href=""https://en.wikipedia.org/wiki/Oskar_Morgenstern"" target=""_blank"">Oskar Morgenstern</a>, who had just recently co-authored <em>The Theory of Games and Economic Behavior </em>with <a href=""https://en.wikipedia.org/wiki/John_von_Neumann"" target=""_blank"">John von Neumann</a>, creating the new field of game theory. This stay at Princeton was brief, only one year, and Newell did not become a game theorist or a pure mathematician, but his experiences and contacts there started him on a new path, one that connected the powerful abstractions of formal mathematics with the messy realities of empirical experience.</p>
<p>The first step on this new path was to join John Williams (another Princeton mathematician) in the mathematics division at the newly created <a href=""https://en.wikipedia.org/wiki/RAND_Corporation"" target=""_blank"">RAND Corporation</a> in Santa Monica, CA. At RAND, the Air Force’s “think tank”, Newell’s first work applied game-theoretic methods to organization theory—and organizational reality—resulting in a pair of reports co-authored with <a href=""https://en.wikipedia.org/wiki/Joseph_Kruskal"" target=""_blank"">Joseph Kruskal</a>. This work led to his involvement with a series of experiments on decision-making in groups being conducted by John L. Kennedy, William Biel, and Robert Chapman at RAND’s Systems Research Center (which was spun off almost a decade later as the Systems Development Corporation).</p>
<p>One of the chief problems in the study of human behavior is the difficulty of creating a true controlled experiment. At the Systems Research Center, Kennedy, Biel, and Chapman sought to create a simulated environment (a model air defense control center) that could be controlled so as to give insight into how people working in this environment interacted with each other, their machines, and with the information presented to them. Newell’s specific task for the group was to use a computer (an <a href=""https://en.wikipedia.org/wiki/IBM_CPC"" target=""_blank"">IBM Card Programmed Calculator</a>, then quickly becoming a dinosaur in the new age of stored-program machines) to create simulated radar maps. In the process, Newell became fascinated with how people in this environment processed information and made decisions. In addition, experience taught him to think of computers as symbol processors and simulators rather than as big, fast calculators. Symbol processing, decision-making, problem-solving, and simulation thus went together for Newell, leading him to think of minds, computers, experiments, and organizations in new ways.</p>
<p>One of the consultants to the Systems Research Center was Herbert Simon, then a professor at Carnegie Institute of Technology’s new Graduate School of Industrial Administration. Newell and Simon met during Simon’s visit to RAND during the summer of 1952 and immediately discovered that they spoke the same language of symbols, problem-solving, heuristics, and simulations. At the time, they both focused on decision-making in organizations, and Newell saw the computer as a tool for simulating experimental environments, not experimental subjects. It was not until 1954, after attending a seminar at RAND with <a href=""https://en.wikipedia.org/wiki/Oliver_Selfridge"" target=""_blank"">Oliver Selfridge</a>, that Newell had his “conversion experience,” suddenly seeing the possibilities of using computers to simulate human problem-solving.<a href=""#_ftn1"" name=""_ftnref1"" title=""""><sup><sup>[1]</sup></sup></a> Indeed, to Newell, the analogy between humans and computing machines was tight: problem-solving was something done by “physical symbol systems,” a category that included both humans and computers as separate species of the same genus.</p>
<p>After several years of collaboration at a distance (and in person during summers), Newell moved to Pittsburgh to work with Simon at Carnegie Tech in early 1955. While nominally Simon’s PhD student, Newell was in truth an equal partner in their growing research program. The first fruits of their collaboration was the first successful Artificial Intelligence program, the <em>Logic Theorist </em>(<em>LT</em>), completed in late 1955 and first run on a computer in 1956, which was used to prove the theorems of Russell and Whitehead’s <a href=""https://en.wikipedia.org/wiki/Principia_Mathematica"" target=""_blank""><em>Principia Mathematica</em></a><em>. </em>In a wonderful twist of irony, Newell and Simon first used Simon’s family to simulate the workings of the <em>Logic Theorist </em>before it was programmed into a computer, so they had people simulate the workings of a machine designed to simulate the workings of people’s minds!</p>
<p>In addition to employing principles of heuristic problem-solving, the <em>Logic Theorist</em> was an error-controlled, feedback “machine” that compared the goal state (the statement to prove) with the current state and performed one of a small set of basic operations in order to reduce the difference between the two states. The <em>Logic Theorist</em> was a remarkable success, and Simon, Newell, and Shaw elaborated on its basic principles in creating another renowned program, The<em> General Problem Solver</em> (<em>GPS</em>) in 1957-1958. The <em>GPS</em> was not quite so universal as its name implied, but it was startlingly good at solving certain kinds of well-defined problems. Even more, <em>GPS</em>, like <em>LT, </em>appeared to solve them in much the same ways that humans did, employing a core method of means-ends analysis that was both simple and general, if not quite universal.</p>
<p>As part of this work on cognitive simulation, Newell, Simon, and Shaw developed the first list-processing language, IPL, which, according to Simon “introduced many ideas that have become fundamental for computer science in general, including lists, associations, schemas (frames), dynamic memory allocation, data types, recursion, associative retrieval, functions as arguments, and generators (streams).”<a href=""#_ftn2"" name=""_ftnref2"" title=""""><sup><sup>[2]</sup></sup></a> <a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy’s</a> LISP, which became the standard language in the AI community after its development in 1958, incorporated these basic principles of IPL in a language with an improved syntax and a “garbage collector” that recovered unused memory.</p>
<p>Newell collaborated with Simon for a number of years, but their paths did diverge over time, with Newell working on speech recognition, computer architectures, and human-computer interaction during the late 60s and 70s before turning his focus to his “<a href=""https://en.wikipedia.org/wiki/Soar_%28cognitive_architecture%29"" target=""_blank"">Soar</a>” project from the 1970s until his death in 1992. The Soar project was Newell’s attempt to develop a unified theory of cognition (<em>a</em> unified theory, not <em>the</em> unified theory, he would note). This unified theory centered on problem-solving, which it described in terms of “production systems<em>”</em> (sets of “if-then” statements), and it incorporated a theory of learning by “chunking” into the problem-solving schema<sup><a href=""#_ftn3"" name=""_ftnref3"" title="""">[3]</a></sup>. While the Soar architecture is informed by psychological and neuro-scientific data, it is a structural model of human cognition than does not seek to explain how this structure is realized physically in the brain. The Soar project continues today, long after Newell’s death, fulfilling Newell’s maxim that one should “choose a final project that will outlast you.”</p>
<p>Newell, like Simon, was an accomplished institution-builder and grant-winner as well as a profound thinker. In these administrative capacities, Newell was instrumental in transforming Carnegie Mellon’s (CMU) Psychology Department into one of the most influential in the United States, in creating CMU’s pioneering School of Computer Science, and in creating CMU’s campus-wide computer network (one of the first in the nation) in 1982.</p>
<p>For all these labors, intellectual and institutional, Newell received a great many honors, including the Harry Goode Award of the American Federation of Information Processing Societies (1971); the A.M. Turing Award of the Association of Computing Machinery (in 1975, with Simon); the Distinguished Scientific Contribution Award of the American Psychological Association (1985); and, just before he died, the National Medal of Science (1992). He was survived by his wife, Noël, his son Paul, and his sister Ann.</p>
<p>For further information on Newell, see the following, especially the pieces by Piccinini and Simon, on which I have drawn heavily for this essay.</p>
<p style=""margin-left:28.35pt;"">Boden. Margaret, <em>Mind as Machine: A History of Cognitive Science</em>, Oxford University Press, 2006.</p>
<p style=""margin-left:28.35pt;"">Laird, John E. and Paul S. Rosenbloom. “The Research of Allen Newell.”<em> AI Magazine</em>, Vol. 13, Num. 4 (1992), pp. 17-45.</p>
<p style=""margin-left:28.35pt;"">Michon, John and Aladin Akyurek, eds.<em> Soar: A Cognitive Architecture in Perspective</em>, Kluwer Academic, Norwell, MA, 1992.</p>
<p style=""margin-left:28.35pt;"">Piccinini, Gualtiero, “Allen Newell.” In the <em>New Dictionary of Scientific Biography</em>, Thomson Gale.</p>
<p style=""margin-left:28.35pt;"">Herbert A. Simon,<em> “</em>Allen Newell: 1927-1992.”<em> IEEE Annals of the History of Computing</em>, Vol. 20, Num. 2 (1998), pp. 63-76<em>.</em></p>
<p style=""margin-left:28.35pt;"">Steier, David and Tom M. Mitchell, eds.<em> Mind Matters: A Tribute to Allen Newell</em>, Lawrence Eribaum Associates, Mahwah, N.J., 1996.</p>
<p align=""right""><span class=""callout"">Author: Hunter Heyck</span></p>
<div><br clear=""all"">
<hr align=""left"" size=""1"" width=""33%"">
<p><a href=""#_ftnref1"" name=""_ftn1"" title=""""><sup><sup>[1]</sup></sup></a> Simon, Herbert A<em>. “</em>Allen Newell: 1927-1992,”<em> IEEE Annals of the History of Computing</em>,<em> </em>Vol.<em> </em>20, Num. 2 (1998), pp. 63-76<em>. </em></p>
<div id=""ftn2"">
<p><a href=""#_ftnref2"" name=""_ftn2"" title=""""><sup><sup>[2]</sup></sup></a> Ibid.</p>
</div>
<div id=""ftn3"">
<p><sup><a href=""#_ftnref3"" name=""_ftn3"" title="""">[3]</a></sup> Learning by chunking means that when Soar solves a problem, it creates a new production system that links condition to result; the next time Soar has to deal with the same problem, the system follows the link from condition to result without having to solve the problem again.</p>
</div>
</div>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/newell_3167755.cfm""><img src=""/images/lg_aw/3167755.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Allen Newell ""></a>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>March 19, 1927, San Francisco, CA.</p>
<h6><span class=""label"">DEATH: </span></h6>
<p>July 19, 1992, Pittsburgh, Pennsylvania.</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>BS, Physics (Stanford University, 1949); PhD (Carnegie Institute of Technology, Graduate School of Industrial Administration, 1957.</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>US Navy (1943-45); RAND Corporation (1950-1961); Carnegie-Mellon University (1955-1992:graduate student, 1955-57, Professor of Computer Science and Psychology, 1961-1992)</p>
<h6><span class=""label"">HONORS AND AWARDS:</span></h6>
<p>Harry Goode Memorial Award, American Federation of Information Processing Societies (1971); elected to the United States National Academy of Science. (1972); elected Fellow of the American Academy of Arts and Sciences (1972); A.M. Turing Award of the ACM (1975); Alexander C. Williams Jr. Award (with William C. Biel, Robert Chapman and John L. Kennedy), Human Factors Society (1979); elected to the United States National Academy of Engineering (1980); First President, American Association for Artificial Intelligence (1980); IEEE Computer Society Computer Pioneer Award (1981, charter recipient); Louis E. Levy Medal from the Franklin Institute (1982); Distinguished Scientific Contribution Award, American Psychological Association (1985); Honorary doctorate awarded from University of Pennsylvania (1986); William James Lectures, Harvard University (1987); Award for Research Excellence, International Joint Conference on Artificial Intelligence (1989); Doctor in the Behavioral and Social Sciences (Honorary), University of Gröningen, The Netherlands (1989); William James Fellow Award, American Psychological Society (1989, charter recipient); IEEE Emanuel Piore Award (1990); U.S. National Medal of Science (1992); The ACM/AAAI Allen Newell Award was named in his honor, as well as the Award for Research Excellence of the Carnegie Mellon School of Computer Science.</p>","","https://dl.acm.org/author_page.cfm?id=81100393604","Allen Newell","<li class=""bibliography""><a href=""/bib/newell_3167755.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283930&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/newell_3167755.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/newell_3167755.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179809-710","https://amturing.acm.org/award_winners/liskov_1108679.cfm","","<p><strong>Barbara Liskov, née Barbara Jane Huberman, was born on November 7, 1939, in California.</strong> She earned her BA in mathematics at the University of California, Berkeley in 1961. Rather than go directly to graduate school, she took a job at the Mitre Corporation where she learned that she was a natural at computer programming. After a year at Mitre, she moved to Harvard to work on computer translation of human languages.</p>
<p>Returning to California to do graduate work at Stanford, she was given financial support in <a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy’s</a> lab partly because her earlier work on natural language translation was in the general area of artificial intelligence. In 1968 she became one of the first women in the United States to be awarded a computer science PhD. Her thesis on chess end-games was supervised by John McCarthy.</p>
<p>After receiving her PhD, Barbara married Nathan Liskov and moved back to the Boston area to work at the Mitre Corporation in Bedford, MA on computer design and operating systems. Using an Interdata 3 computer that had the ability to change the instruction set via microcode, she created the “Venus Computer” tailored to supporting the construction of complex software. The <a href=""https://dl.acm.org/citation.cfm?id=361272"" target=""_blank"">Venus operating system</a> was a small timesharing system for the Venus machine used to experiment with how different architectures helped or hindered this process. The Venus system supported 16 teletypes and each user was connected to a virtual machine so that major errors would not compromise the entire system, only the virtual machine for that user.</p>
<p>In 1971, shortly after finishing her experiments with Venus and presenting a conference paper on the topic, Liskov was urged by another attendee to consider a position at MIT. She left Mitre and joined the MIT faculty as a professor in the Laboratory for Computer Science. Building on her experience at the Mitre Corporation, her research has focused on creating more reliable computer systems.</p>
<p>At MIT she led the design and implementation of the <a href=""https://en.wikipedia.org/wiki/CLU_%28programming_language%29"" target=""_blank"">CLU</a> programming language, which emphasized the notions of modular programming, data abstraction, and polymorphism. These concepts are a foundation of object-oriented programming used in modern computer languages such as Java and C#, although many other features of modern object oriented programming are missing from this early language.</p>
<p>Her MIT group also created the <a href=""https://en.wikipedia.org/wiki/Argus_%28programming_language%29"" target=""_blank"">Argus</a> language, which extended the ideas of CLU to ease implementation of programs distributed over a network, including support for nested transactions. An example of such a distributed program might be a network based banking system. Argus provided object abstractions called “guardians” that encapsulate related procedures. As an experimental language, Argus influenced others developers but was never widely adopted or used for deployed networked applications.</p>
<p>Liskov's subsequent work has mainly been in the area of distributed systems, which use several computers connected by a network. Her research has covered many aspects of operating systems and computation, including important work on object-oriented database systems, garbage collection, caching, persistence, recovery, fault tolerance, security, decentralized information flow, modular upgrading of distributed systems, geographic routing, and practical Byzantine fault tolerance. Many of these, like Byzantine fault tolerance, deal with situations where a complex system fails in arbitrary ways. Liskov developed methods to allow correct operation even when some components are unreliable. With Jeannette Wing she developed a new notion of subtyping, known as the <a href=""https://en.wikipedia.org/wiki/Liskov_substitution_principle"" target=""_blank"">Liskov substitution principle</a>. Her contributions have influenced advanced system developments and set a standard for clarity and usefulness.</p>
<p>Liskov is currently the Ford Professor of Engineering at MIT. She leads the Programming Methodology Group at MIT, with a current research focus in Byzantine fault tolerance and distributed computing. She became a full professor at MIT in 1980. She served as the Associate Head for Computer Science from 2001 to 2004, and in 2007 was appointed Associate Provost for Faculty Equity. In 2008, MIT named her an Institute Professor, the highest honor awarded to an MIT faculty member.</p>
<p>""Barbara is revered in the MIT community for her role as scholar, mentor and leader,"" said MIT President Susan Hockfield. ""Her pioneering research has made her one of the world's leading<br>
authorities on computer language and system design. In addition to her seminal scholarly contributions, Barbara has served MIT with great wisdom and judgment in several administrative roles, most recently as Associate Provost for Faculty Equity.""</p>
<p>She has supervised the research programs of more than twenty PhD students and large numbers of MSc students.</p>
<p>Her son Moses Liskov was awarded a PhD in computer science by MIT in 2004, and is now a professor of computer science at the College of William and Mary.</p>
<h4>Web sites</h4>
<p>Professor Liskov's home page,<br>
<a href=""http://www.pmg.csail.mit.edu/~liskov/"" target=""_blank"">http://www.pmg.csail.mit.edu/~liskov/</a><br>
Biography of Prof. Liskov by Prof. John Guttag, in a book about MIT's EECS department,<br>
<a href=""https://www.eecs.mit.edu/spotlights/liskov_jvg-bio.html"" target=""_blank"">http://www.eecs.mit.edu/spotlights/liskov_jvg-bio.html</a><br>
Article on Professor Liskov in Technology Review (MIT),<br>
<a href=""https://www.technologyreview.com/article/24108/page1/"" target=""_blank"">http://www.technologyreview.com/article/24108/page1/</a></p>
<p style=""text-align: right;""><span class=""callout"">Author: Tom van Vleck</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/liskov_1108679.cfm""><img src=""/images/lg_aw/1108679.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Barbara Liskov""></a>
<br><br>
<h6 class=""label""><a href=""/photo/liskov_1108679.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label""></h6>
<hr>
<h6 class=""label"">BIRTH:</h6>
<p>November 7, 1939, California.</p>
<h6 class=""label"">EDUCATION:</h6>
<p>BSc in Mathematics, University of California, Berkeley (1961); PhD in computer science, Stanford University (1968)</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Mitre Corporation, (1968-1972); MIT (1972 onwards, 2001-2004 as Associate Department Head and later as Associate Provost)</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Member, National Academy of Engineering (1989); Fellow, American Academy of Arts and Sciences (1992); Fellow, ACM 1(996); Society of Women Engineers Achievement Award (1996); IEEE John Von Neumann medal (2004); MIT Institute Professor (2008); ACM SIGPLAN Programming Languages Lifetime Achievement Award (2008); ACM SIGSOFT Impact Paper Award (2008); ACM A. M. Turing Award (2009); CMU Katayanagi Prize for Research Excellence in Computer Science (2011).<br>
Honorary Doctorates: ETH, Zurich, Switzerland (2005); Northwestern University, Chicago (2011); University of Lugano, Switzerland (2011).<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100323833","Barbara Liskov","<li class=""bibliography""><a href=""/bib/liskov_1108679.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/liskov_1108679.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/liskov_1108679.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/liskov_1108679.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/liskov_1108679.cfm""><span></span>Video Interview</a></li>"
"1573178967-652","https://amturing.acm.org/award_winners/bengio_3406375.cfm","For conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.","","<div class=""featured-photo"">
<a href=""/award_winners/bengio_3406375.cfm""><img src=""/images/lg_aw/3406375.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Yoshua Bengio""></a>
</div>","","https://dl.acm.org/author_page.cfm?id=81100287057","Yoshua Bengio","<li class=""bibliography""><a href=""/bib/bengio_3406375.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""key-words""><a href=""/keywords/bengio_3406375.cfm""><span></span>Research<br> Subjects</a></li>"
"1573178870-646","https://amturing.acm.org/award_winners/hamming_1000652.cfm","For his work on numerical methods, automatic coding systems, and error-detecting and error-correcting codes.","<p><strong>Richard Hamming was born in Chicago, Illinois, USA on February 11, 1915, the son of Richard J. Hamming and Mabel G. Redfield.</strong> He was brought up in Chicago where he attended school and realized that he was a more able mathematician than his teacher. He wanted to study engineering but the only offer of a scholarship came from the University of Chicago, which had no engineering department. He entered the University of Chicago receiving his B.S. in mathematics.</p>
<p>After his undergraduate studies he went to the University of Nebraska, where he was awarded an M.A. in 1939. He received a Ph.D. in mathematics in 1942 from the University of Illinois at Urbana-Champaign. His doctoral dissertation, <em>Some Problems in the Boundary Value Theory of Linear Differential Equations,</em> was supervised by Waldemar Trjitzinsky (1901-1973). Hamming, however, developed interests in ideas that were quite far removed from his study of differential equations when he discovered George Boole's <a href=""https://www.gutenberg.org/ebooks/15114"" target=""_blank""><em>An Investigation of the Laws of Thought.</em></a> He found Boole's book interesting, relevant, and believable. The ideas in it would prove highly significant later in his life when he became interested in coding theory.</p>
<p>After earning his doctorate, Hamming married Wanda Little on September 5, 1942. He taught first at the University of Illinois, and then at the J. B. Speed Scientific School of the University of Louisville. In 1945, encouraged by a friend, he joined the Manhattan Project, a U.S. government research project to produce an atomic bomb at Los Alamos, New Mexico. A month after he arrived at Los Alamos he was joined by his wife, who was also employed on the Manhattan Project. Hamming was put in charge of the IBM calculating machines that played a vital role in the project. He came in contact with many leading scientists, including Richard Feynman, Enrico Fermi, Edward Teller and J. Robert Oppenheimer. The theoretical physicist Hans Bethe was his boss. Wanda Hamming began by doing computations with desk calculators, and later worked for Enrico Fermi and Edward Teller.</p>
<p>After the Manhattan Project ended Hamming remained at Los Alamos for six months, writing up details of the calculations they had done. He felt that it was important to try to understand exactly what had been achieved, and why it had been so successful. It was at this time that he realized that he had done the right thing by not studying engineering; engineers did much of the routine work, but mathematicians like himself were more critical to the cutting edge innovations. He formed a view of mathematics, arising from his Los Alamos experience, that computation was of major importance, but it made him skeptical of the standard approach that emphasized formal abstract mathematical theories.</p>
<p>In 1946 he accepted a position in the mathematics department at the Bell Telephone Laboratories in New Jersey. However, he didn't entirely break his link with Los Alamos Scientific Laboratories, and made two week visits each summer as a consultant.</p>
<p>At Bell Labs he was able to work with both Claude Shannon, with whom he shared an office, and John Tukey. Some other young mathematicians had joined the Mathematical Research Department at Bell Labs just prior to Hamming. These included Donald Percy Ling and Brockway McMillan, who had been at Los Alamos at the same time as Hamming. Shannon, Ling, McMillan and Hamming called themselves the <em>Young Turks</em>. Hamming often related how they had all been affected by growing up in the depression, and all learned new skills with their war work. It led them, he said, to do unconventional things in unconventional ways. Hamming, for example, lunched with the physics group rather than his mathematics group, and they were fascinated by his unorthodox ideas and views. Not all his colleagues were happy to tolerate his unconventional ways. Some have described him as egotistical, saying he sometimes went off ""half-cocked, after some half-baked idea."" Unconventional ideas sometimes produce flashes of brilliance, but they sometimes also lead to failures.</p>
<p>Before discussing Hamming's highly significant work on error-correcting codes, we first note the many and varied problems he worked on in Bell Labs. These include problems involving design of telephone systems, traveling wave tubes, the equalization of television transmission lines, the stability of complex communication systems, and the blocking of calls through a telephone central office. He continued to work for Bell Telephones until 1976, although he became increasingly interested in teaching, and held visiting or adjunct professorships at Stanford University, the City College of New York, the University of California at Irvine and Princeton University between 1960 and 1976. After retiring from Bell Labs in 1976, he became a professor of computer science at the Naval Postgraduate School at Monterey, California. At this point he gave up his research career and concentrated on teaching and writing books. He believed that the way mathematics was being taught was wrong, and that the only way to change it was to write textbooks with a new approach. Here are two examples of his views on mathematics teaching:</p>
<p style=""margin-left: 40px;""><span class=""callout"">We live in an age of exponential growth in knowledge, and it is increasingly futile to teach only polished theorems and proofs. We must abandon the guided tour through the art gallery of mathematics, and instead teach how to create the mathematics we need. In my opinion, there is no long-term practical alternative.</span></p>
<p>and</p>
<p style=""margin-left: 40px;""><span class=""callout"">The way mathematics is currently taught it is exceedingly dull. In the calculus book we are currently using on my campus, I found no single problem whose answer I felt the student would care about! The problems in the text have the dignity of solving a crossword puzzle - hard to be sure, but the result is of no significance in life.</span></p>
<p>His attempt to move to a new way of teaching calculus is exhibited in his 1985 book <em>Methods of Mathematics Applied to Calculus, Probability, and Statistics</em> [<a href=""/bib/hamming_1000652.cfm#link_8"">8</a>]. He said that the book is ""very different from the standard texts and its success or failure will tell us something about the prospects for change and innovation."" Other texts he wrote all attempted to change conventional approaches to the areas they studied.</p>
<p>Richard Hamming is best known for his work at Bell Labs on error-detecting and error-correcting codes. His fundamental paper on this topic, <em>Error detecting and error correcting codes</em> [<a href=""/bib/hamming_1000652.cfm#link_1"">1</a>], appeared in April 1950 in the <em>Bell System Technical Journal</em>. This paper created an entirely new field within information theory. <em>Hamming codes, Hamming distance and Hamming metric,</em> standard terms used today in coding theory and other areas of mathematics, all originated in this classic paper and are of ongoing practical use in computer design. Details can be found <a href=""/info/hamming_1000652.cfm"">here</a>.</p>
<p>In 1956 Hamming worked on the IBM 650, an early vacuum tube, drum memory, computer. His work led to the development of a rudimentary programming language. Hamming also worked on numerical analysis, especially integration of differential equations. The <a href=""https://en.wikipedia.org/wiki/Window_function"" target=""_blank"">Hamming spectral window</a>, still widely used in computation, is a special type of digital filter designed to pass certain frequencies and discriminate against closely related frequencies.</p>
<p>In addition to the Turing Award, Hamming received many awards for his pioneering work. He was made a fellow of the Association for Computing Machinery in 1994. The Institute of Electrical and Electronics Engineers (IEEE) awarded him the Emanuel R Piore Award in 1979.</p>
<p>The IEEE created ""The Richard W. Hamming Medal"" in his honor. He was the first recipient of this $10,000 prize medal in 1988. He was elected a member of the National Academy of Engineering in 1980, and received the Harold Pender Award from the University of Pennsylvania in 1981. In 1996, in Munich, Hamming received the prestigious $130,000 Eduard Rheim Award for Achievement in Technology for his work on error correcting codes.</p>
<p>In 1997 Hamming retired from teaching at the Naval Postgraduate School and was made Distinguished Professor Emeritus. Shortly before he retired, he said that when he left Bell Labs, he knew that that was the end of his research career. It really would be the end, he said, when he retired from teaching. Indeed he was right, for having taught up to December 1997, he died of a heart attack in the following month. Richard Franke of the Naval Postgraduate School at Monterey wrote of Richard Hamming:</p>
<p style=""margin-left: 40px;""><span class=""callout"">He will be long remembered for his keen insights into many facets of science and computation. I'll also long remember him for his red plaid sport coat and his bad jokes.</span></p>
<p style=""text-align: right;""><span class=""callout"">Author: Edmund F. Robertson</span><br>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/hamming_1000652.cfm""><img src=""/images/lg_aw/1000652.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Richard W. Hamming""></a>
</div>
<h5><a href=""/photo/hamming_1000652.cfm""><img alt="""" src=""/images/misc/bhlight.jpg"" style=""float: left;""></a>&nbsp; <a href=""/photo/hamming_1000652.cfm"">Photo-Essay</a><br>
&nbsp;</h5>
<h6 class=""label"">BIRTH:</h6>
<p>11 February 1915, Chicago, Illinois, USA</p>
<h6 class=""label"">DEATH:</h6>
<p>7 January 1998, Monterey, California,&nbsp;USA</p>
<h6 class=""label"">EDUCATION:</h6>
<p>BS University of Chicago, Chicago, USA (1934 - mathematics); MA University of Nebraska, Lincoln, Nebraska, USA (1939); PhD University of Illinois, Urbana-Champaign, Illinois, USA (1942 - mathematics).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Instructor in Mathematics, University of Illinois, Urbana-Champaign, Illinois 1942-44; Assistant Professor, J.B. Speed Scientific School, University of Louisville, Louisville, Kentucky1944-45; Manhattan project, Los Alamos, New Mexico, 1945-46; Bell Telephone Laboratories, 1946-76; Member of the faculty Naval Postgraduate School, Monterey, California, 1976-97; Adjunct Professor of Computer science Naval Postgraduate School, Monterey, California, 1976-97.</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Fellow, Institute of Electrical and Electronics Engineers; IEEE Piore award 1979; $10,000 prize medal ""The Richard W. Hamming Medal"" named in his honor 1986 - 1st recipient of same 1988); Fellow, Association of Computing Machinery, 1994; Turing Award 1968; Member, National Academy of Engineering (1980); Harold Pender Award, University of Pennsylvania, 1981; Eduard Rheim Foundation Prize, 1996.</p>","","https://dl.acm.org/author_page.cfm?id=81100153796","Richard W. Hamming","<li class=""bibliography""><a href=""/bib/hamming_1000652.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283923&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/hamming_1000652.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/hamming_1000652.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179011-655","https://amturing.acm.org/award_winners/scott_1193622.cfm","Along with Michael O. Rabin, for their joint paper ""Finite Automata and Their Decision Problem,"" which introduced the idea of nondeterministic machines, which has proved to be an enormously valuable concept. Their (Scott & Rabin) classic paper has been a continuous source of inspiration for subsequent work in this field.","<p>Dana Scott is an internationally recognized mathematical logician whose work has spanned computer science, mathematics, and philosophy. He made seminal contributions to <a href=""https://en.wikipedia.org/wiki/Automata_theory"" target=""_blank"">automata theory</a>, <a href=""https://en.wikipedia.org/wiki/Modal_logic"" target=""_blank"">modal logic</a>, <a href=""https://en.wikipedia.org/wiki/Model_theory"" target=""_blank"">model theory</a>, <a href=""https://en.wikipedia.org/wiki/Set_theory"" target=""_blank"">set theory</a>, and the <a href=""https://en.wikipedia.org/wiki/Programming_language_theory"" target=""_blank"">theory of programming languages</a>. He has made fundamental contributions to contemporary logic and is known for his creation of <a href=""https://en.wikipedia.org/wiki/Domain_theory"" target=""_blank"">domain theory</a>, a branch of mathematics that is essential for analyzing computer programming languages.</p>
<p>Scott’s work is highly theoretical and a full description of it is not possible in this limited space. Perhaps an alternate is the description of his interests contained in the introduction to his Turing Award Lecture:</p>
<p style=""margin-left:.5in;"">Logic has been long interested in whether answers to certain questions are computable in principle, since the outcome puts bounds on the possibilities of formalization. More recently, precise comparisons in the efficiency of decision methods have become available through the developments in complexity theory. These, however, are applications to logic, and a big question is whether methods of logic have significance in the other direction for the more applied parts of computability theory. Programming languages offer an obvious opportunity as their syntactic formalization is well advanced; however, the semantical theory can hardly be said to be complete. Though we have many examples, we have still to give wide-ranging mathematical answers to these queries: What is a machine? What is a computable process? How (or how well) does a machine simulate a process? Programs naturally enter in giving descriptions of processes.</p>
<p style=""margin-left:.5in;"">The definition of the precise meaning of a program then requires us to explain what are the objects of computation (in a way, the statics of the problem) and how they are to be transformed (the dynamics). So far the theories … have formalized only a portion of the field, and there has been perhaps too much concentration on the finite-state and algebraic aspects. It would seem that the understanding of higher-level program features involves us with infinite objects and forces us to pass through several levels of explanation to go from the conceptual ideas to the final simulation on a real machine. These levels can be made mathematically exact if we can find the right abstractions to represent the necessary structures.</p>
<p>Born October 11, 1932, in Berkeley, California, Scott attended the University of California, Berkeley, studying philosophy and logic. He was adept at his studies and rapidly began to attend classes and seminars at the graduate level. Graduating with a BA in 1954, he moved to Princeton University and completed a PhD under the supervision of Alonzo Church in 1958.</p>
<p>After completing his Ph.D. studies, he took an appointment at the University of Chicago, as an instructor and remained there until 1960. In 1959, while attending a summer session for promising mathematicians, he met <a href=""/award_winners/rabin_9681074.cfm""><u>Michael Rabin</u></a>&nbsp;and the two of them did work leading to their publishing of the paper (<em>Finite Automata and Their Decision Problem</em>) [<a href=""/bib/scott_1193622.cfm#bib_1"">1</a>]&nbsp; that resulted in them both receiving the Turing Award. This paper introduced the concept of <a href=""https://en.wikipedia.org/wiki/Non-deterministic_Turing_machine"" target=""_blank"">nondeterministic machines</a> in which, unlike the standard Turing Machine, there can be several different possible “instructions” that might be executed at each step of the program.</p>
<p>Computational complexity theory is the study of what is possible to calculate given a specific set of resources such as time (number of steps), memory available, limits on communication, the number of processors, etc. Rather than focusing on a single algorithm and analyzing its requirements, computational complexity theory examines all possible algorithms for a specific task and assigning these tasks to different categories of complexity in terms of resources that must be used. As the Turing Award citation indicates, Scott and Rabin’s concept of nondeterministic machines has proved extremely productive in this research area.</p>
<p>Scott is not only known for his work in complexity, but is also well regarded for his research into the study of program properties and language definitions—usually known by the terms <a href=""https://en.wikipedia.org/wiki/Semantics_%28computer_science%29"" target=""_blank""><em>semantics of programming languages</em></a> or <a href=""https://en.wikipedia.org/wiki/Denotational_semantics"" target=""_blank""><em>denotational semantics</em></a>. After working at the University of California, Berkley, Stanford University, and Princeton University, Scott moved to take the position of Professor of Mathematical Logic at Oxford University in 1972. While there he worked closely with <a href=""https://en.wikipedia.org/wiki/Christopher_Strachey"" target=""_blank"">Christopher Strachey</a> to provide a mathematical foundation for the semantics of programming languages. This Scott-Strachey semantics, as it was initially called, has proved to be one of the most influential works in theoretical computer science. One of Scott’s major contributions was the theoretical work that allowed the difficult subjects of loops and recursive functions to be included into this denotational semantic structure. While it may seem elementary to the typical computer programmer (who uses these constructs in programs) the theoretical work behind the analysis of these constructs is anything but easy and it is some indication of Scott’s ability that he was able to develop both this semantics area and his earlier automata results—two major areas of theoretical computer science when one would be considered a grand achievement on its own.</p>
<p>Scott remained at Oxford for nine years but in 1981 was enticed back to America to accept the position of University Professor of Computer Science, Mathematical Logic, and Philosophy, at Carnegie Mellon University. In 1989 he took the prestigious position of Hillman Professor of Computer Science at the same institution and remained there until his retirement in 2003. Rather than resting on his laurels, he again tackled several difficult theoretical projects. He proposed the theory of equilogical spaces as a replacement for <a href=""https://en.wikipedia.org/wiki/Domain_theory"" target=""_blank"">domain theory</a> when attempting to define denotational semantics for programming languages, particularly functional languages. While difficult to describe in a few words, this move enabled him to once again broaden a narrow theoretical subject to one with much more power to describe subjects of interest to theoreticians. In particular it allowed the extension of earlier work by <a href=""https://en.wikipedia.org/wiki/Alfred_Tarski"" target=""_blank"">Alfred Tarski</a> to apply to programming languages as well as building on the work of Alonzo Church’s lambda calculus.</p>
<p>During his academic career he has supervised about 50 PhD students and numerous other graduate projects.</p>
<p>Further information on his life and particularly on his work can be found in his Turing Award Lecture (<a href=""https://dl.acm.org/ft_gateway.cfm?id=359826&amp;ftid=289770&amp;dwn=1&amp;CFID=93420280&amp;CFTOKEN=18004706"" target=""_blank"">here</a>) and in the works listed in his <a href=""/bib/scott_1193622.cfm""><u>selected bibliography</u></a>.</p>
<p>Quotations:</p>
<p style=""margin-left:.5in;"">“Learn as much as you can while you are young, since life becomes too busy later.”</p>
<p style=""margin-left:.5in;"">“Try to regard mathematics as an experimental science.</p>","<div class=""featured-photo"">
<a href=""/award_winners/scott_1193622.cfm""><img src=""/images/lg_aw/1193622.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Dana S Scott""></a>
</div>
<p>&nbsp;</p>
<h6 class=""label""><strong>BIRTH:</strong></h6>
<p>October 11, 1932, Berkeley, California, USA.<br>
&nbsp;</p>
<h6 class=""label""><strong>EDUCATION:</strong></h6>
<p>BA (University of California, Berkeley, 1954); PhD (Princeton University, 1958).</p>
<h6 class=""label""><br>
<strong>EXPERIENCE:</strong></h6>
<p>University of Chicago (Instructor,1958-1960); University of California, Berkeley (Assistant Professor of mathematics, 1960-1962; Associate Professor of mathematics, 1962-1963);, Stanford University (Associate Professor of logic and mathematics, 1963-1967, Professor of logic and mathematics, 1967-1969); University of Amsterdam (Visiting Professor of mathematics, 1968-1969); Princeton University (Professor of philosophy and mathematics, 1969-1972); Oxford University (Professor of mathematical logic, 1972-1981); Carnegie Mellon University (University professor of computer science, mathematical logic, and philosophy, 1981-1989, Hillman Professor of Computer Science, 1989-2003, emeritus since 2003); University of Linz, Austria (Osterreich University Professor, symbolic computation and logic,1992-1993).<br>
&nbsp;</p>
<h6 class=""label""><strong>HONORS AND AWARDS:</strong></h6>
<p>LeRoy P. Steele Prize, American Mathematical Society (1972); ACM Turing Award, with Michael Rabin (1976); Harold Pender Award, University of Pennsylvania (1990); Rolf Schock Prize in Logic and Philosophy, Royal Swedish Academy of Sciences (1997); Bolzano Medal for Merit in the Mathematical Sciences, Czech Academy of Sciences (2001); European Association for Theoretical Computer Science (EATCS) Award (2007); Russian Academy of Science’s Sobolev Institute of Mathematics Gold Medal (2009). He is a member of the American Academy of Arts and Sciences, British Academy, Finnish Academy of Sciences and Letters, New York Academy of Sciences, US National Academy of Sciences, Academia Europaea; and a fellow of the ACM.</p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100545006","Dana Stewart Scott","<li class=""bibliography""><a href=""/bib/scott_1193622.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283932&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/scott_1193622.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/scott_1193622.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179453-685","https://amturing.acm.org/award_winners/dijkstra_1053701.cfm","For fundamental contributions to programming as a high, intellectual challenge; for eloquent insistence and practical demonstration that programs should be composed correctly, not just debugged into correctness; for illuminating perception of problems at the foundations of program design.","<p>Edsger W. Dijkstra was born in 1930 in Rotterdam, The Netherlands. His father, a high-school chemistry teacher, served as president of the Dutch Chemical Society. His mother, who never held a formal job, had a lasting influence on his approach to mathematics and his emphasis on elegance.</p>
<p>Graduating from high school in 1948 and intending to become a theoretical physicist, Dijkstra thought the ability to use an electronic computer might be advantageous. Three years of programming at the Mathematical Center in Amsterdam convinced him that the intellectual challenge of programming exceeded that of theoretical physics, but where was the sound body of knowledge that could support programming as an intellectually respectable discipline? His boss, A. van Wijngaarden, persuaded him that in the years to come he could be one of the people to make programming a respectable discipline. Completing his study of physics as quickly as he could, Dijkstra forsook physics for programming.<br>
<br>
At the Mathematical Centre a major project was building the ARMAC computer. For its official inauguration in 1956, Dijkstra devised a program to solve a problem interesting to a nontechnical audience: Given a network of roads connecting cities, what is the shortest route between two designated cities? The best known algorithms had running times which grew as the cube of the network’s size; the running time of Dijkstra’s algorithm grew only as the square. Developed in 20 minutes while Dijkstra was relaxing on a café terrace with his fiancée, Maria (Ria) C. Debets, his Shortest Path algorithm is still used in such applications as packet-switching software for computer communications.<br>
<br>
Around the same time, Dijkstra invented another very efficient network algorithm for use in designing the <a href=""https://en.wikipedia.org/wiki/Electrologica_X1"" target=""_blank"">X1 computer</a>. Known as the Minimum Spanning Tree algorithm, it finds the shortest length of wire needed to connect a given set of points on a wiring panel. He published both network algorithms in a single paper in 1959.[<a href=""/bib/dijkstra_1053701.cfm#link_3"">3</a>]&nbsp;&nbsp;</p>
<p>When Dijkstra and Maria Debets married in 1957, the marriage rites required him to state his profession. When he stated that he was a programmer, the authorities objected that there was no such profession, and the marriage certificate instead identifies him as a theoretical physicist.</p>
<p>While at the Mathematical&nbsp;Center, Dijkstra worked on the very important “<a href=""/info/dijkstra_1053701.cfm"">real-time interrupt</a>”&nbsp;problem, which became the topic of his Ph.D. thesis [<a href=""/bib/dijkstra_1053701.cfm#link_2"">2</a>].&nbsp; Several computer manufacturers of the day were facing the same problem, but they had not approached the problem with the rigor that Dijkstra applied to it.</p>
<p>At the Mathematical Center, Dijkstra and J.A.Zonneveld developed the first <a href=""/info/dijkstra_1053701.cfm"">compiler</a>&nbsp;for Algol-60, a high-level programming language designed by an international committee. Completed in August 1960, their compiler predates the second Algol-60 compiler by more than a year. One of Algol-60’s great innovations, for which Dijkstra was instrumental, was the explicit introduction of <a href=""/info/dijkstra_1053701.cfm"">recursion</a>.&nbsp;He was probably the first to introduce the notion of a “<a href=""/info/dijkstra_1053701.cfm"">stack</a>”&nbsp;for translating recursive programs, reporting this seminal work in a short article [<a href=""/bib/dijkstra_1053701.cfm#link_4"">4</a>]. &nbsp;In the Oxford English Dictionary, the terms “<a href=""/info/dijkstra_1053701.cfm"">vector</a>”&nbsp;and “stack” in a computing context are attributed to Dijkstra.</p>
<p>In 1962 Dijkstra was appointed Professor of Mathematics at the Eindhoven University of Technology. There he built the <a href=""/info/dijkstra_1053701.cfm"">THE operating system&nbsp;</a>(named for the university, then known as Technische Hogeschool te Eindhoven), which has influenced the design of many subsequent operating systems. It introduced a number of <a href=""/info/dijkstra_1053701.cfm"">design principles</a>&nbsp;which have become part of the working vocabulary of every professional programmer. Introducing the reprint of Dijkstra’s article on the THE operating system in the 25th Anniversary issue of <em>Communications of the ACM</em>, the Editor-in-Chief wrote, “This project initiated a long line of research in multilevel systems architecture—a line that continues to the present day because hierarchical modularity is a powerful approach to organizing large systems.”</p>
<p><br>
In 1968, Dijkstra published a brief letter to the editor in&nbsp;<em>Communications of ACM</em>, titled “Go To statement considered harmful”[<a href=""/bib/dijkstra_1053701.cfm#link_5"">5</a>], arguing that the&nbsp;<a href=""/info/dijkstra_1053701.cfm"">GO TO</a>&nbsp;statement, found in many high-level programming languages, is a major source of errors, and should therefore be eliminated. There ensued a giant commotion in the computing community, with combatants taking positions on all sides of the issue. The debate has long since subsided; programming languages now provide alternatives to the GO TO. Few programmers today use it liberally, and most never use it at all.<br>
&nbsp;</p>
<p>Around this time, Dijkstra was beginning to formulate some of his early ideas about programming as a mathematical discipline. He pointed out that software productivity and reliability is closely related to rigor in design, which eliminates software flaws at an early stage. He was particularly impressed by the immensity of the so-called “software crisis” when he attended the famous 1968 <em>NATO Conference on Software Engineering</em>, the first conference devoted to the growing epidemic of software delivered late, over budget, and full of flaws. Convinced that programming methodology should become a scientific discipline, he decided to study how to avoid complexity in software designs.<br>
&nbsp;</p>
<p>Dijkstra’s “<a href=""/info/dijkstra_1053701.cfm"">Notes on Structured Programming</a>,”&nbsp;circulated to a few friends for their comments and soon became a sensation, and major corporations initiated programs based on his ideas to integrate rigorous practices into their programming projects. Subsequently published [<a href=""/bib/dijkstra_1053701.cfm#link_1"">1</a>]&nbsp;and still in print after nearly 40 years, this work has had far-reaching impact on all areas of computer science, from the teaching of the first courses in programming to the design of complex software. Mathematical analyses of program design and specifications have become central activities in computer science research.<br>
<br>
<br>
Dijkstra’s acceptance speech for the <a href=""/info/dijkstra_1053701.cfm"">1972 ACM Turing Award</a>,&nbsp;titled “The humble programmer”[<a href=""/bib/dijkstra_1053701.cfm#link_6"">6</a>], includes a vast number of observations on the evolution of programming as a discipline and prescriptions for its continued growth. It is required reading for any aspiring computer scientist.<br>
<br>
In August 1973 Dijkstra joined Burroughs Corporation as a Research Fellow. His duties consisted of consulting at some of the company’s research centers a few times a year and pursuing his own research. Among his significant contributions from this period is the development of a theory of <a href=""/info/dijkstra_1053701.cfm"">nondeterminacy</a>&nbsp;[<a href=""/bib/dijkstra_1053701.cfm#link_7"">7</a>], a concept outside traditional mathematics. Dijkstra was the first to observe not only that nondeterminacy is central in computations whose components interact asynchronously, but also that even when no asynchrony is involved, nondeterminacy is an effective tool for reasoning about programs and simplifying program design.<br>
<br>
His other major contribution during this period was the development of “<a href=""/info/dijkstra_1053701.cfm"">predicate transformers</a>”&nbsp;as a basis for defining program semantics and as a tool for deriving programs. His ideas refined the earlier ideas of&nbsp;<a href=""/award_winners/hoare_4622167.cfm"">C. A. R. Hoare</a>&nbsp;for an axiomatic basis of computer programming. He expounded these ideas along with nondeterminacy in A Discipline of Programming [<a href=""/bib/dijkstra_1053701.cfm#link_8"">8</a>],&nbsp;which has been identified by the Science Citation Index as a “Citations Classic”.<br>
<br>
The Burroughs years were Dijkstra’s most prolific in terms of research articles. He wrote nearly 500 documents in the&nbsp;<a href=""/info/dijkstra_1053701.cfm"">EWD series</a>,&nbsp;most of them technical reports, for private circulation within a select&nbsp;group.<br>
<br>
As a frequent visitor to the Burroughs Research Center in Austin, Texas starting in the late 1970s, Dijkstra had become familiar with the Department of Computer Science of the University of Texas at Austin. In 1984 Dijkstra accepted an appointment as the Department’s Schlumberger Centennial Chair.<br>
<br>
During his eighteen years in Austin, Dijkstra continued as a prolific researcher. Having earlier embarked on a long-term project for “<a href=""/info/dijkstra_1053701.cfm"">Streamlining Mathematical Arguments</a>”,&nbsp;in Austin he co-authored a book on <a href=""/info/dijkstra_1053701.cfm"">predicate calculus&nbsp;</a>[<a href=""/bib/dijkstra_1053701.cfm#link_9"">9</a>]&nbsp;advocating a “<a href=""/info/dijkstra_1053701.cfm"">calculational</a><a href=""/info/dijkstra_1053701.cfm""> proof style</a>”&nbsp;for mathematical arguments. He continued to apply his method in a number of diverse areas: coordinate geometry, linear algebra, graph theory, designs of sequential and distributed programs, and many others.<br>
<br>
The years in Austin saw Dijkstra at his best as a teacher and mentor for a generation of undergraduate and graduate students. From his days in the Eindhoven University of Technology he had thought deeply about how computer science should be taught, and Austin provided him the opportunity for trying out his ideas [<a href=""/bib/dijkstra_1053701.cfm#link_10"">10</a>]. He enjoyed the experience, appreciating “... brilliant students who made it a challenge and a privilege to lecture for them” [<a href=""/bib/dijkstra_1053701.cfm#link_11"">11</a>].&nbsp; He urged universities not to shrink from the challenge of <a href=""/info/dijkstra_1053701.cfm"">teaching radical novelties</a>.<br>
<br>
On the occasion of Dijkstra’s 60th birthday in 1990, the Department of Computer Sciences organized a two-day seminar in his honor. Speakers came from all over the US and Europe, and a group of computer scientists contributed research articles which were edited into a book [<a href=""/bib/dijkstra_1053701.cfm#link_12"">12</a>].&nbsp;<br>
<br>
Dijkstra retired from active teaching in November 1999. To mark the occasion and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, which took place on his 70th birthday in May 2000. The symposium was attended by a large number of prominent computer scientists as well as current and former students.<br>
<br>
Returning to The Netherlands in February 2002, Dijkstra died in Nuenen on 6 August 2002.<br>
&nbsp;</p>
<h4>Dijkstra’s Aphorisms and Epigrams</h4>
<p><br>
Dijkstra was famous for his wit and eloquence, such as in his remark:</p>
<p style=""margin-left: 40px;""><span class=""callout"">The question of&nbsp;whether computers can think is like the question of whether&nbsp;submarines can swim;</span></p>
<p><br>
or his advice to a promising researcher, who asked how to select a topic for research:</p>
<p style=""margin-left: 40px;""><span class=""callout"">Do only what only you can do;</span></p>
<p>and his remark in his Turing Award acceptance speech:</p>
<p style=""margin-left: 40px;""><span class=""callout"">In&nbsp;their capacity as a tool,&nbsp;computers will be but a ripple on the surface of our culture. In their&nbsp;capacity as&nbsp;intellectual challenge, they are without precedent in the cultural history of mankind.</span></p>
<p>Compiled below are a few more of Dijkstra’s other memorable epigrams.</p>
<p style=""margin-left: 40px;""><span class=""callout"">The tools we use have a profound and devious influence on our thinking habits, and therefore on our thinking abilities.</span></p>
<p style=""margin-left: 40px;""><span class=""callout"">Brainpower is by far our scarcest resource.<br>
<br>
Program testing can at best show the presence of errors but never their absence.</span></p>
<p style=""margin-left: 40px;""><span class=""callout"">The competent programmer is fully aware of the strictly limited size of his own skull; therefore he approaches the programming task in full humility, and among other things he avoids clever tricks like the plague.<br>
<br>
So-called natural language is wonderful for the purposes it was created for, such as to be rude in, to tell jokes in, to cheat or to make love in (and Theorists of Literary Criticism can even be content-free in it), but it is hopelessly inadequate when we have to deal unambiguously with situations of great intricacy, situations which unavoidably arise in such activities as legislation, arbitration, mathematics or programming. (foreword to Teaching and Learning Formal Methods, edited by C. N. Dean and M. G. Hinchey, Academic&nbsp;Press, 1996)<br>
<br>
Computer Science is no more about computers than astronomy is about telescopes.</span></p>
<p style=""margin-left: 40px;""><span class=""callout"">Being abstract is something profoundly different from being vague.</span></p>
<p style=""margin-left: 40px;""><span class=""callout"">Nothing is as expensive as making mistakes.</span></p>
<p style=""margin-left: 40px;""><span class=""callout"">In the practice of computing, where we have so much latitude for making a mess of it, mathematical elegance is not a dispensable luxury, but a matter of life and death.</span></p>
<p style=""margin-left: 40px;""><span class=""callout"">If 10 years from now, when you are doing something quick and dirty, you suddenly visualize that I am looking over your shoulders and say to yourself, Dijkstra would not have liked this, well that would be enough immortality for me.</span><br>
&nbsp;</p>
<p><strong>Acknowledgment</strong>: This profile borrows much from the <a href=""https://www.utexas.edu/faculty/council/2002-2003/memorials/Dijkstra/dijkstra.html"" target=""_blank"">Memorial Resolution </a>solicited by the Faculty Council of The University of Texas at Austin.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Hamilton Richards</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/dijkstra_1053701.cfm""><img src=""/images/lg_aw/1053701.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Edsger W. Dijkstra""></a>
<br><br>
<h6 class=""label""><a href=""/photo/dijkstra_1053701.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>11 May 1930, Rotterdam, The Netherlands</p>
<h6 class=""label"">DEATH:</h6>
<p>6 August 2002, Nuenen, The Netherlands.</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Gymnasium Erasmianum in Rotterdam (1948); undergraduate degree, physics, University of Leyden, (1956); PhD computer science, University of Amsterdam (1959). Honorary Degrees: Queen’s University Belfast, Athens University of Economics and Business.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Programmer, Computation Department of the Mathematical Centre in Amsterdam (1952–1962); Professor of Mathematics, Eindhoven University of Technology (1962–1973); Research Fellow, Burroughs Corporation (1973–1984); Schlumberger Centennial Chair in the Computer Science Department at the University of Texas at Austin (1984–2000).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Member of the Royal Netherlands Academy of Arts and Sciences (1971); Distinguished Fellow of the British Computer Society (1971); AFIPS Harry Goode Memorial Award (1974); Foreign Honorary member of the American Academy of Arts and Sciences (1975); IEEE Computer Society Computer Pioneer Award (1982); ACM/SIGCSE Award for outstanding contributions to computer science education (1989); ACM Fellow (1994); ACM Influential Paper Award for: “Self-stabilizing systems in spite of distributed control” Communications of the ACM 17, Vol. 11 (1974), pp. 643–644, 2002 (in 2003 this annual award was renamed the Dijkstra Prize).</p>","","https://dl.acm.org/author_page.cfm?id=81100248871","Edsger Wybe Dijkstra","<li class=""bibliography""><a href=""/bib/dijkstra_1053701.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283927&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/dijkstra_1053701.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/dijkstra_1053701.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179519-689","https://amturing.acm.org/award_winners/engelbart_5078811.cfm","For an inspiring vision of the future of interactive computing and the invention of key technologies to help realize this vision.","<div class=""bibliography"">
<p><strong>The first error people make about Doug Engelbart is to confuse him with a computer scientist.</strong> He is not a computer scientist, but an engineer by training and an inventor by choice. His numerous technological innovations (including the computer mouse, hypertext, and the split screen interface) were crucial to the development of personal computing and the Internet. His work helped to change the way computers work, from specialized machinery that only trained technicians could use, to a <em>medium</em> designed to augment the intelligence of its users and foster their collaboration. In every text, conference talk, and media appearance for the past fifty years, this stubborn man, this hopeful man, has constantly repeated the same thing:</p>
<p style=""margin-left: 40px;""><em>As human matters are getting increasingly complex and urgent, long terms solution will more likely come through the development of more powerful problem solving tools than through piecemeal solutions on specific problems.</em></p>
<p>Attempting to solve these ever more complex/urgent problems with the help of computer hardware and software has been the story of Douglas Engelbart’s professional life, his “crusade”.</p>
<p>Douglas C. Engelbart was born in Portland, Oregon, in 1925, the second of three children of a couple of Scandinavian and German descent. His father was an electrical engineer who owned a radio shop until he died (when Douglas was nine years old). He graduated from high school in 1942, and went on to study Electrical Engineering at Oregon State University, where he was trained as a radar technician, before he was drafted into the military in 1944. The radar training proved to be central for the rest of his career, and first triggered an absolute fascination in his young mind. He was in the Navy from 1944 to 1946, and was stationed for a year at the Philippines Sea Frontier, in the Manila bay. During that year he read Vannevar Bush's <a href=""https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/3881/"" target=""_blank"">As We May Think</a> article—a crucial influence on his later work. After the war, he went back to university in Corvallis, Oregon to finish his degree in Electrical Engineering. He graduated in 1948, and then took a job in California at the Ames Navy Research Center, where he stayed for 3 years.</p>
<p>Douglas Engelbart's decision to get involved in computing research happened in a complex move that encompassed most aspects of his personal and professional life. Engelbart identifies with a specific American generation, the <em>depression kids</em>—a generation born in adverse conditions that came of age during World War II. The war had left these kids in a paradoxical situation where science and technology had been the key to a <a href=""https://en.wikipedia.org/wiki/Pyrrhic_victory"" target=""_blank"">Pyrric victory</a>, and where an idealistic opening of new era was both full of hopes and fears, including a moral obligation to prevent such events to ever happen again. This paradoxical situation implied a specific way to situate oneself, in respect to ambivalent feelings and goals, toward the <em>general good of mankind,</em> best expressed in Engelbart’s military-religious metaphor of his<em> crusade for the augmentation of human intellect.</em></p>
<p>Engelbart decided to go to graduate school at Berkeley, where he earned his Ph.D. in electrical engineering in 1955 (John Woodyard was his advisor). This degree at Berkeley had reinforced his commitment to his crusade but had not provided him, at least directly, with the means to research and implement his ideas. He decided to form a corporation, Digital Techniques, to capitalize on his Ph.D. work on gas discharge devices. The experience did not last long as Digital Techniques closed down in 1957 after an assessment report from a team of experts concluded that solid state devices would soon doom their project.</p>
<p>Engelbart joined Stanford Research Institute (SRI, now Stanford Research International) in the Summer of 1957. SRI provided him with an environment best suited for the implementation of his Research Center for the Augmentation of Human Intellect (ARC), which soon became the source of many crucial hardware and software innovations such as: the mouse, integrated email, display editing, windows, cross-file editing, idea/outline processing, hypermedia, shared-screen teleconferencing, online publishing, and groupware—all integrated into their <em>oN-Line System</em> (NLS hereafter). Demonstrated for the first time in the famous May,1968 <em>Mother of All Demos</em> in San Francisco, NLS was designed to allow Computer-Supported Collaborative Work (CSCW), a field he essentially created.</p>
<p>Engelbart’s strategic vision began with the recognition of a major reason why some large-scale problems continue to elude humankind’s best efforts. His<em> radical scale change</em> principle asserts that as a complex system increases in scale, it changes not only in size but also in its qualities. This principle, in fact one of the three basic laws of <a href=""https://en.wikipedia.org/wiki/Dialectic#Hegelian_dialectic"" target=""_blank"">Hegelian dialectic</a>, seems at odds with the common-sense view that large-scale systems are reducible to smaller-scale parts without loss of qualities. Even if Engelbart knew that the computer was<em> just another artifact</em> at a time when <em>more engineering was not necessarily the solution,</em> he also knew that this <em>specific language artifact </em>was offering unusual characteristics. He understood that the computer was opening the cognitive realm to more dimensions than the usual three, allowing non-linear thinking. But, most importantly, it was extremely fast; it could calculate, display and help organize ideas at a blazing speed. He realized that the introduction of the computer, as a powerful auxiliary to human intellect, could turn a quantitative change into a qualitative change. Facing numerous too urgent and complex problems, the <em>little inelastic mind</em> of the human being could, with the augmentation of the computer, become up to the challenge. Most importantly, this basic tenet of his philosophy ended up playing as important of part in the<em> human system</em> as in the<em> tool system.</em></p>
<p>That the existence of a critical mass of augmented humans necessary to bootstrap, and thus augment the whole species, afforded no doubt in Engelbart’s mind. That is, in essence, what his notion of co-evolution between man and computer, tool system and human system, means. Human beings need a methodology and training that organizes their efforts at the levels of scale that are appropriate to the problems they are trying to solve. His <em>intellect augmentation</em> is such a method. The method provides a human-centered design for information technology that stands in contrast to the common automation approach to technology. In the automation approach, technology serves to replace human effort, thus saving time and freeing users to concentrate on more important matters. Automation, like Artificial Intelligence, does not alter the fundamental capabilities of the human users. In contrast, Engelbart’s intellect augmentation provides a model of technology that is deliberately designed so that human abilities will increase in response to using it.</p>
<p>An excellent example of intellect augmentation is the computer mouse (<a href=""https://www.google.com/patents?id=_bR0AAAAEBAJ&amp;printsec=abstract&amp;zoom=4&amp;source=gbs_overview_r&amp;cad=0#v=onepage&amp;q&amp;f=false"" target=""_blank"">U.S. Patent # 3,541,541</a>). Prior to Engelbart’s <em>x-y position indicator, </em>computer input was entered as symbols via keyboards or punch cards. The mouse allows a direct manipulation of elements within the computer environment and thus crosses the physical boundary between human and computer. It makes the interface an extension of human action rather than a mediator between the human and the machine. It extends, or <em>augments,</em> a very basic human ability, two-dimensional hand movement, into the ability to manipulate digital media. All of Engelbart’s efforts aim at extension of human capabilities in combination with technological innovation. The result is not merely people who have less to do thanks to the machine (automation) but people who have abilities to do more with the machine (augmentation).</p>
<p>Engelbart slipped into relative obscurity after 1976 due to various misfortunes and misunderstandings. Several of Engelbart's best researchers had become alienated from him and left his organization when Xerox PARC was created in 1970. The <a href=""https://www.nsf.gov/nsb/documents/2000/nsb00215/nsb50/1970/mansfield.html"" target=""_blank"">Mansfield Amendment</a>, the end of the Vietnam War, and the end of Project Apollo reduced his funding from ARPA and NASA. SRI's management, which did not understand what he was trying to accomplish, fired him in 1976. In 1978, a company called Tymshare bought NLS, hired him as a Senior Scientist, and offered commercial services based upon NLS. Engelbart soon found himself marginalized and relegated to obscurity—operational concerns superseded his desire to do further research. Various executives at Tymshare and McDonnell Douglas (which took over Tymshare in 1982) expressed interest in his ideas, but never committed the funds or the people to further develop them. He left McDonnell Douglas in 1986.</p>
<p>Since the mid 1990s, several important prizes and awards have recognized the seminal importance of Engelbart's contributions: In 1996 he was awarded the Yuri Rubinsky Memorial Award; in 1997 the Lemelson-MIT Prize, the world's largest single prize for invention and innovation, and the Turing Award. In 1999 Paul Saffo, from the Institute for the Future, hosted a large symposium at Stanford University's Memorial Auditorium, to honor Engelbart and his ideas. In December 2000 he was awarded the American National Medal of Technology, and in 2001 he was awarded the British Computer Society's Lovelace Medal.</p>
<p>Until his death in 2013 he served as the director emeritus of the Douglas Engelbart Institute (formerly Bootstrap Institute), which he founded in 1988 with his daughter, Christina Engelbart. It is located in Fremont, California and promotes the latest refinement of his philosophy, the concept of Collective IQ, and development of what he called Open Hyper-Document Systems (OHS), and HyperScope, a subset of OHS.</p>
<h5>Additional Links</h5>
<p>Douglas Engelbart Institute: <a href=""http://dougengelbart.org/"" target=""_blank"">http://dougengelbart.org/</a></p>
<p>Stanford University MouseSite: <a href=""http://sloan.stanford.edu/MouseSite/MouseSitePg1.html"" target=""_blank"">http://sloan.stanford.edu/MouseSite/MouseSitePg1.html</a></p>
<p>Video of the 1968 Demo: <a href=""https://www.archive.org/details/XD301_69ASISconfPres_Reel1"" target=""_blank"">http://www.archive.org/details/XD301_69ASISconfPres_Reel1</a></p>
<p>Stanford Oral History interviews by Henry Lowood and Judy Adams, edited by Thierry Bardini: <a href=""http://www-sul.stanford.edu/depts/hasrg/histsci/ssvoral/engelbart/engfmst1-ntb.html"" target=""_blank"">http://www-sul.stanford.edu/depts/hasrg/histsci/ssvoral/engelbart/engfmst1-ntb.html</a></p>
<p>Frode Hegland &amp; Fleur Klijnsma, Invisible Revolution: The Doug Engelbart Story, a web documentary: <a href=""http://www.invisiblerevolution.net/"" target=""_blank"">http://www.invisiblerevolution.net/</a></p>
<p style=""text-align: right;""><span class=""callout"">Author: Thierry Bardini</span></p>
<p><br>
&nbsp;</p>
</div>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/engelbart_5078811.cfm""><img src=""/images/lg_aw/5078811.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Douglas Engelbart""></a>
<br><br>
<h6 class=""label""><a href=""/photo/engelbart_5078811.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>January 30, 1925 in Portland, Oregon</p>
<h6 class=""label"">DEATH:</h6>
<p>July 2, 2013 in Atherton, California</p>
<h6 class=""label"">EDUCATION:</h6>
<p>B.S. in Electrical Engineering, Oregon State University (1948); M.S. in Electrical Engineering with a specialty in Computers, University of California at Berkeley (1953); Ph.D. in Electrical Engineering with a specialty in Computers, University of California at Berkeley (1955).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>US Navy, electronic/radar technician, WW II (1944-46); Electrical engineer, NACA Ames Laboratory, Mountain View, California (now NASA) (1948-51); Assistant Professor, electrical engineering, University of California at Berkeley (1955-56); Researcher, Stanford Research Institute (1957-59); Director, Augmentation Research Center, Stanford Research Institute (1959-77); Senior Scientist, Tymshare, Inc., Cupertino, California (1977-84); Senior Scientist, McDonnell Douglas Corporation ISG, San Jose, California (1984-89); Director, Bootstrap Project, Stanford University (1989-90); Director, Bootstrap Institute &amp; Bootstrap Alliance (now DEI), Menlo Park, California (1990-2008).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Over forty awards and honors, including: American Ingenuity Award (1991); IEEE Computer Pioneer Award (1993); Honorary Doctorate, Oregon State University (1994); Lemelson-MIT Prize (1997); ACM A.M. Turing Award (1997); ACM-CHI Lifetime Achievement Award (1998); IEEE John Von Neumann Medal Award (1999); USA National Medal of Technology (2000); Honorary Doctorate, Santa Clara University (2001); British Computer Society’s Lovelace Medal (2001); Norbert Wiener Award for Professional and Social Responsibility (2005).</p>","","https://dl.acm.org/author_page.cfm?id=81100342853","Douglas Engelbart","<li class=""bibliography""><a href=""/bib/engelbart_5078811.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""key-words""><a href=""/keywords/engelbart_5078811.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179672-699","https://amturing.acm.org/award_winners/emerson_1671460.cfm","","<p><strong>In 1981, Edmund M. Clarke and E. Allen Emerson working in the USA, and Joseph Sifakis working independently in France, authored seminal papers that founded what has become the highly successful field of <a href=""https://en.wikipedia.org/wiki/Model_checking"" target=""_blank"">Model Checking</a>.</strong> This verification technology provides an algorithmic means of determining whether an abstract model representing, for example, a hardware or software design, satisfies a formal specification expressed in <a href=""https://en.wikipedia.org/wiki/Temporal_logic"" target=""_blank"">temporal logic</a>. Moreover, if the property does not hold, the method identifies a counterexample that shows the source of the problem. The progression of Model Checking to the point where it can be successfully used for complex systems has required the development of sophisticated means of coping with what is known as the “state explosion problem”. Great strides have been made on this problem since 1981 by what is now a very large international research community. As a result, many major hardware and software companies are now using Model Checking in practice. Examples of its use include the verification of VLSI circuits, communication protocols, software device drivers, real-time embedded systems, and security algorithms.&nbsp;</p>
<p>The work of Drs. <a href=""/award_winners/clarke_1167964.cfm"">Clarke</a>, Emerson, and <a href=""/award_winners/sifakis_1701095.cfm"">Sifakis</a> has been central to the success of this research area. Their work over the years has led to the creation of new logics for specification, new verification algorithms, and surprising theoretical results. Model Checking tools, created by both academic and industrial teams, have resulted in an entirely novel approach to verification and test case generation. This approach, for example, often enables engineers in the electronics industry to design complex systems and have considerable assurance regarding the correctness of their designs.</p>
<p style=""text-align:right""><em>-- from the long citation</em></p>
<p><strong>Life</strong></p>
<p>E. Allen Emerson II was born and grew up in Dallas, Texas. He was always interested in scientific and mathematical topics. He taught himself calculus several years before he took it in public school. Emerson took a course on computer programming in high school, and learned BASIC on a GE Mark I Time Sharing System. Subsequently, he taught himself Fortran and Algol (from the Algol 60 report), and ran programs on a <a href=""https://en.wikipedia.org/wiki/Burroughs_large_systems"" target=""_blank"">Burroughs B5500</a> mainframe computer.</p>
<p>Emerson studied as an undergraduate at the University of Texas in Austin, where he earned his B.S. in Mathematics in 1976. He went on to graduate school at Harvard University, obtaining a Ph.D. in Applied Mathematics (Computer Science) in 1981. Shortly thereafter he joined the University of Texas in Austin as a faculty member, and has remained there since. He is now Regents Chair and Professor of Computer Science.</p>
<p>He has an interest in formal methods for establishing program correctness that dates back to his college days. This was inspired in part by reading a <em>Communications of the ACM</em> paper by <a href=""/award_winners/hoare_4622167.cfm"">Tony Hoare</a>, ""<a href=""https://dl.acm.org/citation.cfm?doid=362452.362489"" target=""_blank"">Proof of Program: Find</a>"". Also inspirational was a talk by Zohar Manna on fixpoints and the <a href=""https://en.wikipedia.org/wiki/Knaster%E2%80%93Tarski_theorem"" target=""_blank"">Tarski-Knaster Theorem</a> given in the 1970's at the University of Texas. He was also intrigued by the work of J. W. De Bakker, W. P De Roever, and <a href=""/award_winners/dijkstra_1053701.cfm"">Edsger W. Dijkstra</a> on <a href=""https://en.wikipedia.org/wiki/Predicate_transformer_semantics"" target=""_blank"">predicate transformers</a>.</p>
<p>Emerson is an Highly Cited Researcher of the Information Sciences Institute, a recognition given to the 250 most referenced computer science researchers. He has served as editor for leading formal methods journals, including <em>ACM Transactions on Computational Logic</em> (<em>ToCL</em>), <em>Formal Methods in Systems Design</em> (<em>FMSD</em>), <em>Formal Aspects of Computing</em> (<em>FAC</em>), and <em>Methods of Logic in Computer Science</em> (<em>MLCS</em>). He is a founding member of the steering committee for the formal methods conference <em>Automated Tools for Verification and Analysis</em> (<em>ATVA</em>), and he serves on the steering committee of <em>Verification, Model Checking, and Abstract Interpretation</em> (<em>VMCAI</em>).</p>
<p>A description of Emerson's technical work can be found <a href=""/info/emerson_1671460.cfm"">here</a>.</p>
<p align=""right""><span class=""callout"">Author: Thomas Wahl</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/emerson_1671460.cfm""><img src=""/images/lg_aw/1671460.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""E. Allen Emerson""></a>
<br><br>
<h6 class=""label""><a href=""/photo/emerson_1671460.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH:</span></h6>
<p>2 June, 1954, Dallas,TX, USA</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>B.S. in Mathematics, University of Texas at Austin, (1976); Ph.D. in Applied Mathematics, Harvard University (1981).</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Professor of Computer Science, University of Texas at Austin (from 1981).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>ACM Paris Kanellakis Theory and Practice Award, (1998); CMU Allen Newell Award for Research Excellence, (1999); IEEE Logic in Computer Science Test-of-Time Award (2006); ACM A. M. Turing Award (2007).<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81452614401","E. Allen Emerson","<li class=""bibliography""><a href=""/bib/emerson_1671460.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/emerson_1671460.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/emerson_1671460.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/emerson_1671460.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/emerson_1671460.cfm""><span></span>Video Interview</a></li>"
"1573179245-672","https://amturing.acm.org/award_winners/minsky_7440781.cfm","For his central role in creating, shaping, promoting, and advancing the field of Artificial Intelligence.","<p><strong>Marvin Minsky is Toshiba Professor of Media Arts and Sciences, Emeritus, and Professor of Electrical Engineering and Computer Science, Emeritus, at the Massachusetts Institute of Technology.</strong> His research includes important contributions to cognitive psychology, neural networks, automata theory, symbolic mathematics, and especially artificial intelligence, including work on learning, knowledge representation, common sense reasoning, computer vision, and robot manipulation. He has also made important contributions to graphics and microscope technology.</p>
<p>Minsky was born in New York City. He attended the Fieldston School and the Bronx High School of Science, in New York City, followed by Phillips Academy, in Andover, Massachusetts. After spending time in the United States Navy toward the end of World War II, he continued his education, earning a BA in Mathematics from Harvard College, followed by a PhD in Mathematics from Princeton University.</p>
<p>During his undergraduate years at Harvard, he interacted with the distinguished mathematician Andrew Gleason and the eminent psychologist George Miller. Minsky impressed Gleason with some fixed point theorems in topology, which first established his depth in mathematics and hinted at his eventual elevation to the National Academy of Science.</p>
<p>While at Princeton he built a learning machine, with tubes and motors, which established his passion for building and forecasted his elevation to the National Academy of Engineering. At Princeton, John Tukey and John von Neumann were on his thesis committee.</p>
<p>When he finished his PhD work, John von Neumann, Norbert Wiener, and Claude Shannon supported his admission to the select group of Junior Fellows at Harvard. As a Junior Fellow, Minsky invented the confocal scanning microscope for thick, light-scattering specimens. Light travels from the light source, through a beam splitter, comes to a point inside the specimen, bounces back to the beam splitter, and from there into the viewing optics. Because only one point is viewed at a time, the specimen has to be moved to form a complete image.</p>
<p>Minsky’s invention disappeared from view for many years because the lasers and computer power needed to make it really useful had not yet become available. About ten years after the original patent expired, it started to become a standard tool in biology and materials science.</p>
<p>Minsky’s work on Artificial Intelligence using symbol manipulation dates from the field’s earliest days in the 1950s and 1960s. Many consider his 1960 paper, “<a href=""http://web.media.mit.edu/~minsky/papers/steps.html"" target=""_blank"">Steps toward Artificial Intelligence</a>,” to be the call-to-arms for a generation of researchers. That paper established symbol manipulation—divided into heuristic search, pattern recognition, learning, planning, and induction—to be at the center of any attempt at understanding intelligence.</p>
<p>In the early 1960s, Minsky, along with&nbsp;<a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy</a>, founded the MIT Artificial Intelligence Laboratory. Students and staff flocked to this new laboratory to meet the challenge of understanding intelligence and endowing machines with it. Work in the new laboratory included not only attempts to model human perception and intelligence but also efforts to build practical robots. Minsky himself designed and built mechanical hands with tactile sensors and an arm with fourteen-degree-of-freedom. He exploited the fact that the force and torque vector associated with any single point of contact along an arm can be determined by a sophisticated force-sensing wrist.</p>
<p>From the 1960s, Minsky has argued that space exploration, undersea mining, and nuclear safety would be vastly simpler with manipulators driven locally by intelligent computers or remotely by human operators. Early on, he foresaw that microsurgery could be done by surgeons who work at one end of a telepresence system at a comfortably large scale while the other end machines do the chore required at the small scale where tiny nerve bundles are knitted together or clogged blood vessels are reamed out.</p>
<p>In the late 1960s, Minsky began to work on&nbsp;<a href=""https://en.wikipedia.org/wiki/Perceptron"" target=""_blank"">perceptrons</a>, which are simple computational devices that capture some of the characteristics of neural behavior. Minsky and Seymour Papert showed what perceptrons could and could not do, thus raising the sophistication of research on neurally-inspired mechanisms to a new level. Renewed interest in neurally-inspired mechanisms, twenty years later, led to a reprinting of their classic book, <em>Perceptrons</em> [<a href=""/bib/minsky_7440781.cfm#link_3"">3</a>], with a new chapter treating contemporary developments.</p>
<p>Taken together, Minsky’s steps toward Artificial Intelligence, his early work on symbol manipulation and perceptrons, the founding of the MIT Artificial Intelligence Laboratory, and the work of his earliest students firmly establish Minsky as one of the founders of Artificial Intelligence.</p>
<p>Minsky and Papert continued their collaboration into the 1970s and early 1980s, synergistically bringing together Minsky’s computational ideas with Papert’s understanding of developmental psychology. They worked both together and individually to develop theories of intelligence and radical new approaches to childhood education using <a href=""https://en.wikipedia.org/wiki/Logo_%28programming_language%29"" target=""_blank"">Logo</a>, the educational programming language developed by Papert and his colleagues.</p>
<p>Minsky’s best known work from the mid-1970s centers on a family of ideas that he called the Theory of Frames. He emphasized two key concepts in his famous, often reprinted paper, “<a href=""http://web.media.mit.edu/~minsky/papers/Frames/frames.html"" target=""_blank"">A Framework for Representing Knowledge</a>.” Minsky’s frames can be summarized by noting two things:</p>
<p style=""margin-left: 40px;""><br>
1. objects and situations can be represented as sets of slots and slot-filling values;<br>
2.&nbsp;many slots ordinarily can be filled by inheritance from the default descriptions embedded in a class hierarchy.</p>
<p>A frame describing a birthday party, for example, would have a slot for the person celebrated, the person’s age, the location, and a list of the gifts presented. When published, the Theory of Frames offered not only a fresh way to consider human thinking, but also had high impact on Artificial Intelligence as an emerging engineering discipline: the popular expert-system shells developed during the following decade all offered tools for developing, manipulating, and displaying frames.</p>
<p>A few years later, in “<a href=""https://dspace.mit.edu/bitstream/handle/1721.1/5739/AIM-516.pdf?sequence=2"" target=""_blank"">K-lines: A Theory of Memory</a>” (1979), Minsky addressed four key questions:</p>
<p style=""margin-left: 40px;""><br>
1. How is information represented?<br>
2. How is it stored?<br>
3. How is it retrieved?<br>
4. How is it used.</p>
<p><br>
His answer was that knowledge lines help us solve a problem by actuating those parts of our brains that put us back in a mental state much like one we were in when we thought about a similar problem before. An elementary physics problem, for example, might take a student into a mental state partially populated with previous applications of Newton’s laws, the conservation of energy, force diagrams, and the role of friction.</p>
<p>In 1985, frames, k-lines and many other ideas came together in Minsky’s book, <em>The Society of Mind </em>[<a href=""/bib/minsky_7440781.cfm#link_4"">4</a>].&nbsp;&nbsp;As its name suggests, the book is not about a single idea. Instead it is a statement that intelligence emerges from the cooperative behavior of myriad little agents, no one of which is intelligent by itself. Throughout the book, Minsky presents example after example of these little agents at work, some supporting natural language understanding, some solving problems, others accumulating new ideas, and still others acting as critics.</p>
<p>In 2006, Minsky published a second seminal book,&nbsp;<em>The Emotion Machine </em>[<a href=""/bib/minsky_7440781.cfm#link_6"">6</a>], which is full of ideas about consciousness, emotions, levels of thinking, and common sense. Multiplicity is a dominant theme. Minsky wrote that our resourceful intelligence arises from many ways of thinking, such as search, analogy, divide and conquer, elevation, reformulation, contradiction, simulation, logical reasoning, and impersonation. These ways of thinking are spread across many levels of mental activity, such as instinctive reactions, learned reactions, deliberative thinking, reflective thinking, self-reflective thinking, and self-conscious emotions. The upper levels of mental activity enable many ways of modeling self, such as physical, emotional, intellectual, professional, spiritual, social, political, economic, and familial. Concepts such as awareness and consciousness seem complex largely because such words do not label single, tightly bounded processes, but rather many different ways of thinking, spread across many levels of mental activity, involving many ways of modeling self; awareness and consciousness are suitcase words so big you can stuff anything into them.</p>
<p>Minsky and his wife, Gloria Rudisch Minsky, have three children, Margaret, Julie, and Henry.</p>
<p>&nbsp;</p>
<p style=""text-align: right;""><span class=""callout"">Author: Patrick Henry Winston</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/minsky_7440781.cfm""><img src=""/images/lg_aw/7440781.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Marvin Minsky ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/minsky_7440781.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>New York City, August 9, 1927</p>
<h6 class=""label"">DEATH:</h6>
<p>Boston, January 24, 2016</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Harvard University, B.A. Mathematics (1950); Princeton University, Ph.D. Mathematics (1954)</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>United States Navy (1944–45); Junior Fellow, Harvard Society of Fellows (1954–1957); Staff Member, M.I.T. Lincoln Laboratory (1957–1958); Faculty, M.I.T., (from 1959)</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Turing Award, Association for Computing Machinery (1970); Doubleday Lecturer, Smithsonian Institution (1978); Messenger Lecturer, Cornell University (1979); Killian Award, MIT (1989); Japan Prize Laureate (1990); Research Excellence Award, IJCAI (1991); Joseph Priestly Award (1995); Rank Prize, Royal Society of Medicine (1995); IEEE Computer Society Computer&nbsp; Pioneer Award (1995); Optical Society of America R.W. Wood Prize (2001); Franklin Institute Benjamin Franklin Medal (2001); World Skeptics Congress In Praise of Reason Award (2002); Computer History Museum Fellow (2006); Induction into IEEE Intelligent System's Hall of Fame for ""significant contributions to the field of AI and intelligent systems"" (2011); Dan David Prize (2014). Past President, American Association for Artificial Intelligence; Fellow, American Academy of Arts and Sciences; Fellow, Institute of Electrical and Electronic Engineers; Fellow, Harvard Society of Fellows; Board of Advisors, Planetary Society; Board of Governors, National Space Society; Member, U.S. National Academy of Engineering; Member, U.S. National Academy of Sciences; Member, Argentine National Academy of Science.<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81336491376","Marvin Minsky","<li class=""bibliography""><a href=""/bib/minsky_7440781.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283924&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/minsky_7440781.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/minsky_7440781.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179691-701","https://amturing.acm.org/award_winners/stearns_1081900.cfm","With Juris Hartmanis, in recognition of their seminal paper which established the foundations for the field of computational complexity theory.","<p>The seminal paper “On the Computational Complexity of Algorithms” [<a href=""/bib/stearns_1081900.cfm#bib_4"">4</a>] mentioned in the citation for the Turing Award was written with <a href=""/award_winners/hartmanis_1059260.cfm"">Juris Hartmanis</a> in 1965<strong>.</strong> It provides a precise definition of the <a href=""https://en.wikipedia.org/wiki/Computational_complexity_theory"" target=""_blank""><em>complexity</em></a> of an algorithm, defines the concept of a <a href=""https://en.wikipedia.org/wiki/Complexity_class"" target=""_blank""><em>complexity class</em></a>, and shows that there is an infinite sequence of distinct complexity classes and therefore an infinite sequence of increasingly hard problems.</p>
<p>An algorithm is a step-by-step procedure that takes inputs and produces corresponding outputs. For example, a sorting algorithm will take a list of numbers as input and output the list in numerical order. Such an algorithm is said to solve the <em>sorting problem</em>. Another algorithm might take a number as an input and output YES or NO depending on whether or not the input number is a prime number. Such an algorithm is said to solve the problem of <em>primality testing</em>. The complexity of an algorithm is based on the number of steps required, as a function of the input size, for the algorithm to solve a problem. Such a function is called a <em>time bound.</em> For mathematical purposes, algorithms are imagined to be executed on a <a href=""https://en.wikipedia.org/wiki/Turing_machine"" target=""_blank"">Turing machine</a>, a simple model of a computer. The complexity of a problem is the complexity of the fastest algorithm for solving that problem.</p>
<p>Hartmanis and Stearns defined a <em>complexity class </em>as the set of all problems solvable within a specified time bound. Their paper [<a href=""/bib/stearns_1081900.cfm#bib_4"">4</a>] shows that there is an infinite hierarchy of complexity classes (for example, problems for which the fastest algorithm takes a time proportional to <em>n, n log n, n<sup>2</sup>, n<sup>3</sup>, 2<sup>n</sup></em>, and so on) where a small increase in the time bound enables more problems to be solved.</p>
<p>A second paper [<a href=""/bib/stearns_1081900.cfm#bib_5"">5</a>] (with Philip M. Lewis) showed that a similar hierarchy exists when the complexity is defined in terms of the amount of memory space required (as a function of input size) to solve the problem on a Turing machine. Changing the simple model of a Turing Machine by separating the “input tape” from the “work tape” allowed sub-linear space-bounded complexity classes to be defined.</p>
<p>These papers led to the establishment of computational complexity theory as a fundamental part of the computer science discipline. Many computer scientists have written papers extending and enhancing the theory. Although the reasoning and the results of many of these papers are highly theoretical, they have practical implications for real-world problems. Many computer scientists and software engineers have used these results in the design and implementation of practical systems</p>
<h4>Background</h4>
<p><strong>Richard (“Dick”) Edwin Stearns, born on July 5, 1936 in Caldwell New Jersey, was the son of Dr. Edwin I. Stearns and Winifred T. Scales.</strong> He married Charlotte A Reed in 1963. They have two grown children, Chris R. Stearns and Dr. Elizabeth R. Gumustop.</p>
<p>Stearns received a B.A. in Mathematics from Carleton College in 1958. In 1961, he received a Ph.D. in Mathematics from Princeton University. His thesis, <em>Three Person Cooperative Games without Side Payments</em>, was done under the supervision of <a href=""https://en.wikipedia.org/wiki/Harold_W._Kuhn"" target=""_blank"">Harold W. Kuhn</a> with mentoring from <a href=""https://en.wikipedia.org/wiki/Robert_Aumann"" target=""_blank"">Robert J. Aumann</a>.</p>
<p>In the summer of 1960 Stearns worked at the General Electric Research Laboratory in Schenectady N.Y., where he and Juris Hartmanis started joint work on the state assignment problem. After receiving his Ph.D. in 1961, Stearns joined Hartmanis as a permanent employee of the GE Research Laboratory’s Information Studies Branch, managed and led by Richard L. Shuey. The atmosphere and environment at the Research Lab encouraged the free and unbounded type of research they both enjoyed.</p>
<p>Stearns and Hartmanis initially worked on the decomposition of sequential machines: how models of simple computers can be decomposed into a set of smaller sequential machines that accomplish the same tasks. They published several papers on the subject, and in 1966 summarized their work in a book [<a href=""/bib/stearns_1081900.cfm#bib_1"">1</a>].&nbsp; Later they worked on computational complexity, and in 1965 published the paper for which they received the Turing award [<a href=""/bib/stearns_1081900.cfm#bib_4"">4</a>].</p>
<p>After Hartmanis left GE to become Chair of the Computer Science Department at Cornell University, Stearns worked with <a href=""http://fellows.acm.org/fellow_citation.cfm?id=1051721&amp;srt=all"" target=""_blank"">Daniel J. Rosenkrantz</a> and <a href=""http://fellows.acm.org/fellow_citation.cfm?id=1144468&amp;srt=alpha&amp;alpha=L"" target=""_blank"">Philip M. Lewis</a>. Some of this collaboration continued the work on computational complexity.</p>
<p>One of the implications of the hierarchy of complexity classes is that there are many real-life problems whose complexity is so high that it is not practical to solve them. The work on approximating computationally complex problems[<a href=""/bib/stearns_1081900.cfm#bib_9"">9</a>], showed that some problems that would take an impractically long time to compute exactly can be approximately solved by algorithms that take a much shorter time yet yield a solution that is provably close to the optimal solution.</p>
<p>Rosenkrantz, Lewis, and Stearns published a number of papers and, in 1976, a book [<a href=""/bib/stearns_1081900.cfm#bib_2"">2</a>] on compiler design theory. A compiler is the program that translates programs written in a high-level programming language into the machine code of a particular computer. One compiler design problem is the problem of<em> parsing</em> a program in the programming language. For natural languages, parsing a sentence involves finding the <em>subject</em>, the <em>verb</em>, the <em>object</em>, etc. Programming languages are usually expressed in terms of <a href=""https://en.wikipedia.org/wiki/Context-free_grammar"" target=""_blank""><em>context free</em> <em>grammars</em></a>, and programs written using such grammars must be parsed by the compiler. The computational complexity of parsing a “sentence” (program) of length <em>n</em> written using a context free grammar is<em> n<sup>3</sup></em>, which is too high to be practical in a compiler. The authors defined a special class of context-free grammars, <a href=""https://en.wikipedia.org/wiki/LL_parser"" target=""_blank"">LL(k) grammars</a>, that can be parsed in linear time, where the complexity is only <em>n</em>. The method they described for parsing LL(k) grammars is called <em>top-down</em> <em>parsing</em> and is now an important part of compiler design. &nbsp;They also showed that much of the desired translation from a programming language into a computer language can be done at the same time the parsing is being done.</p>
<p>After Stearns left GE in 1978 to go to the State University of New York (SUNY) at Albany (Rosenkrantz had left to go to Albany in 1977), he continued working with Lewis. They did research on concurrent database systems, where a number of transactions simultaneously read and write items in the same database. One goal of such systems is for the transactions to run as concurrently as possible to increase the throughput of the system, but without destroying the correctness of the database. &nbsp;Their paper on this subject [<a href=""/bib/stearns_1081900.cfm#bib_10"">10</a>] shows that a necessary and sufficient condition for consistency and correctness in concurrent execution is <em>serializability</em> of the transactions—that is, the overall effect of the read and write requests of all the concurrently-running transactions must be the same as if the transactions had been run serially (one after the other) in some order. Previously it had been known that serializability is a sufficient condition for consistency: &nbsp;if the execution is serializable, it is correct. This paper showed that, with the exception of certain read-only transactions, it is also a necessary condition: the execution must be serializable to be correct. This result is now a key part of all database courses and the design of all database systems.</p>
<p>At SUNY Albany, Stearns began a long collaboration with Harry B. Hunt III. Problems of interest are often proven hard by “reductions” (mappings) from other problems known to be hard (assuming <a href=""https://en.wikipedia.org/wiki/P_versus_NP_problem"" target=""_blank"">P ≠ NP</a>). Hunt and Stearns studied deeper issues involving reductions. They looked for reductions that mapped to simple subsets of instances, because instances of practical interest might all be simple. They also looked for reductions of small size because the smaller the size, the stronger the conclusions that can be drawn about complexity. They formalized some of these ideas with the concept of a <em>power index. </em>Rather than looking for reductions one at a time, they looked for reduction principles which could be applied to many kinds of objects at once, particularly to various kinds of algebras. In this way, they showed that many problems were hard for the same simple reason.</p>
<p>Hunt and Stearns studied sum-of-products problems where the plus and times operators are from commutative semi-rings. They showed that such problems can be solved with a reduced number of operations if the problem had good structure as displayed in a <em>structure tree.</em> “Good structure” here means small “weighted depth” for top-down processing or small “channelwidth” for bottom-up processing. Channelwidth is a manifestation of “treewidth” if sum-of-product problems are looked at graphically, but the structure tree concept provides clarity if the problems are looked at algebraically. Then, with the addition of an algebraic condition, they extended the structure tree concept to apply to quantified formulas. They also established a tight connection between sub-linear space and sub-linearly treewidth bounded OR-of-AND problems.</p>
<p>In collaboration with S.S. Ravi, Harry Hunt III, Daniel Rosenkrantz, and Madhav Marathe, Stearns has been working on Dynamical Systems.</p>
<p>In September 2000, Stearns retired to live with his wife in Slingerlands, N.Y. He still visits and collaborates with former colleagues at the University.</p>
<p align=""right""><span class=""callout"">Author: Philip M. Lewis</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/stearns_1081900.cfm""><img src=""/images/lg_aw/1081900.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Richard E Stearns""></a>
</div>
<h6><span class=""label"">BIRTH:</span></h6>
<p>July 5, 1936 in Caldwell, New Jersey</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>B.A. in Mathematics (Carleton College, 1958); Ph.D. in Mathematics (Princeton University, 1961).</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>General Electric Research Laboratory (1961-1978); State University of New York at Albany (1978-2000, Chair, Computer Science Department,1982-1989); Editor, SIAM Journal of Computing (1972-1988); Member-at-Large, SIGACT Executive Committee (1973-1975); Visiting Professor, Hebrew University (1975); Adjunct Professor, Rensselaer Polytechnic Institute (1977-1978); Visitor, Mathematical Science Research Institute (1985).</p>
<h6><span class=""label"">HONORS AND AWARDS:</span></h6>
<p>Lanchester<span style=""line-height: 20.8px;"">&nbsp;Prize in Operations Research, co-winner (1977);&nbsp;</span>ACM Alan M. Turing Award (1993); ACM Fellow (1994); Distinguished Professor, State University of New York (1994).&nbsp;</p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81452610514","Richard (""Dick"") Edwin Stearns","<li class=""bibliography""><a href=""/bib/stearns_1081900.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283950&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/stearns_1081900.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/interviews/stearns_1081900.cfm""><span></span>Video Interview</a></li>"
"1573179389-681","https://amturing.acm.org/award_winners/reddy_9634208.cfm","For pioneering the design and construction of large scale artificial
intelligence systems, demonstrating the practical importance and potential commercial impact
of artificial intelligence technology.","<div class=""bibliography"">
<p><strong>Raj Reddy pioneered the construction of systems for recognizing continuous speech.</strong> He developed the first system,&nbsp;<em>Hearsay I</em>, capable of continuous speech recognition. In this system and subsequent systems like&nbsp;<em>Hearsay II</em>,<em> Harpy</em>, and <em>Dragon</em>, he and his students developed most of the ideas underlying modern commercial speech recognition technology. Some of these ideas—most notably the “blackboard model” for coordinating multiple knowledge sources—have been adopted across the spectrum of applied artificial intelligence. Together, the joint Turing Award recipients in 1994, Edward Feigenbaum and Raj Reddy, have been seminal leaders in defining the emerging field of applied artificial intelligence and demonstrating its technological significance.</p>
<p>Raj Reddy is renowned for his work in computer speech recognition, robotics, human-computer interaction, innovations in higher education, and efforts to bring digital technology to people on the other side of the “digital divide.”</p>
<p>Dabbala Rajagopal (Raj) Reddy was born on June 13, 1937 in Katoor, Andhra Pradesh, India. His father, Srdenivasulu Reddy, was an agricultural landlord and his mother, Pitchamma, was a homemaker. Reddy attended the ZP High School at Sri Kalahasti in Chittoor District, and received his Bachelor’s degree in civil engineering from Guindy College of Engineering, Madras (now Anna University, Chennai), India, in 1958. As an ROTC student in India he learned to fly, and later said that he used to fly bi-planes and do aerobatics. After his undergraduate work, he moved to Australia as an exchange student and received a Master’s degree in technology in 1960 from the University of New South Wales in Sydney, Australia. Upon finishing his Master’s, he worked as an Applied Science Representative for IBM in Australia.</p>
<p>In 1963, Reddy came to Stanford University as a PhD student. In early 1964, he began a class project under John McCarthy (himself a Turing Award recipient) on speech recognition, employing the Stanford AI Lab’s newly acquired analog-to-digital converter and PDP-1 computer to process speech waveforms. In a later interview, Reddy said that he chose that project, among several others suggested by McCarthy, because he was interested in natural languages and what could be learned about them using computers. Little did he know that his “class project” would occupy a lifetime. Reddy completed his PhD dissertation in 1966 under the supervision of McCarthy, on speech recognition. It was the first PhD granted by Stanford’s newly-formed Department of Computer Science.</p>
<p>Reddy stayed at Stanford as an assistant professor, doing and directing work on speech recognition, image processing, and face recognition. In 1969, attracted by Allen Newell, Herbert Simon and Alan Perlis (all three are also Turing Award recipients), he accepted a position as an associate professor at Carnegie Mellon University, where he continued his research on speech recognition and image processing. He became a Full Professor in 1973, and a University Professor in 1984. He served as the founding Director of the Robotics Institute from 1980 to 1992 and as the Dean of the School of Computer Science from 1991 to 1999. He became the Founding Director of Carnegie Mellon’s West Coast Campus in 2001, serving in that position until 2004.</p>
<p>In 1970, not long after Reddy arrived at CMU, Allen Newell chaired a committee sponsored by DARPA, the Defense Advanced Research Projects Agency, to investigate the feasibility of beginning a large-scale, five-year, community-wide project on “speech understanding.” CMU was among the groups funded to carry out research outlined by the committee, and Reddy headed the project. Even though CMU made notable achievements during that time, fielding successful speech-understanding systems called <em>HEARSAY II</em>, <em>HARPY</em>, and&nbsp;<em>DRAGON</em>, DARPA decided not to renew the main projects as such, but it did continue to support Reddy’s work on speech understanding at a reduced level at CMU under its basic research program.</p>
<p>Over a span of three decades, Reddy and his colleagues created several historic demonstrations of spoken language systems, such as voice control of robots, large-vocabulary connected speech recognition, speaker independent speech recognition, and unrestricted vocabulary dictation. They developed many of the ideas that underlie modern commercial speech recognition products.</p>
<p>Reddy and his colleagues have also made seminal contributions to other areas of artificial intelligence and computer science, notably to task-oriented architectures, analysis of natural scenes, and autonomous robotic systems. The “blackboard architecture” for coordinating multiple knowledge sources, developed under CMU’s speech understanding research program, has been widely adopted.</p>
<p>From about 1975 on, Reddy’s research interests expanded in several directions. He was one of the major collaborators at CMU with DARPA, and was instrumental in getting DARPA work started on VLSI research, sensor networks, operating systems (the “MACH” system, which is the foundation of Apple’s MAC OSX), and user interfaces and workstations. He also experimented with graphics printing.</p>
<p>In 1978 and 1979 Reddy persuaded the Westinghouse Corporation and others to support the newly-created Robotics Institute at CMU. He served as its founding director from 1979 to 1991. Reddy was able to persuade several gifted scientists to join. The Institute carries on research in several robotics-related fields, including space robotics, computer graphics, medical robotics, computer vision, and artificial intelligence. It played, and still plays, a leading role in making Pittsburgh a center for robotics research and applications. Its headquarters building is sometimes affectionately called the “Raj Mahal” in honor of its founder.</p>
<p>Computer science at CMU gradually outgrew in scope and size what could be housed in one department. In 1988, it became the School of Computer Science, and Reddy served as Dean from 1991 to 1999. In that position he helped create the Language Technologies Institute, the Human Computer Interaction Institute, the Center for Automated Learning and Discovery (since renamed the Department of Machine Learning), and the Institute for Software Research.</p>
<p>In 2005, Reddy was honored as the first recipient of the Mozah Bint Nasser Chair of Computer Science and Robotics. A gift from the Qatar Foundation, the chair was awarded as part of the inaugural celebration honoring the opening of CMU’s new campus in Qatar.</p>
<p>Reddy continues to innovate technically, organizationally, and as a computer science spokesperson. He was one of the founders of the American Association for Artificial Intelligence (now called the Association for the Advancement of Artificial Intelligence) and was its President from 1987 to 1989. He was a co-chair of the President's Information Technology Advisory Committee (PITAC) from 1999 to 2001. He serves on the International Board of Governors of the Peres Center for Peace based in Israel.</p>
<p>He actively participates in a number of organizations in India. He is a member of the governing board of the GVK Emergency Management and Research Institute, and of the Indian Institute of Health Management. He is the chairman of the Governing Council of the International Institute of Information Technology, Hyderabad, where Reddy and colleagues have developed Indian language processing. He helped found and is the Chancellor and the Chairman of the Governing Council of the Rajiv Gandhi University of Knowledge Technologies, which caters to the educational needs of gifted rural youth. In 2001, Reddy was awarded the Padma Bhushan, the third-highest civilian award given by the Indian Government, for distinguished service of a high order to the nation.</p>
<p>Reddy has participated in many other organizing activities. Among them are his role in getting a Silicon Valley branch of CMU started, and his involvement in the Universal Digital Library Project, whose goal is to coordinate all the world’s knowledge on the Web. “All of my projects are interrelated,” he told one interviewer, “In order to solve any one of them, you have to solve all of them.”</p>
<p>Reddy is extraordinarily talented at persuading people to help with his projects. Jim Morris, a former Dean of CMU’s School of Computer Science, mentioned that one of Reddy’s most effective tools of persuasion is something called the “full Raj” embrace—not to be confused with the mere “half Raj,” a tactic used for less critical tasks. Morris defined a “half Raj” as just an arm on your shoulder; a “full Raj” brings you into his neck.</p>
<p>Reddy’s accomplishments have led to many awards and honors. In addition to being a co-recipient with Ed Feigenbaum of the ACM Turing Award in 1994, he is a member of the National Academy of Engineering and the American Academy of Arts and Sciences. He was awarded the Legion d’Honneur, by French President Francois Mitterrand in 1984 for his work in developing countries; the Okawa Prize in 2004 for “pioneering researches of large scale artificial intelligence system, human-computer interaction… outstanding contributions to information and telecommunications policy”, the Honda Prize in 2005 for his “outstanding achievements in computer science and robotics,” the 2005 IJCAI Donald E. Walker Distinguished Service Award for “his outstanding service to the AI community,” and the Vannevar Bush Award in 2006 for his “pioneering research in robotics and intelligent systems, and his significant contributions in the formulation of national information and telecommunications policy.”</p>
<p>For a 1991 oral history interview of Reddy, see: <a href=""http://conservancy.umn.edu/bitstream/107605/1/oh231rr.pdf"" target=""_blank"">http://conservancy.umn.edu/bitstream/107605/1/oh231rr.pdf</a></p>
<p style=""text-align: right;""><span class=""callout"">Author: Nils J. Nilsson</span></p>
</div>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/reddy_9634208.cfm""><img src=""/images/lg_aw/9634208.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Raj Reddy""></a>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>June 13, 1937, Katoor, Andhra Pradesh, India</p>
<h6 class=""label"">EDUCATION:</h6>
<p>B.S., Civil Engineering, Guindy College of Engineering, Madras, (Now Anna University, Chennai), India, 1958; MTech, University of New South Wales, Sydney, Australia, 1960; PhD Stanford University, 1966.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Applied Science Representative, IBM (Australia), Sydney, Australia, 1960 – 1963; Assistant Professor of Computer Science, Stanford University 1966 – 1969; Associate Professor Computer Science, Carnegie Mellon University 1969 – 1973; Professor of Computer Science, Carnegie Mellon University 1973 – 1984; University Professor of Computer Science, Carnegie Mellon University; 1984 – present; Founding Director, Robotics Institute, Carnegie Mellon University; 1980 – 1992; Dean, School of Computer Science, Carnegie Mellon University, 1991 – 1999; Herbert A. Simon University Professor of Computer Science and Robotics, Carnegie Mellon University, 1992 – 2005; Founding Director, Carnegie Mellon University West Coast Campus, Mountain View, California; 2001 – 2004; Mozah Bint Nasser University Professor of Computer Science and Robotics, Carnegie Mellon University, 2005 – present</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Fellow, Acoustical Society of America; Fellow, Institute of Electrical and Electronics Engineers, Inc. (IEEE); Founding Fellow of the American Association for Artificial Intelligence (now called the Association for the Advancement of Artificial Intelligence, AAAI); Foreign Member, Chinese Academy of Engineering; Foreign Fellow, Indian National Science Academy (INSA); Foreign Fellow, Indian National Academy of Engineering(INAE); Recipient, Legion d’Honneur, presented by President Mitterrand of France (1984); Member of the National Academy of Engineering (1984); President, American Association for Artificial Intelligence (now called the Association for the Advancement of Artificial Intelligence, AAAI) (1987-1989); IBM Research Ralph Gomory Visiting Scholar Award (1991); Co-Recipient, Association for Computing Machinery Turing Award (jointly with Ed Feigenbaum) (1994); Member of the American Academy of Arts and Sciences (1995); Recipient, Padma Bhushan Award, presented by President of India (2001); Okawa Prize (2004); Honda Prize (2005); IJCAI Donald E. Walker Distinguished Service Award (2005); Vannevar Bush Award (2006); The IEEE James L. Flanagan Speech and Audio Processing Award (2008);&nbsp;<span style=""line-height: normal;"">inducted into&nbsp;</span><span style=""line-height: normal;"">IEEE Intelligent Systems</span><span style=""line-height: normal;"">' AI's Hall of Fame for the ""significant contributions to the field of AI and intelligent systems"" (2011).</span></p>
<p>Honorary Doctorates: Sri Venkateswara University, Henri Poincaré University, University of New South Wales, Jawaharlal Nehru Technology University, University of Massachusetts, University of Warwick, Anna University, Indian Institute for Information Technology (Allahabad), Andhra University, IIT Kharagpur, and Hong Kong University of Science and Technology</p>","","https://dl.acm.org/author_page.cfm?id=81329491459","Dabbala Rajagopal (""Raj"") Reddy","<li class=""bibliography""><a href=""/bib/reddy_9634208.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283952&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/reddy_9634208.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/reddy_9634208.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/reddy_9634208.cfm""><span></span>Video Interview</a></li>"
"1573179640-697","https://amturing.acm.org/award_winners/lecun_6017366.cfm","For conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.","","<div class=""featured-photo"">
<a href=""/award_winners/lecun_6017366.cfm""><img src=""/images/lg_aw/6017366.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Yann LeCun""></a>
</div>","","https://dl.acm.org/author_page.cfm?id=81350597740","Yann LeCun","<li class=""award-video""><a href=""/vp/lecun_6017366.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/lecun_6017366.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179153-666","https://amturing.acm.org/award_winners/pearl_2658896.cfm","For fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning.","<p>Judea Pearl created the representational and computational foundation for the processing of information under uncertainty.</p>
<p>He is credited with the invention of <a href=""https://en.wikipedia.org/wiki/Bayesian_network"" target=""_blank""><em>Bayesian networks</em></a>, a mathematical formalism for defining complex probability models, as well as the principal algorithms used for inference in these models. This work not only revolutionized the field of artificial intelligence but also became an important tool for many other branches of engineering and the natural sciences. He later created a mathematical framework for <a href=""https://en.wikipedia.org/wiki/Inductive_reasoning"" target=""_blank"">causal inference</a> that has had significant impact in the social sciences.</p>
<p>Judea Pearl was born on September 4, 1936, in Tel Aviv, which was at that time administered under the British Mandate for Palestine. He grew up in <a href=""https://en.wikipedia.org/wiki/Bnei_Brak"" target=""_blank"">Bnei Brak</a>, a Biblical town his grandfather went to reestablish in 1924. In 1956, after serving in the Israeli army and joining a Kibbutz, Judea decided to study engineering. He attended the Technion, where he met his wife, Ruth, and received a B.S. degree in Electrical Engineering in 1960. Recalling the Technion faculty members in a 2012 interview in the <em>Technion Magazine</em>, he emphasized the thrill of discovery:</p>
<p style=""margin-left:.5in;"">Professor Franz Olendorf always spoke as if he was personally present in Cavendish laboratory, where the electron was discovered, Professor Abraham Ginzburg made us feel the winds blowing in our face as we travelled along those line integrals in the complex plane. And Professor Amiram Ron gave us the feeling that there is still something we can add to Maxwell’s theory of electromagnetic waves.</p>
<p>Judea then went to the United States for graduate study, receiving an M.S. in Electronics from Newark College of Engineering in 1961, an M.S. in Physics from Rutgers University in 1965, and a Ph.D. in electrical engineering from the Polytechnic Institute of Brooklyn in the same year. The title of his Ph.D. thesis was “<em>Vortex Theory of Superconductive Memories;”</em> the term<em> “</em><a href=""https://en.wikipedia.org/wiki/Pearl_vortex"" target=""_blank"">Pearl vortex</a><em>”</em> has become popular among physicists to describe the type of superconducting current he studied<em>. </em>He worked at RCA Research Laboratories in Princeton, New Jersey on superconductive parametric amplifiers and storage devices, and at Electronic Memories, Inc. in Hawthorne, California on advanced memory systems. Despite the apparent focus on physical devices, Pearl reports being motivated even then by potential applications to intelligent systems.</p>
<p>When industrial research on magnetic and superconducting memories was curtailed by the advent of large-scale semiconductor memories, Pearl decided to move into academia to pursue his long-term interest in logic and reasoning. In 1969, he joined the faculty of the University of California, Los Angeles, initially in Engineering Systems, and in 1970 he received tenure in the newly formed Computer Science Department. In 1976 he was promoted to full professor. In 1978 he founded the Cognitive Systems Laboratory – a title that emphasized his desire to understand human cognition. The laboratory’s research facility was Pearl’s office, on the door of which hung a permanent sign reading, “<em>Don’t knock. Experiments in Progress.</em>”</p>
<p>Pearl’s reputation in computer science was established initially not in probabilistic reasoning –a highly controversial topic at that time – but in combinatorial search. A series of journal papers beginning in 1980 culminated in the publication of the book, <em>Heuristics</em>: <em>Intelligent Search Strategies for Computer Problem Solving</em>, [<a href=""/bib/pearl_2658896.cfm#bib_6"">6</a>] in 1984. This work included many new results on traditional search algorithms such as <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" target=""_blank"">A*</a>, and on game-playing algorithms, raising AI research to a new level of rigor and depth. It also set out new ideas on how admissible heuristics might be derived automatically from relaxed problem definitions, an approach that has led to dramatic advances in planning systems. Despite the book’s formal style, it drew its inspiration from, as Pearl said, “the ever-amazing observation of how much people can accomplish with that simplistic, unreliable information source known as <em>intuition</em>.” Ira Pohl wrote in 2011 that “The impact of Pearl’s monograph was transformative … [The book] was a tour de force summarizing the work of three decades.”</p>
<p>Soon after arriving at UCLA, Pearl began teaching courses on probability and decision theory, which was a rarity in computer science departments at that time. Probabilistic methods had been tried in the 1960s and found wanting; a system for estimating the probability of a disease given <em>n</em> possible symptoms was thought to require a set of probability parameters whose size is exponential in <em>n</em>. The 1970s, on the other hand, saw the rise of <em>knowledge-based systems</em>, based primarily on logical rules or on rules augmented with “certainty factors.”</p>
<p>Pearl believed that sound probabilistic analysis of a problem would give intuitively correct results, even in those cases where rule-based systems behaved incorrectly. One such case had to do with the ability to reason both <em>causally</em> (from cause to effect) and <em>diagnostically</em> (from effect to cause). “If you used diagnostic rules, you could not do prediction, and if you used predictive rules you could not reason diagnostically, and if you used both, you ran into positive-feedback instabilities, something we never encountered in probability theory.” Another case concerned the “explaining-away” phenomenon, whereby the degree of belief in any cause of a given effect is increased when the effect is observed, but then decreases when some other cause is found to be responsible for the observed effect. Rule-based systems could not exhibit the explaining-away phenomenon, whereas it happens automatically in probabilistic analysis.</p>
<p>In addition to these basic qualitative questions, Pearl was motivated by <a href=""https://en.wikipedia.org/wiki/David_Rumelhart"" target=""_blank"">David Rumelhart’s</a> 1976 paper on reading comprehension. As he wrote later in his 1988 book,</p>
<p style=""margin-left:.5in;"">In this paper, Rumelhart presented compelling evidence that text comprehension must be a distributed process that combines both top-down and bottom-up inferences. Strangely, this dual mode of inference, so characteristic of Bayesian analysis, did not match the capabilities of either the “certainty factors” calculus or the inference networks of PROSPECTOR<a href=""#_ftn1"" name=""_ftnref1"">[1]</a>−the two major contenders for uncertainty management in the 1970s. I thus began to explore the possibility of achieving distributed computation in a “pure” Bayesian framework.</p>
<p>Pearl realized that the concept of <a href=""https://en.wikipedia.org/wiki/Conditional_independence"" target=""_blank""><em>conditional independence</em></a> would be the key to constructing complex probability models with polynomially many parameters and to organizing distributed probability computations. The paper “Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach”[<a href=""/bib/pearl_2658896.cfm#bib_8"">8</a>] introduced probability models defined by directed acyclic graphs and derived an exact, distributed, asynchronous, linear-time inference algorithm for trees – an algorithm we now call <em>belief propagation</em>, the basis for <a href=""https://en.wikipedia.org/wiki/Turbo_code"" target=""_blank"">turbocodes</a>. There followed a period of remarkable creative output for Pearl, with more than 50 papers covering exact inference for general graphs, approximate inference algorithms using <a href=""https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"" target=""_blank"">Markov chain Monte Carlo</a>, conditional independence properties, learning algorithms, and more, leading up to the publication of <em>Probabilistic Reasoning in Intelligent Systems</em>[<a href=""/bib/pearl_2658896.cfm#bib_15"">15</a>] in 1988. This monumental work combined Pearl’s philosophy, his theories of human cognition, and all his technical material into a persuasive whole that sparked a revolution in the field of artificial intelligence. Within just a few years, leading researchers from both the logical and the neural-network camps within AI had adopted a probabilistic – often called simply the <em>modern</em> – approach to AI.</p>
<p>Pearl’s Bayesian networks provided a syntax and a calculus for multivariate probability models, in much the same way that <a href=""https://en.wikipedia.org/wiki/George_Boole"" target=""_blank"">George Boole</a> provided a syntax and a calculus for logical models. Theoretical and algorithmic questions associated with Bayesian networks form a significant part of the modern research agenda for machine learning and statistics, Their use has also permeated other areas, such as natural language processing, computer vision, robotics, computational biology, and cognitive science. As of 2012, some 50,000 publications have appeared with Bayesian networks as a primary focus.</p>
<p>Even while developing the theory and technology of Bayesian probability networks, Pearl suspected that a different approach was needed to address the issue of <em>causality</em>, which had been one of his concerns for many years. In his 2000 book on Causality [<a href=""/bib/pearl_2658896.cfm#bib_20"">20</a>], he described his early interest as follows:</p>
<p style=""margin-left:.5in;"">I got my first hint of the dark world of causality during my junior year of high school. My science teacher, Dr. Feuchtwanger, introduced us to the study of logic by discussing the 19th century finding that more people died from smallpox inoculations than from smallpox itself. Some people used this information to argue that inoculation was harmful when, in fact, the data proved the opposite, that inoculation was saving lives by eradicating smallpox.</p>
<p style=""margin-left:.5in;"">And here is where logic comes in,” concluded Dr. Feuchtwanger, “To protect us from cause-effect fallacies of this sort.” We were all enchanted by the marvels of logic, even though Dr. Feuchtwanger never actually showed us how logic protects us from such fallacies.</p>
<p style=""margin-left:.5in;"">It doesn’t, I realized years later as an artificial intelligence researcher. Neither logic, nor any branch of mathematics had developed adequate tools for managing problems, such as the smallpox inoculations, involving cause-effect relationships.</p>
<p>A Bayesian network such as <em>Smoking</em> --&gt; <em>Cancer</em> fails to capture causal information; indeed, it is mathematically equivalent to the network <em>Cancer</em> --&gt; <em>Smoking</em>. The key characteristic of a <em>causal network</em> is the way in which it captures the potential effect of exogenous intervention. In a causal network <em>X</em> --&gt; <em>Y</em>, <em>intervening</em> to set the value of <em>Y</em> should leave one’s prior belief in <em>X</em> unchanged and simply breaks the link from <em>X</em> to <em>Y</em>; thus, <em>Smoking</em> --&gt; <em>Cancer</em> as a causal network captures our beliefs about how the world works (inducing cancer in a subject does not change one’s belief in whether the subject is a smoker), whereas <em>Cancer</em> --&gt; <em>Smoking</em> does not (inducing a subject to smoke does change one’s belief that the subject will develop cancer). This simple analysis, which Pearl calls the <em>do-calculus</em>, leads to a complete mathematical framework for formulating causal models and for analyzing data to determine causal relationships. This work has overturned the long-held belief in statistics that causality can be determined only from controlled random trials – which are impossible in areas such as the biological and social sciences. Referring to this work, Phil Dawid (Professor of Statistics at Cambridge) remarks that Pearl is “the most original and influential thinker in statistics today.” Chris Winship (Professor of Sociology at Harvard) writes that, “Social science will be forever in his debt.”</p>
<p>In 2010 a Symposium was held at UCLA in Pearl’s honor, and a Festschrift was published containing papers in all the areas covered by his research. The volume also contains reminiscences from former students and other researchers in the field. Ed Purcell, Pearl’s first PhD student, wrote, “In class I was immediately impressed and enchanted by Judea’s knowledge, intelligence, brilliance, warmth and humor. His teaching style was engaging, interactive, informative and fun.” Hector Geffner, a PhD student in the late 1980s, wrote, “He was humble, fun, unassuming, respectful, intelligent, enthusiastic, full of life, very easy to get along with, and driven by a pure and uncorrupted <em>passion for understanding</em>.” Nils Nilsson, former professor and Chair of the Computer Science Department at Stanford and an AI pioneer, described Pearl as “a towering figure in our field.”</p>
<p>Pearl’s outside interests include music (several early conferences were entertained by his impromptu piano renditions and very realistic trumpet imitations), philosophy, and early books – particularly the great works of science throughout history, of which he possesses several first editions. Judea and Ruth Pearl had three children, Tamara, Michelle, and Daniel. Since Daniel’s kidnap and murder in Pakistan in 2002, Professor Pearl has devoted a significant fraction of his time and energy to the <a href=""https://en.wikipedia.org/wiki/Daniel_Pearl_Foundation"" target=""_blank"">Daniel Pearl Foundation</a>, which he and his wife founded to promote Daniel’s values of “uncompromised objectivity and integrity; insightful and unconventional perspective; tolerance and respect for people of all cultures; unshaken belief in the effectiveness of education and communication; and the love of music, humor, and friendship.”</p>
<p wrap="""">Pearl will donate a major portion of the Turing Prize money to support the projects of the Daniel Pearl Foundation and another portion to promote the introduction of causal inference in statistics education.</p>
<p align=""right""><span class=""callout"">Author: Stuart J. Russell</span></p>
<div><br clear=""all"">
<hr align=""left"" size=""1"" width=""33%"">
<div id=""ftn1"">
<p><a href=""#_ftnref1"" name=""_ftn1"" title="""">[1]</a> An expert system that finds ore deposits from geological information; created in the 1970s by Richard Duda, Peter Hart, and others at Stanford Research Institute (SRI).</p>
</div>
</div>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/pearl_2658896.cfm""><img src=""/images/lg_aw/2658896.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Judea Pearl""></a>
</div>
<h5><a href=""/photo/pearl_2658896.cfm""><img alt="""" src=""/images/misc/bhlight.jpg"" style=""float: left;""></a>&nbsp; <a href=""/photo/pearl_2658896.cfm"">Photo-Essay</a><br>
&nbsp;</h5>
<h6><span class=""label"">BIRTH: </span></h6>
<p>September 4, 1936, Tel Aviv.</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>B.S., Electrical Engineering (Technion, 1960); M.S., Electronics (Newark College of Engineering, 1961); M.S., Physics (Rutgers University, 1965); Ph.D., Electrical Engineering (Polytechnic Institute of Brooklyn, 1965).</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>Research Engineer, New York University Medical School (1960–1961); Instructor, Newark College of Engineering (1961); Member of Technical Staff, RCA Research Laboratories, Princeton, New Jersey (1961–1965); Director, Advanced Memory Devices, Electronic Memories, Inc., Hawthorne, California (1966–1969); Assistant Professor of Engineering Systems, UCLA (1969–1970); Associate Professor of Computer Science, UCLA (1970–1976); Director, Cognitive Systems Laboratory, UCLA (from 1978); Professor of Computer Science, UCLA (from 1976—Emeritus&nbsp; since 1994); Professor of Statistics, UCLA (from 1996–Emeritus since 1994); President, Daniel Pearl Foundation (from 2002); International Advisory Board, NGO Monitor (from 2011).</p>
<p>&nbsp;</p>
<h6><span class=""label"">HONORS AND AWARDS:</span></h6>
<p>RCA Laboratories Achievement Award (1963); NATO Senior Fellowship in Science (1974); Pattern Recognition Society Award for an Outstanding Contribution (1978); Fellow, IEEE (1988); Fellow, American Association of Artificial Intelligence (1990); Named “The Most Published Scientist in the Artificial Intelligence Journal,” (1991); Member, National Academy of Engineering (1995); UCLA Faculty Research Lecturer of the Year (1996); IJCAI Research Excellence Award (1999); AAAI Classic Paper Award (2000); Lakatos Award, London School of Economics and Political Science (2001); Corresponding Member, Spanish Academy of Engineering (2002); Pekeris Memorial Lecture (2003); ACM Allen Newell Award (2003); Purpose Prize (2006); Honorary Doctorate, University of Toronto (2007); Honorary Doctorate, Chapman University (2008); Benjamin Franklin Medal in Computers and Cognitive Science (2008); Festschrift and Symposium in honor of Judea Pearl (2010); Rumelhart Prize Symposium in honor of Judea Pearl (2011); David E. Rumelhart Prize (2011); IEEE Intelligent Systems’ AI Hall of Fame (2011); ACM Turing Award (2011); Harvey Prize (2012); elected to National Academy of Sciences (2014).<br>
&nbsp;</p>
<h6><a href=""https://www.acm.org/press-room/news-releases/2012/turing-award-11/"" target=""_blank""><span class=""label"">PRESS RELEASE</span></a><br>
<a href=""https://www.acm.org/press-room/press-coverage"" target=""_blank""><span class=""label"">MEDIA COVERAGE</span></a></h6>","","https://dl.acm.org/author_page.cfm?id=81100419881","Judea Pearl","<li class=""bibliography""><a href=""/bib/pearl_2658896.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/pearl_2658896.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/pearl_2658896.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/pearl_2658896.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/pearl_2658896.cfm""><span></span>Video Interview</a></li>"
"1573179294-675","https://amturing.acm.org/award_winners/codd_1000892.cfm","For his fundamental and continuing contributions to the theory and practice of database management systems.","<p><strong>Edgar Frank (Ted) Codd, the youngest of seven children, was born August 19th, 1923, on the Isle of Portland in the county of Dorset on the south coast of England.</strong> His father was a leather manufacturer and his mother a schoolteacher. During the 1930s he attended Poole Grammar School in Dorset. He was awarded a full scholarship to Oxford University (Exeter College), where he initially read chemistry (1941 1942). In 1942—despite the fact that he was eligible for a deferment because of his studies— he volunteered for active duty and became a flight lieutenant in the Royal Air Force Coastal Command, flying Sunderlands. After the war he returned to Oxford to complete his studies, switching to mathematics and obtaining his degree in 1948.</p>
<p>As part of his service in the RAF, Codd was sent to the United States for aviation training. That experience led to a lifelong love of recreational flying, also to a recognition that the United States had a great deal to offer for someone of a creative bent like himself. As a consequence, he emigrated to the United States soon after graduating in 1948. After a brief period with Macy’s in New York City, working as a sales clerk in the men’s sportswear department, he found a job as a mathematics lecturer at the University of Tennessee in Knoxville, where he taught for six months.</p>
<p>Codd’s computing career began in 1949, when he joined IBM in New York City as a programming mathematician, developing programs for the Selective Sequence Electronic Calculator (IBM’s first electronic—or at least electromechanical—computer, a huge and noisy vacuum tube machine). He also lived for a brief period in Washington DC, where he worked on IBM’s Card Programmed Electronic Calculator. In the early 1950s, he became involved in the design and development of IBM’s 701 computer. (The 701, originally known as the Defense Calculator, was IBM’s first commercially available computer for scientific processing; it was announced in 1952 and formally unveiled in 1953.)</p>
<p>In 1953, Codd left the United States (and IBM) in protest against Senator Joseph McCarthy’s witch-hunting and moved to Ottawa, Canada, where he ran the data processing department for Computing Devices of Canada Limited (which was involved in the development of the Canadian guided missile program). A chance meeting with his old IBM manager led to his return to the U.S. in 1957, when he rejoined IBM. Now based in Poughkeepsie, New York, he worked on the design of STRETCH (i.e., the IBM 7030, which subsequently led to IBM’s 7090 mainframe technology); in particular, he led the team that developed the world’s first multiprogramming system. (“Multiprogramming” refers to the ability of programs that have been developed independently of one another to execute concurrently. The basic idea is that while one program is waiting for some event to occur, such as the completion of a read or write operation, another program can be allowed to make use of the computer’s central processing unit. Multiprogramming is now standard on essentially all computer systems except for the smallest personal computers.) In 1961, on an IBM scholarship, he moved to Ann Arbor, Michigan, where he attended the University of Michigan and obtained an M.Sc. and Ph.D. in communication sciences (1965). His thesis—which was published by Academic Press in 1968 under the title <em>Cellular Automata</em>—represented a continuation and simplification of von Neumann’s work on self-reproducing automata; in it, Codd showed that the 29 states required by von Neumann’s scheme could be reduced to just eight.</p>
<p>During this period Codd also became a U.S. citizen—though he never lost his British accent, his British sense of humor, or his British love for a good cup of tea.</p>
<p>Codd then returned to IBM Poughkeepsie, where he worked on high level techniques for software specification. He then turned his attention to database issues and in 1968 transferred to the IBM Research Laboratory in San Jose, California. (He subsequently claimed that what initially motivated him in this research was a presentation by a representative from a database company who seemed—incredibly, so far as Codd was concerned—to have no knowledge or understanding of predicate logic.) Several database products did indeed exist at that time; however, they were without exception ad hoc, cumbersome, and difficult to use—they could really only be used by people having highly specialized technical skills—and they rested on no solid theoretical foundation. Codd recognized the need for such a foundation and, applying his knowledge of mathematical logic, was able to provide one by creating the invention with which his name will forever be associated: the <a href=""/info/codd_1000892.cfm"">relational model of data</a>.</p>
<p>The relational model is widely recognized as one of the great technical achievements of the 20th century. It revolutionized the way databases were perceived; indeed, it transformed the entire database field—which previously consisted of little more than a collection of ad hoc products, proposals, and techniques—into a respectable scientific (and academic) discipline. More specifically, it provided a theoretical framework within which a variety of important database problems could be attacked in a scientific manner. As a consequence, it is no exaggeration to say that essentially all databases in use or under development today are based on Codd’s ideas. Whenever anyone uses an ATM machine, or purchases an airline ticket, or uses a credit card, he or she is effectively relying on Codd’s invention.</p>
<p>Codd described his model further and explored its implications in a series of research papers, staggering in their originality, that he published over the next several years (see annotated bibliography). Throughout this period, he was helpful and supportive to all who approached him—the author of these notes included— with a serious interest in learning more or with a view to helping disseminate, and perhaps elaborate on, his ideas. At the same time, he was steadfast and unyielding in defending those same ideas from adverse criticism.<br>
It should be noted, incidentally, that the relational model was in fact the very first abstract database model to be defined. Thus, Codd not only invented the relational model in particular, he actually invented the data model concept in general.</p>
<p>During the 1970s Codd also explored the possibility of constructing a natural language question and answer application on top of a relational database system, leading a small team that built a prototype of such an application, called Rendezvous. Rendezvous allowed a user with no knowledge of database systems (and with, perhaps, only limited knowledge of the exact content of the database) to engage in a dialog with the system, starting with a query—possibly not very precisely stated—and winding up with a precise query and corresponding answer, where the entire dialog was conducted in natural language (English, in the case of the prototype).</p>
<p>Throughout this time, Codd continued to be employed by IBM. Perhaps because it was heavily invested in its existing nonrelational database product IMS and was anxious to preserve the revenue from that product, however, IBM itself was initially quite unreceptive (not to say hostile) to Codd’s relational ideas. As a consequence, other vendors, including Relational Software Inc. (later renamed Oracle Corporation) and Relational Technology Inc. (later renamed Ingres Corporation), were able to steal a march on IBM and bring products to market well before IBM did. Seeing the way the winds were blowing, senior IBM management decided in the late 1970s that IBM should build a relational product of its own. That decision resulted in the announcement of SQL/DS for the VSE environment in 1981 and DB2 for the MVS environment in 1983.</p>
<p>In Codd’s opinion, however, those IBM products, though clearly superior to their nonrelational predecessors, were less than fully satisfactory because their support for the relational model was incomplete (and in places incorrect). Partly for that reason, Codd resigned from IBM in 1984. After a year or so working as an independent consultant, in 1985 he formed, along with colleagues Sharon Weinberg (later Sharon Codd) and Chris Date, two companies—The Relational Institute and Codd &amp; Date Consulting Group—specializing in all aspects of relational database management, relational database design, and database product evaluation. (C&amp;DCG subsequently grew into a family of related companies, including a parent company called Codd &amp; Date International and a European subsidiary called Codd &amp; Date Limited.)</p>
<p>Over the next several years, Codd saw the relational database industry grow and flourish, to the point where it was—and continues to be—worth many tens of billions of dollars a year (though he himself never benefited directly from that huge financial growth). Throughout that period, and indeed for the remainder of his professional life, he worked tirelessly to encourage vendors to develop fully relational products and to educate users, vendors, and standards organizations regarding the services such a product would provide and why users need such services. He was also interested in the possibility of extending his relational ideas to support complex data analysis, coining the term OLAP (On-Line Analytical Processing) as a convenient label for such activities. At the time of his death, he was investigating the possibility of applying his ideas to the problem of general business automation.</p>
<p>Codd died April 18th, 2003, in Williams Island, Florida. He is survived by his wife Sharon; his first wife Libby; a daughter, Katherine; three sons, Ronald, Frank, and David; and several grandchildren.</p>
<p style=""text-align: right;""><span class=""callout"">Author: C. J. Date</span><br>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/codd_1000892.cfm""><img src=""/images/lg_aw/1000892.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Edgar F. Codd""></a>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>August 19th, 1923, Isle of Portland, England</p>
<h6 class=""label"">DEATH:</h6>
<p>April 18th, 2003, Williams Island,&nbsp;Florida</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Honors degree in mathematics (B.A., subsequently M.A.), Exeter College, Oxford University, England (1942 and 1948); M.Sc. and Ph.D., computer and communication sciences, University of Michigan, Ann Arbor, Michigan (1961 and 1965).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Flight lieutenant, Royal Air Force (1942-1946); lecturer in mathematics, University of Tennessee (1949); programming mathematician and computer scientist, IBM (1949-1953 and 1957-1984); head of data processing, Computing Devices of Canada (1953-1957); chief scientist, The Relational Institute (1985-1994). Codd was also active at various times on various editorial boards, including those of the IBM Systems Programming Series of books, IEEE Transactions on Software Engineering, ACM Transactions on Database Systems, and the Journal of Information Systems.</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Fellow, British Computer Society (1974); IBM Fellow (1976); ACM Turing Award (1981); Elected member, National Academy of Engineering (1981); IDUG 1st Annual Achievement Award (1986); ACM Fellow (1994); Elected member, American Academy of Arts and Sciences (1994); IEEE Computer Pioneer Award (1996); DAMA International Achievement Award (2001); Inductee, Computing Industry Hall of Fame (2004, post.); and Member, Phi Beta Kappa and Sigma Xi.<br>
In 2004, the ACM Special Interest Group on Management of Data (SIGMOD), with the unanimous approval of ACM Council, decided to change the name of its annual innovations award to the SIGMOD Edgar F. Codd Innovations Award to honor Codd who invented the relational model and was responsible for the significant development of the database field as a scientific discipline. SIGMOD, now one of the largest of the ACM Special Interest Groups, had its origins in an earlier ACM organization called SICFIDET (Special Interest Committee on File Definition and Translation), which was founded by Codd himself in 1970.</p>","","https://dl.acm.org/author_page.cfm?id=81100425534","Edgar F. (""Ted"") Codd","<li class=""bibliography""><a href=""/bib/codd_1000892.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283937&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/codd_1000892.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/codd_1000892.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179357-679","https://amturing.acm.org/award_winners/brooks_1002187.cfm","For landmark contributions to computer architecture, operating systems, and software engineering.","<p><strong>Frederick Phillips Brooks, Jr. was born April 19, 1931, in Durham, North Carolina.</strong> Growing up in the Raleigh/Durham region, he earned his AB in physics at Duke University in 1953. Brooks then joined the pioneering degree program in computer science at Harvard University, where he earned his SM in 1955 and his PhD in 1956. At Harvard he was a student of Howard Aiken, who during World War II developed the <a href=""https://en.wikipedia.org/wiki/Harvard_Mark_I"" target=""_blank"">Harvard Mark I</a>, one of the largest electromechanical calculators ever built, and the first automatic digital calculator built in the United States.</p>
<p>After graduation Brooks was recruited by IBM, where for the first several years of his career he served in various positions in Poughkeepsie and Yorktown Heights, New York. During that time he helped design the <a href=""https://en.wikipedia.org/wiki/IBM_7030_Stretch"" target=""_blank"">IBM 7090 “Stretch”</a> supercomputer, so called because it was a considerable “stretch” to the technology and performance of most computers of the time. Stretch was IBM’s first transistorized computer, containing some 150,000 transistors. Although it was a commercial failure, it pioneered a number of advanced concepts quite important to contemporary computing, such as instruction look-ahead, overlapping and pipelining of instruction execution, error checking and correction, and the 8-bit addressable character. Brooks and fellow engineer Dura Sweeney patented an interrupt system for the Stretch which has been widely copied as an essential mechanism in all contemporary computers that conduct concurrent activities and react to events from the physical world (U.S. Patent 3,048,332). Brooks went on to participate in the design of the architecture of the IBM Harvest, a variant of the Stretch with special features for the National Security Agency. He later helped the government assess the computing capability of the Soviet Union.</p>
<p>Brooks was next assigned to help design the IBM 8000, a new transistorized mainframe computer intended to replace the IBM 700/7000 series. But by the early 1960s, the global market for computers was incredibly crowded, with numerous companies offering incompatible, proprietary systems. As customers replaced their older systems with faster ones, they realized that their investment in software was a growing problem, because they had to rewrite it for every new system. Bob Evans promoted IBM’s vision to develop a single product line of general purpose computers with a common instruction set that permitted customers to preserve their investment in software as the moved from slower machines to faster ones. Evans assigned Brooks to lead the team to design this product line, called the System/360, which was announced in 1964. Brooks coined the term “computer architecture” to mean the structure and behavior of computer processors and associated devices, as separate from the details of any particular hardware implementation.</p>
<p>The importance of the System /360 cannot be understated: it was a widely successful project that transformed the face of business computing and reshaped the landscape of the computer companies throughout the world. Among many important contributions to the design of the System/360, Brooks was particularly proud of the 8-bit byte, which permitted the use of uppercase and lowercase alphabets and expanded the role of computers in text processing.</p>
<p>While the hardware architecture for the System/360 was well underway, it was clear that there was considerable risk in delivering the operating system for the new series of machines. Brooks was assigned to lead the software team in building what was perhaps the largest operating system project of its time. Brooks describes the lessons he learned in his classic text on software engineering,<em> The Mythical Man-Month</em>. It is from that experience that Brooks proposed “Brook’s Law”: that “adding manpower to a late software project makes it later.”</p>
<p>After the successful delivery of the System/360 and its operating system, Brooks was invited to the University of North Carolina, where he founded the University’s computer science department in 1964. He chaired the department from 1964 to 1984, and served as the Kenan Professor of Computer Science. His principal research area, real-time three-dimensional graphics, provides virtual environments that let biochemists reason about the structure of complex molecules, and let architects walk through buildings under design. Brooks has also pioneered the use of a haptic force feedback display to supplement visual graphics.</p>
<p>Brooks married Nancy Lee Greenwood in Falls Church, Virginia, on June 17, 1956. They have three children and nine grandchildren. In addition to his professional roles, Brooks has been involved in church activities and in national politics.</p>
<p>More photos of Fred Brooks can be found <a href=""/photo/brooks_1002187.cfm"">here</a></p>
<p style=""text-align: right;""><span class=""callout"">Author: Grady Booch</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/brooks_1002187.cfm""><img src=""/images/lg_aw/1002187.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Frederick Brooks""></a>
<br><br>
<h6 class=""label""><a href=""/photo/brooks_1002187.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>April 19, 1931, Durham, North Carolina, United States</p>
<h6 class=""label"">EDUCATION:</h6>
<p>AB, Duke University (1953– physics); SM, Harvard University (1955 – computer science); PhD Harvard University (1956 – applied mathematics/computer science); Honorary PhD ETH-Zurich (1991 - technical science)</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>IBM Corporation (1956 – 1965); University of North Carolina (from 1964); Defense Science Board (1983 – 1986); Artificial Intelligence Task Force (1983 – 1984); Military Software Task Force chairman (1985 – 1987); Computers in Simulation and Training Task Force (1986 – 1987); National Science Board (1987 – 1992)</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>IEEE Fellow (1968); IEEE McDowell Award (1970); DPMA Computer Science Award (1970); Guggenheim Fellowship, Cambridge University (1975); American Academy of Arts and Sciences Fellow (1976); member, National Academy of Engineering (1976); IEEE Computer Society Pioneer Award (1980);: National Medal of Technology (1985); University of North Carolina Thomas Jefferson Award (1986); ACM Distinguished Service Award (1987); AFIPS Harry Goode Award (1989); Royal Netherlands Academy of Arts and Sciences member (1991); IEEE John von Neumann Medal (1993); ACM Fellow (1994); British Computer Society Distinguished Fellow (1994); member UK Royal Academy of Engineering (1994); ACM Allen Newell award (1994); Franklin Institute Bower Award and Prize in Science (1995); CyberEdge Journal Sutherland Award (1997); ACM Turing Award (1999); member National Academy of Science (2001); Fellow Computer History Museum (2001); ACM/IEEE Eckert-Mauchly Award (2004); IEEE Virtual Reality Career Award (2010).<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100077256","Frederick (""Fred"") Brooks","<li class=""bibliography""><a href=""/bib/brooks_1002187.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""key-words""><a href=""/keywords/brooks_1002187.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/brooks_1002187.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179164-667","https://amturing.acm.org/award_winners/shamir_2327856.cfm","Together with Leonard M. Adleman and Ronald Rivest, for their ingenious contribution to making public-key cryptography useful in practice.","<div class=""bibliography"">
<p>
Adi Shamir is an internationally recognized cryptographer. He has a number of claims to fame including being a co-inventor of the RSA public-key cryptography algorithm for encoding and decoding messages, co-inventor of a zero-knowledge proof scheme that allows one individual to show they know certain information without actually divulging it, and a major contributor to what has become known as differential cryptanalysis as well as other significant contributions to computer science.</p>
<p>
Shamir was born in Tel Aviv in 1952. After attending local schools he enrolled in Tel Aviv University, obtaining a BSc in mathematics in 1973 and then went to the Weizmann Institute where he studied computer science and received his MSc (1975) and PhD (1977). After completing his doctorate he spent a year at the University of Warwick in Coventry, England continuing with his research in a postdoctoral position. In 1978 he joined the research staff at the Massachusetts Institute of Technology (MIT).</p>
<p>
At MIT he met <a href=""/award_winners/rivest_1403005.cfm"">Ronald Rivest</a>, and <a href=""/award_winners/adleman_7308544.cfm"">Leonard M. Adleman</a> who collaborated with Adi on their fundamental advance in cryptography. They were inspired by a 1976 paper [<a href=""/bib/shamir_0028491.cfm#bib_3"">3</a>] by cryptographers <a href=""https://en.wikipedia.org/wiki/Whitfield_Diffie"" target=""_blank"">Whitfield Diffie</a> and <a href=""https://en.wikipedia.org/wiki/Martin_Hellman"" target=""_blank"">Martin Hellman</a> discussing several new developments in cryptography. It described ways for the sender and receiver of private messages to avoid needing a shared secret key, but it did not provide any realistic way to implement these concepts. Rivest, Shamir, and Adleman presented practical implementations in their 1977 paper, “A method for obtaining digital signatures and public-key cryptosystems,” [<a href=""/bib/shamir_0028491.cfm#bib_1"">1</a>] which showed how a message could easily be encoded, sent to a recipient, and decoded with little chance of it being decoded by a third party who sees it.</p>
<p>
The method, known as <a href=""https://en.wikipedia.org/wiki/Public-key_cryptography"" target=""_blank"">Public Key Cryptography</a>, uses two different but mathematically linked keys: one public key used to encrypt the message, and a completely different private key used to decrypt it. The encrypting key is made public by individuals who wish to receive messages, but the secret decrypting key is known only them. The two keys are linked by some well-defined mathematical relationship, but determining the decryption key from its publically available counterpart is either impossible or so prohibitively expensive that it cannot be done in practice. The “RSA” method (from the first letters of the names of the three authors) relies on the fact that nobody has yet developed an efficient algorithm for <a href=""https://en.wikipedia.org/wiki/Prime_factor"" target=""_blank"">factoring</a> very large integers. There is no guarantee, however, that it will be difficult forever; should a large <a href=""https://en.wikipedia.org/wiki/Quantum_computer"" target=""_blank"">quantum computer</a> ever be built, it might be able to break the system.</p>
<p>
A detailed discussion of the theory and practice behind RSA can be found <a href=""https://en.wikipedia.org/wiki/RSA_%28algorithm%29"" target=""_blank"">here</a>. The computer code to implement it is quite simple, and as long as suitably large integer keys are used, no one knows how to break an encoded message.</p>
<p>
RSA is used in almost all internet-based commercial transactions. Without it, commercial online activities would not be as widespread as they are today. It allows users to communicate sensitive information like credit card numbers over an unsecure internet without having to agree on a shared secret key ahead of time. Most people ordering items over the internet don’t know that the system is in use unless they notice the small padlock symbol in the corner of the screen. RSA is a prime example of an abstract elegant theory that has had great practical application.</p>
<p>
After developing the basic method in 1977, the three Turing Award recipients founded <a href=""https://en.wikipedia.org/wiki/RSA_%28security_firm%29"" target=""_blank"">RSA Data Security</a> in 1983. The company was later acquired by Security Dynamics, which was in turn purchased by EMC in 2006. It has done cryptographic research, sponsored conferences, shown how earlier encryption systems could be compromised, and spun off other companies such as <a href=""https://en.wikipedia.org/wiki/VeriSign"" target=""_blank"">Verisign</a>. When the 1983 patent on RSA [<a href=""/bib/shamir_0028491.cfm#bib_2"">2</a>] was about to expire, RSA Data Security published all the details of its implementation so that there would be no question that anyone could create products incorporating the method.</p>
<p>
The three Turing Award recipients were not aware that a similar method had been developed years before by British mathematician <a href=""https://en.wikipedia.org/wiki/Clifford_Cocks"" target=""_blank"">Clifford Cocks</a>, who extended the even earlier work of <a href=""https://en.wikipedia.org/wiki/James_H._Ellis"" target=""_blank"">James H. Ellis</a>. Cocks was doing his encryption work at the British Government Communications Headquarters (GCHQ), so it was classified as secret and not released until 1997, twenty years after Rivest, Adleman, and Shamir had published their independent discovery.</p>
<p>
Another of Adi’s contributions is known as <a href=""https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing"" target=""_blank"">Shamir’s Secret Sharing</a>. This is a scheme in which a number of pieces of the secret are shared between individuals, but it requires either some or all of them to collaborate in order to reveal the total secret. It is essentially a mathematical mechanism equivalent to having several individuals present with their physical and other keys before an ICBM can be launched. The scheme is flexible enough to accommodate the situation where, for example, a senior individual can unlock the secret alone but it requires three or more junior officials to unlock the answer. A simple example can be found <a href=""https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing"" target=""_blank"">here</a>.</p>
<p>
Shamir’s interest in cryptography has led him to investigate methods of attacking the decoding of a message. He and <a href=""https://en.wikipedia.org/wiki/Eli_Biham"" target=""_blank"">Eli Biham</a>, Adi’s graduate student, are usually given credit for the invention of what has become known as <a href=""https://en.wikipedia.org/wiki/Differential_cryptanalysis"" target=""_blank"">differential cryptanalysis</a>, although the mechanism was evidently known, and kept secret, by IBM and the American National Security Agency (NSA) prior to the 1993 public release of Shamir’s and Biham’s book [<a href=""/bib/shamir_0028491.cfm#bib_4"">4</a>] on the subject. It involves a series of tests to code variations on a plain text message and note the differences in the resulting coded output. This can be used to discover where the cipher shows non-random behavior which can then be used to ease the recovery of the secret key. The discovery came when they were investigating the security of the 1977 <a href=""https://en.wikipedia.org/wiki/Data_Encryption_Standard"" target=""_blank"">Data Encryption Standard </a>(DES) and they noted that the algorithm for the encoding was created in such a way that even a small modification would have made breaking the code much easier. It turned out that IBM and NSA, already knowing something about the techniques, had deliberately designed it with that in mind.</p>
<p>
Another of Adi’s advances is known as <a href=""https://en.wikipedia.org/wiki/ID-based_cryptography"" target=""_blank"">identity-based cryptography</a>. In this mechanism the public key used in RSA is simply some easily obtained information about the potential recipient of a message. It could be something as simple as the recipient’s email address, domain name, or a physical IP address. The first implementation of identity-based signatures and an email-address based system was developed by Adi 1984 [<a href=""/bib/shamir_0028491.cfm#bib_5"">5</a>]. &nbsp;It allowed users to digitally “sign” documents using only publicly available information.</p>
<p>
Shamir also proposed a very similar <a href=""https://en.wikipedia.org/wiki/ID-based_encryption"" target=""_blank"">identity-based encryption</a> scheme which was of interest because it did not require the user to obtain a public-key to be used in encrypting a message. While Shamir had the initial concept in 1984, the first actual implementation was done in 2001 by two different groups [<a href=""/bib/shamir_0028491.cfm#bib_6"">6</a>,<a href=""/bib/shamir_0028491.cfm#bib_7"">7</a>].&nbsp;</p>
<p>
In 1994 Shamir collaborated with <a href=""https://en.wikipedia.org/wiki/Moni_Naor"" target=""_blank"" title=""Moni Naor"">Moni Naor</a> to produce yet another interesting scheme known as <a href=""https://en.wikipedia.org/wiki/Visual_cryptography"" target=""_blank"">visual cryptography</a> [<a href=""/bib/shamir_0028491.cfm#bib_8"">8</a>]. &nbsp;An image (which could be text) is broken up in such a way that the resulting pieces appear to be simply a random scattering of white and dark pixels. When all the pieces are overlaid the message appears. The beauty of this scheme is that if someone manages to gather all but one of the pieces the message is still unreadable. It is even more interesting than being “unreadable” in that it is possible to construct a missing piece that will reveal <u>any</u> message and thus the secret will remain hidden until the last true piece is put in place. While this sounds good, it also means that someone with all but one piece is capable of deception by constructing a final piece to show any message they like. A simple example is available <a href=""https://en.wikipedia.org/wiki/Visual_cryptography"" target=""_blank"">here</a>.</p>
</div>","<div class=""featured-photo"">
<a href=""/award_winners/shamir_2327856.cfm""><img src=""/images/lg_aw/2327856.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Adi Shamir""></a>
</div>
<h6 class=""label""><strong>BIRTH:</strong></h6>
<p>July 6, 1952, Tel Aviv, Israel</p>
<h6 class=""label""><strong>EDUCATION:</strong></h6>
<p>BSc (Mathematics, Tel Aviv University, 1973); MSc (Computer Science, Weizmann Institute, Israel, 1975); PhD (Computer Science, Weizmann Institute, Israel, 1977)</p>
<h6 class=""label""><strong>EXPERIENCE:</strong></h6>
<p>Post doctorate position, Warwick University, England (1976); Instructor, Department of Mathematics, MIT (1977-1978); Assistant Professor Department of Mathematics, MIT (1978-1980); Associate Professor at Department of Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel (1980-1984); Professor, Department of Applied Mathematics, The Weizmann Institute of Science, Rehovot, Israel(1984 onward) Invited Professor, École Normale Supérieure, Paris (2006 onward).</p>
<h6 class=""label""><strong>HONORS &amp; AWARDS:</strong></h6>
<p><span style=""line-height: 20.8px;"">Israel Mathematical Society&nbsp;</span>Erd?s<span style=""line-height: 20.8px;"">&nbsp;Prize (1983);&nbsp;</span><span style=""line-height: 20.8px;"">IEEE&nbsp;</span>WRG<span style=""line-height: 20.8px;"">&nbsp;Baker Award (1986);&nbsp;</span>UAP<span style=""line-height: 1.3;"">&nbsp;Scientific Prize (1990);&nbsp;</span><span style=""line-height: 1.3;"">Vatican Pontifical Academy PIUS XI Gold Medal (1992);&nbsp;</span><span style=""line-height: 1.3;"">ACM Paris&nbsp;</span>Kanellakis<span style=""line-height: 1.3;"">&nbsp;Theory and Practice Award, jointly with others for RSA (1996);&nbsp;</span><span style=""line-height: 1.3;"">Elected to the Israeli Academy of Science (1998);&nbsp;</span><span style=""line-height: 1.3;"">IEEE&nbsp;</span>Koji<span style=""line-height: 1.3;"">&nbsp;Kobayashi Computers and Communications Award (2000);&nbsp;</span><span style=""line-height: 1.3;"">ACM Turing Award, jointly with R.&nbsp;</span>Rivest<span style=""line-height: 1.3;"">&nbsp;and L.&nbsp;</span>Adleman<span style=""line-height: 20.8px;"">&nbsp;(2002</span><span style=""line-height: 1.3;"">);&nbsp;</span><span style=""line-height: 1.3;"">Fellow, International Association of Cryptographic Research (2004); Elected to the US National Academy of Sciences (2005);&nbsp;</span><span style=""line-height: 20.8px;"">Israel Prize (2008).&nbsp;</span><span style=""line-height: 1.3;"">Honorary Doctorates from </span>École<span style=""line-height: 1.3;""> </span>Normale<span style=""line-height: 1.3;""> </span>Supérieure<span style=""line-height: 1.3;""> (2003) and the University of Waterloo (2009).</span></p>","","https://dl.acm.org/author_page.cfm?id=81100081898","Adi Shamir","<li class=""bibliography""><a href=""/bib/shamir_2327856.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/shamir_2327856.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/shamir_2327856.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/shamir_2327856.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179777-708","https://amturing.acm.org/award_winners/sutherland_3467412.cfm","For his pioneering and visionary contributions to computer graphics, starting with Sketchpad, and continuing after.","<p>
<strong>Ivan Edward Sutherland was born May 16, 1938 in Hastings, Nebraska, United States.</strong> His ancestry is from Scotland and New Zealand, despite his claim that he was named for his mother’s fictitious Russian lover. His father was a practicing engineer with a Ph.D. in civil engineering. His mother was a teacher who engendered in him and his brother Bert a love of learning.</p>
<p>
In 8th grade he built a gantry crane with surplus motors brought home by his father. His favorite subject in high school was geometry. Sutherland describes himself as a visual thinker (“If I can picture possible solutions, I have a much better chance of finding the right one”), which led to his interest in computer graphics.</p>
<p>
His first computer processing experience was with a computer called <a href=""https://en.wikipedia.org/wiki/Simon_%28computer%29"" target=""_blank"">SIMON</a>, a relay-based computer with six words of two-bit memory, which was lent to the Sutherland household in 1950 by its designer, <a href=""https://en.wikipedia.org/wiki/Edmund_Berkeley"" target=""_blank"">Edmund Berkeley</a>, one of the founders of the ACM. Its 12 bits of memory permitted SIMON to add up to 15. Sutherland's first significant program allowed SIMON to divide. To make division possible, he used a table look-up and added a conditional stop to SIMON's instruction set. His brother Bert participated by making the modifications to the hardware. This program was a significant accomplishment—it was the longest program ever written for SIMON, requiring eight feet of paper tape.</p>
<p>
Sutherland was one of only a few young students writing computer programs during that era. For a 12th grade science fair project, he made a magnetic drum memory with 128 2-bit words. At age 19, he published “An Electro-Mechanical Model of Simple Animals” [<a href=""/bib/sutherland_3467412.cfm#link_6"">6</a>] in Computers and Automation. At age 21 he published “Stability in Steering Control” [<a href=""/bib/sutherland_3467412.cfm#link_7"">7</a>] in Electrical Engineering.</p>
<p>
After graduating from Scarsdale High School (20 miles north of New York City) in 1955, Sutherland attended Carnegie Institute of Technology (now Carnegie Mellon University) on a full scholarship, which made it affordable for him. He received a Bachelor of Science Degree from Carnegie in 1959, a Master of Science Degree from the California Institute of Technology in 1960 (which, he claims, he selected to get as far as possible from his new mother-in-law), and a Doctor of Philosophy Degree in Electrical Engineering from the Massachusetts Institute of Technology in 1963. His doctoral thesis supervisor was Claude E. Shannon, who is often referred to as the “father of information theory. Sutherland also received an Honorary Master of Arts Degree from Harvard University in 1966.</p>
<p>
His doctoral thesis, Sketchpad: A Man-machine Graphical Communications System, described the first computer graphical user interface (GUI). <a href=""/photo/sutherland_3467412.cfm#photo_1"">Sketchpad</a> was developed on a unique computer, the TX-2, built by Wesley Clark. In early 1960s computers ran ""batches"" of jobs, but were not interactive. The TX-2 was an ""on-line"" computer used to investigate surface barrier transistors for digital circuits. It had a variety of input and output devices, including a nine inch CRT with a 512 x 512 array of directly-addressed pixels and no hardware character generator. For software it had virtually nothing—no operating system, just a macro assembler. But it had a light pen, first used on the SAGE air defense project for identifying computer-drawn objects. Sutherland used it to do something new: allow the user to draw directly on the computer display. The light pen provided coordinates for drawing commands entered using the keyboard. Previously drawn primitive objects could be recalled, rotated, scaled and moved. Finished drawings could be stored on magnetic tape and edited at a later time. Sketchpad introduced important innovations including hierarchical memory structures to organize objects and the ability to zoom in and out.</p>
<p>
Sketchpad was a groundbreaking interactive computer-aided design system. Its innovations included hierarchical drawings, constraint-satisfaction methods, and an interactive GUI. When asked, “How could you possibly have done the first interactive graphics program, the first non-procedural programming language, the first object oriented software system, all in one year?” Sutherland replied, “Well, I didn't know it was hard<a href=""/award_winners/sutherland_3467412.cfm#foot_1""><sup>1</sup></a>.” Sketchpad's graceful interaction and functionality continue to inspire admiration among computer graphics professionals.</p>
<p>
After graduating from MIT in 1963, Sutherland accepted a U.S. Army commission to fulfill his obligation for military service. He was assigned first to the University of Michigan, and then to the National Security Agency as an electrical engineer, mainly because he knew about computers. In 1964, at age 26, First Lieutenant Sutherland replaced J. C. R. Licklider (who returned to private industry) as the head of the U.S. Department of Defense Advanced Research Project Agency's (DARPA) Information Processing Techniques Office (IPTO). DARPA was started in response to Sputnik to develop technology for putting a U. S satellite in space. As head of this office Sutherland was given $15 million a year to sponsor computer research, particularly in the areas of timesharing and artificial intelligence.</p>
<p>
From 1965 to 1968, Sutherland was an Associate Professor of Electrical Engineering at Harvard University. Work with student Danny Cohen in 1967 led to the development of the Cohen–Sutherland computer graphics <a href=""https://en.wikipedia.org/wiki/Line_clipping"" target=""_blank"">line clipping algorithm</a> for removing parts of lines that extend beyond the viewing region. In 1968, with the help of student Bob Sproull, he created the first virtual reality and augmented reality <a href=""/photo/sutherland_3467412.cfm#photo_2"">head-mounted display</a> system, referred to affectionately as the Sword of Damocles because it was suspended from the ceiling above the user’s head.</p>
<p>
From 1968 to 1974 Sutherland was a Professor of Computer Science at the University of Utah. Among his noted students there were Jim Clark (who designed a virtual reality system and went on to found Silicon Graphics, Netscape, and WebMD), and Henri Gouraud (who devised the <a href=""https://en.wikipedia.org/wiki/Gouraud_shading"" target=""_blank"">Gouraud smooth-shading technique</a> to make a surface approximated by polygons look smooth). He served as a PhD committee member for other famous Utah graduates <a href=""/award_winners/kay_3972189.cfm"">Alan Kay</a> (inventor of the Smalltalk language and 2003 Turing Award recipient) and Edwin Catmull (co-founder of Pixar and later President of Walt Disney and Pixar Animation Studios).</p>
<p>
In 1968 Sutherland co-founded Evans &amp; Sutherland with his friend and colleague David C. Evans, whom he had met at the University of California Berkeley. Evans’ understanding of real-time computing held the key to implementing practical computer graphics. The company, of which Sutherland was Vice President and Chief Scientist, was located in Salt Lake City, Utah. Evans &amp; Sutherland pioneered work in the field of real-time hardware, accelerated 3D computer graphics, and printer languages. Former employees of Evans &amp; Sutherland included the future founders of Adobe (John Warnock) and Myricom (Chuck Seitz).</p>
<p>
Starting in the mid-1970s, Sutherland was affiliated with the RAND Corporation, and investigated making animated movies—an undertaking well ahead of its time.</p>
<p>
From 1974 to 1978 he was the Fletcher Jones Professor of Computer Science at California Institute of Technology, where he was the founding head of that school's Computer Science Department. One area of emphasis was teaching engineers how to design integrated circuits. In 1980 he founded a consulting firm, Sutherland, Sproull and Associates, and served as its Vice President and Technical Director. It was purchased by Sun Microsystems in 1990 to form the seed of its research division, Sun Labs. Sutherland became a Fellow and Vice President at Sun Microsystems.</p>
<p>
Sutherland was a visiting scholar in the Computer Science Division at University of California, Berkeley from 2005 to 2008.</p>
<p>
Other prominent work includes the characterization and categorization of hidden surface algorithms at the University of Utah (1974), and the <a href=""/photo/sutherland_3467412.cfm#photo_3"">Trojan Cockroach</a>, a six-legged walking machine built at Carnegie Mellon University (1983).</p>
<p>
During his career Sutherland has obtained more than 60 <a href=""/info/sutherland_3467412.cfm"">patents</a>.</p>
<p>
In 2006 Ivan Sutherland married Marly Roncken, with whom he established the Asynchronous Research Center at Portland State University to develop self-timed asynchronous computers without the central clocks that must otherwise accommodate the slowest components.</p>
<p>
Sutherland has two children, Juliet and Dean, and four grandchildren, Belle, Robert, William and Rose. Ivan's elder brother, Bert Sutherland, is also a prominent computer scientist.</p>
<p>
<strong>Notable quotes attributed to Sutherland</strong> include:</p>
<ul>
<li>
“It’s not an idea until you write it down.”</li>
<li>
""I just need to figure out how things work.""</li>
<li>
""It's very satisfying to take a problem we thought difficult and find a simple solution. The best solutions are always simple.""</li>
<li>
""A display connected to a digital computer gives us a chance to gain familiarity with concepts not realizable in the physical world. It is a looking glass into a mathematical wonderland.”</li>
<li>
""The ultimate display would, of course, be a room within which the computer can control the existence of matter. A chair displayed in such a room would be good enough to sit in. Handcuffs displayed in such a room would be confining, and a bullet displayed in such a room would be fatal.”</li>
<li>
""When denied my minimum daily adult dose of technology, I get grouchy… without the fun (e.g. the Trojan Cockroach – featured on the cover of Scientific American in January, 1983), none of us would go on.”</li>
<li>
“Never underestimate the power of the professorial windbag;”</li>
<li>
""Knowledge is a rare thing -- you gain by giving it away.”</li>
</ul>
<p>
<strong>Words of wisdom</strong> include:</p>
<ul>
<li>
“Interesting colleagues are essential.”</li>
<li>
“Cherish and develop young minds.”</li>
<li>
“What’s worth working on is what you are deeply qualified to do.”</li>
<li>
“If it isn’t fun, you’re doing the wrong technology.”</li>
</ul>
<p style=""text-align: right;"">
<span class=""callout"">Author: Robert Burton</span></p>
<p>
<sup><a name=""foot_1""></a></sup><sup>1</sup>Quoted by Alan Kay in 1987, in a University Video Communications video,<br>
<a href=""https://www.archive.org/details/AlanKeyD1987"" target=""_blank"">http://www.archive.org/details/AlanKeyD1987</a>.</p>","<div class=""featured-photo"">
<a href=""/award_winners/sutherland_3467412.cfm""><img src=""/images/lg_aw/3467412.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Ivan Sutherland""></a>
<br><br>
<h6 class=""label""><a href=""/photo/sutherland_3467412.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>16 May 1938, Hastings, Nebraska, United States</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Scarsdale High School, Scarsdale, New York, United States (1955); B.S. (electrical engineering) Carnegie Institute of Technology, 1959; M.S. (electrical engineering) California Institute of Technology,1960; Ph.D. (electrical engineering) Massachusetts Institute of Technology,1963, Honorary M.A. Harvard University, (1966)</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Research Laboratory of Electronics at Massachusetts Institute of Technology, (1960 – 1962); Massachusetts Institute of Technology Lincoln Laboratory (1960 – 1962); U.S. Army (1963,University of Michigan, then the National Security Agency); Head, Information Processing Techniques Office (IPTO), U.S. Department of Defense Advanced Research Projects Agency (DARPA) (1964); Associate Professor of Electrical Engineering, Harvard University (1965 – 1968); Professor of Computer Science, University of Utah, (1968 – 1974); co-founder of Evans &amp; Sutherland Computer Corporation (1968); the RAND Corporation (1974); Fletcher Jones Professor of Computer Science and Head, Department of Computer Science, California Institute of Technology (1974 – 1978); co-founder, Vice President, and Technical Director; Sutherland, Sproull and Associates (1980); Fellow and Vice President, Sun Microsystems (1990); Visiting Scholar, Computer Science Division, the University of California, Berkeley (2005 – 2008); Visiting Scientist and Head, Asynchronous Research Center, Portland State University (from 2008).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>George Westinghouse Scholar (1955-1959); American Institute of Electrical Engineers Student Prize Paper Contest for District 2 Winner (1958, 1959); National Science Foundation Fellowship (1959 – 1962); National Academy of Engineering First Zworykin Award (1972); Member, National Academy of Engineering (1973); Member, National Academy of Sciences (1978); IEEE Emanuel R. Piore Award (1986); Computerworld Honors Program, Leadership Award (1987); ACM Turing Award (1988); ACM Software System Award (1993); Electronic Frontier Foundation EFF Pioneer Award (1994); Association for Computing Machinery Fellow (1994); Price Waterhouse Information Technology Leadership Award for Lifetime Achievement (1996); Computerworld Smithsonian Award (1996); the Franklin Institute's Certificate of Merit (1996); IEEE John von Neumann Medal (1998); R&amp;D 100 Award (team) (2004); Computer History Museum Fellow (2005); Kyoto Prize (2012).<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100265287","Ivan Sutherland","<li class=""bibliography""><a href=""/bib/sutherland_3467412.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283946&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/sutherland_3467412.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/sutherland_3467412.cfm""><span></span>Additional<br> Materials</a></li>"
"1573178854-645","https://amturing.acm.org/award_winners/lampson_1142421.cfm","For contributions to the development of distributed, personal computing
environments and the technology for their implementation: workstations, networks, operating systems,
programming systems, displays, security and document publishing.","<p>Butler Lampson was born in Washington DC and educated at <a href=""https://en.wikipedia.org/wiki/Lawrenceville_School"" target=""_blank"">The Lawrenceville School</a>, an elite boarding school 6 miles from Princeton New Jersey.</p>
<p>Butler’s first computer was an underutilized <a href=""https://en.wikipedia.org/wiki/IBM_650"" target=""_blank"">IBM 650</a> at Princeton, which an enterprising high school classmate found and got permission to use while it was idle. It had a drum memory with 2000 words of ten decimal digits, and the only I/O aside from the console was a card reader/punch, controlled partly by the computer and partly by a plug board.</p>
<p>As an undergraduate at Harvard, Lampson learned the APL programming language from <a href=""/award_winners/iverson_9147499.cfm"">Ken Iverson</a>, who spent a year on sabbatical there. At that time there was no implementation of the language, and Ken didn’t want one because he was sure that it would make compromises that would wreck the language. While still an undergraduate Lampson also programmed a <a href=""https://en.wikipedia.org/wiki/PDP-1"" target=""_blank"">PDP-1</a> to analyze spark chamber photographs from the Cambridge Electron Accelerator and wrote a display editor for the PDP-1’s one-point-at-a-time display.</p>
<p>Butler went to the University of California at Berkeley in the fall of 1964 as a graduate student in physics. At the Fall Joint Computer Conference in San Francisco that year he ran across Steve Russell from MIT, who told him about the <a href=""https://en.wikipedia.org/wiki/Project_Genie"" target=""_blank"">Genie project</a> hidden behind an unmarked door in Cory Hall. There he met <a href=""https://en.wikipedia.org/wiki/L_Peter_Deutsch"" target=""_blank"">Peter Deutsch</a> and Mel Pirtle, the Principal Investigator, and was quickly seduced away from physics to study computers.</p>
<p>The Genie project modified a Scientific Data Systems <a href=""https://en.wikipedia.org/wiki/SDS_930"" target=""_blank"">SDS 930</a> minicomputer to create the first time-sharing system with character-by-character interaction. Later SDS marketed it as the SDS 940, the first commercially available general purpose time-sharing system. Butler wrote parts of the operating system and several programming languages, notably Cal, an interactive language for numerical computation derived from Cliff Shaw’s <a href=""https://en.wikipedia.org/wiki/JOSS"" target=""_blank"">Joss</a>, and the QSPL system programming language done with Peter Deutsch.</p>
<p>At Berkeley he also designed the <a href=""https://dl.acm.org/citation.cfm?id=360051.360074&amp;coll=DL&amp;dl=ACM"" target=""_blank"">Cal time-sharing system</a> for a CDC 6400 in the computer center, together with <a href=""/award_winners/gray_3649936.cfm"">Jim Gray</a>, <a href=""https://en.wikipedia.org/wiki/Charles_Simonyi"" target=""_blank"">Charles Simonyi</a>, Howard Sturgis and <a href=""http://queue.acm.org/detail.cfm?id=1036486"" target=""_blank"">Bruce Lindsay</a>, who all went on to later fame. This was the first capability-based system to have a real user community. It pioneered the ideas of shadow pages and redo logs, but also taught us that capabilities are not a good basis for long-term security.</p>
<p>The Genie project researchers couldn’t figure out how to build the much more grandiose second system at Berkeley, so in 1969 they started Berkeley Computer Corporation (BCC) to do it. After two years it burned through $4,000,000 and built just one working system before folding. Butler designed and coded most of the microcoded parts of the operating system and worked on the SPL system programming language. Around this time he also devised the <a href=""https://dl.acm.org/citation.cfm?id=775265.775268&amp;coll=DL&amp;dl=ACM"" target=""_blank"">access matrix model</a> for computer security, unifying the ideas of capabilities and access control lists.</p>
<p>Luckily, as BCC was ending the Xerox Palo Alto Research Center (PARC) was getting started, and Bob Taylor found the core of his Computer Science Laboratory (CSL) there: Chuck Thacker, Peter Deutsch, Charles Simonyi, Jim Mitchell and Butler. The charter of CSL and its System Science Laboratory partner was to invent and deploy the office of the future. No one knew what that meant, but it provided a lot of scope and just the right amount of direction. The standard research machine at the time was the <a href=""https://en.wikipedia.org/wiki/PDP-10"" target=""_blank"">DEC PDP-10</a> with its Tenex operating system (descended from the 940), but since Xerox had just bought DEC’s main competitor SDS, CSL had to home-build a PDP-10 from scratch in 1971.</p>
<p>Two years later the same technology and tools went into the <a href=""https://en.wikipedia.org/wiki/Xerox_Alto"" target=""_blank"">Alto</a>, the first recognizable modern personal computer. It had a 600 x 800 display (black and white with only one bit per pixel, but that still used half the machine’s 128 KB memory), a hard disk, an Ethernet local area network, and a laser printer. Butler designed some of the machine, wrote the <a href=""https://dl.acm.org/citation.cfm?id=800215.806575&amp;coll=DL&amp;dl=ACM"" target=""_blank"">operating system</a>, and with Ron Rider designed the electronics and software for the prototype laser printer that <a href=""https://en.wikipedia.org/wiki/Gary_Starkweather"" target=""_blank"">Gary Starkweather</a> built, which was later adapted to make the Xerox 9700 laser printer. The Alto software included <a href=""https://en.wikipedia.org/wiki/Bravo_%28software%29"" target=""_blank"">Bravo</a>, the first what-you-see-is-what-you-get text editor, designed by Butler and Charles Simonyi and coded by Charles’ <a href=""https://www.parc.com/publication/1940/meta-programming.html"" target=""_blank"">software factory</a>. Bravo eventually led to the development of Microsoft Word. Butler and <a href=""/award_winners/kay_3972189.cfm"">Alan Kay</a> designed the byte code machine language scheme used for <a href=""https://en.wikipedia.org/wiki/Smalltalk"" target=""_blank"">Smalltalk</a>, <a href=""https://en.wikipedia.org/wiki/Mesa_%28programming_language%29"" target=""_blank"">Mesa</a>, and later versions of Alto software that were widespread in universities, the White House, and several other field test sites.</p>
<p>Other significant work at PARC included the first widely published description of using <a href=""https://dl.acm.org/citation.cfm?id=362375.362389&amp;coll=DL&amp;dl=ACM"" target=""_blank"">information flow control for security</a>, the invention (with Howard Sturgis) of two-phase commit for distributed transactions, and the technique of scrubbing disk storage. Butler also designed much of the system programming language Mesa, especially the module system and the process mechanism, which later was the basis for most modern thread systems. Mesa modules led to <a href=""https://dl.acm.org/citation.cfm?id=800226.806846&amp;coll=DL&amp;dl=ACM"" target=""_blank"">early work</a> with Eric Schmidt on systems for building large systems. Another spinoff was <a href=""https://dl.acm.org/citation.cfm?id=954666.971189&amp;coll=DL&amp;dl=ACM"" target=""_blank"">Euclid</a>, the first language specifically designed to be amenable to program verification. Mesa later led to Cedar, an only partially successful attempt to combine the virtues of Mesa and Lisp; Butler did some of the design and wrote a comprehensive description of the language. All of this hardware and software work led to him write a paper on <a href=""https://dl.acm.org/citation.cfm?id=800217.806614&amp;coll=DL&amp;dl=ACM&amp;CFID=71417228&amp;CFTOKEN=45172585"" target=""_blank"">hints for computer system design</a> that has been quite influential.</p>
<p>Later at PARC Butler led the design of the <a href=""https://dl.acm.org/citation.cfm?id=800053.801920&amp;coll=DL&amp;dl=ACM"" target=""_blank"">Dorado</a>, a personal computer that was much more powerful than the Alto, and of the Wildflower, a less expensive 1979 version of the Alto. The Wildflower was the basis for the hardware of the <a href=""https://en.wikipedia.org/wiki/Xerox_Star"" target=""_blank"">Star</a>, Xerox’s office automation system, built by a development group spun out of PARC led by Dave Liddle. Butler also designed the electronics for the Dover, a much cheaper laser printer that was widely used within Xerox and in several universities.</p>
<p>With Paul Rovner and others, Butler designed Modula 2+, an extension of <a href=""/award_winners/wirth_1025774.cfm"">Niklaus Wirth’s</a>&nbsp;Modula 2, which provides a safe subset, automatic storage deallocation, runtime types, exceptions, and concurrency. The language has been used to write more than a million lines of code. Butler worked with Rod Burstall on languages for describing how a system is built from its component modules. After several false starts this work eventually led to the Vesta system-building system, created by Roy Levin, Yuan Yu and others, and currently remains the state of the art for efficient and reliable construction of large software systems with many developers and many versions.</p>
<p>By 1983 the influential group at PARC was beginning to break up, and many of the senior individuals, including Butler, went to work for the Systems Research Center (SRC) of <a href=""https://en.wikipedia.org/wiki/Digital_Equipment_Corporation"" target=""_blank"">Digital Equipment Corporation</a> (DEC). By this time Butler was living in Philadelphia, where his wife Lois was teaching at the University of Pennsylvania. He turned his interests to the developing e-mail systems. Earlier work by Mike Schroeder, Roger Needham, Andrew Birrell and others on the <a href=""https://dl.acm.org/citation.cfm?id=358487"" target=""_blank"">Grapevine distributed e-mail system</a> led to a design and a formal specification for a decentralized, fault-tolerant distributed name service; this later came to be known as “eventual consistency” and was widely used in highly available systems. Thinking about how to make e-mail secure led to the first scheme for <a href=""https://dl.acm.org/citation.cfm?id=2080.2081&amp;coll=DL&amp;dl=ACM"" target=""_blank"">distributed authentication</a>. Another major SRC project that Butler started was two high-speed switched local area networks with an unprecedented combination of speed, availability, and security.</p>
<p>The work on authentication led to a series of papers that laid the foundations of security for distributed systems. Some of this work was done with Morrie Gasser, Andy Goldstein and Charlie Kaufman for the Digital Distributed Systems Security Architecture, which unfortunately was never implemented. Work at SRC with Mike Burrows, Martin Abadi and Ted Wobber introduced the idea of describing trust in terms of principals “speaking for” other principals, and extended the notion of principal from just users to keys, groups, and programs. It was more formal, and it did have an implementation. It evolved further at MIT in the SDSI/SPKI design done with <a href=""/award_winners/rivest_1403005.cfm"">Ron Rivest</a> and Carl Ellison, and in the <a href=""https://en.wikipedia.org/wiki/Next-Generation_Secure_Computing_Base"" target=""_blank"">Microsoft Palladium</a> scheme, done with Paul England, for platforms that can prove to a third party what software they are running. An attempt at a comprehensive security architecture with Paul Leach and others has been implemented only in fragments. Butler’s later work on security deals with the economic factors that cause deployed security to be much worse than the best we know how to do technically.</p>
<p>In 1987 Butler moved to Cambridge Massachusetts because Lois had joined the faculty of the Harvard Medical School. He continued to work for DEC, and also became an Adjunct Professor in the Electrical Engineering and Computer Science Department at MIT. There he worked with <a href=""https://en.wikipedia.org/wiki/Nancy_Lynch"" target=""_blank"">Nancy Lynch</a> on formal specifications and proof for <a href=""https://en.wikipedia.org/wiki/Transmission_Control_Protocol"" target=""_blank"">TCP connection establishment</a> (also known as at-most-once messages) and for <a href=""https://en.wikipedia.org/wiki/Paxos_%28computer_science%29"" target=""_blank"">Leslie Lamport’s Paxos protocol</a>. He worked to persuade the systems community that Paxos and Leslie’s replicated state machines are the right way to build fault-tolerant systems, and after about 15 years it has been widely adopted. He also became interested in how computing evolves, studying how software components have failed and succeeded. With Dave Tennenhouse he analyzed the combination of technical and economic factors that drive or impede the evolution of telecommunications. The wave of interest in Grand Challenges for computing led to proposals for some very concrete grand challenges: reducing highway traffic deaths to zero with self-driving cars, and getting a computer to write a program from its specification as well as a team of programmers could do it.</p>
<p>In 1995 Butler joined Microsoft Research, where he worked on tablet PC software, security, and end-user programming of rich applications. He has served on the Computer Science and Telecommunications Board and on National Academy panels, studying computer security and directing funding for computing research, military command and control, and supercomputing. His work with the Academy gave rise to the infamous <a href=""http://www.nap.edu/openbook.php?record_id=4948&amp;page=20"" target=""_blank"">tiretracks diagram</a> that explains how government and industry support of basic research in computing has led to dozens of multi-billion-dollar industries.</p>
<p>Butler Lampson has had a very active career. He has made contributions to computer architecture, local area networks, raster printers, page description languages, operating systems, remote procedure call, programming languages and their semantics, programming-in-the-large, fault-tolerant computing, transaction processing, computer security, WYSIWYG editors, and tablet computers. He was a co-designers of many systems, including the SDS 940 time-sharing system, the Alto personal distributed computing system, the Xerox 9700 laser printer, two-phase commit protocols, the Autonet LAN, the SDSI/SPKI system for network security, the Microsoft Tablet PC software, the Microsoft Palladium high-assurance stack, and several programming languages.</p>
<p>In addition to the Turing Award, Butler has won the ACM Software Systems Award for his work on the Alto, the IEEE Computer Pioneer award, the IEEE von Neumann Medal, the Computer History Museum Fellows Award, and the National Academy of Engineering’s Draper Prize. He is a member of the National Academies of Sciences and of Engineering, and the American Academy of Arts and Sciences. He holds honorary ScD’s from the Eidgenössische Technische Hochschule, Zurich and the University of Bologna. Three of his <a href=""/bib/lampson_1142421.cfm"">papers</a> have won SigOps (ACM Special Interest Group on Operating System) Hall of Fame awards.</p>
<p align=""right""><span class=""callout"">Author: Roy Levin</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/lampson_1142421.cfm""><img src=""/images/lg_aw/1142421.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Butler W Lampson""></a>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>December 23, 1943, Washington, D.C.</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>AB in Physics (Harvard, 1964); PhD in Electrical Engineering and Computer Science (University of California, Berkeley, 1967).</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>Xerox PARC (1971-1984); Digital Equipment Corporation, Corporate Consulting Engineer (1984-1995); Microsoft. Technical Fellow (from 1995)</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>In addition to the Turing Award, Butler has received many other recognitions, among them are: Fellow of the ACM (1994); ACM Software Systems Award for his work on the Alto; IEEE Computer Pioneer Award, IEEE von Neumann Medal; National Academy of Engineering’s Draper Prize; Fellow of the Computer History Museum (2006). He is a member of the National Academies of Sciences and of Engineering, and the American Academy of Arts and Sciences. He holds honorary doctorates from the Eidgenössische Technische Hochschule, Zurich and the University of Bologna. <a href=""/bib/lampson_1142421.cfm"">Three</a> of his papers have won SigOps Hall of Fame awards.<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100081662","Butler W Lampson","<li class=""bibliography""><a href=""/bib/lampson_1142421.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=2159562&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/lampson_1142421.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/lampson_1142421.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179751-706","https://amturing.acm.org/award_winners/floyd_3720707.cfm","For having a clear influence on methodologies for the creation of efficient
and reliable software, and for helping to found the following important subfields of computer science:
the theory of parsing, the semantics of programming languages, automatic program verification, automatic
program synthesis, and analysis of algorithms.","<p><strong>Bob Floyd was born in New York in 1936.</strong> He was a very bright child and was recognized as a prodigy when he was 6. Despite moving many times during his school years, he completed high school at age 14 and was admitted into a special program for gifted children at the University of Chicago. He received a BA degree in 1953 when he was only 17. He started working to support himself and, at the same time, completed a second bachelor’s degree in physics in 1958.</p>
<p>His introduction to computing came from an early job as a computer operator at the Armour Research Foundation of the Illinois Institute of Technology. His curiosity led him to become a programmer by reading the manuals, and he quickly advanced to being a senior programmer and analyst. At the same time he started his research career by publishing a paper on radio interference [<a href=""/bib/floyd_3720707.cfm#bib_1"">1</a>]. At Armour he became interested in the compilers that translate high level languages into machine code. He published a paper describing a new notation for symbol manipulation systems that could be used to construct compilers. He then published a paper on a new method of scanning arithmetic expressions that results in more efficient machine code. [<a href=""/bib/floyd_3720707.cfm#bib_1"">2</a>]</p>
<p>In 1962 he became Senior Project Scientist at Massachusetts-based Computer Associates. He worked on compilers and published additional papers in the area. In 1966 <a href=""/award_winners/knuth_1013846.cfm"">Donald Knuth</a> was preparing the chapter of his famous book series <em>The Art of Computer Programming</em> dealing with compilers and syntax analysis, and he noticed that “only five really good papers about compilers had been written so far, and Bob had been the author of all five”. Floyd is the most cited author in <em>The Art</em> series.</p>
<p>In 1967 Floyd built on earlier work of <a href=""/award_winners/perlis_0132439.cfm"">Alan Perlis</a>, <a href=""https://en.wikipedia.org/wiki/Saul_Gorn"" target=""_blank"">Saul Gorn</a> and <a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy</a> for proving programs correct. He developed a notation, initially for flowcharts and later for real programs, that assigned conditions at each branch and entry point in the program. Some conditions related to the value of variables, and ensured that if these conditions were true upon entry then they could be proven true at exit. Other conditions proved a program would halt, by requiring that, at each step, some value would decrease that could not decrease indefinitely. Before this approach, ensuring that a program satisfies its specifications required testing with different data, examining the output, fixing bugs, and then trying it again. While sometimes effective, this debugging process could not test every possible situation. &nbsp;Errors were often found in large programs years after they had been put into production. Floyd’s mathematical analysis was the beginning of a long series of attempts by him and others to prove a program correct before it was released to users. His paper on this topic [<a href=""/bib/floyd_3720707.cfm#bib_1"">3</a>] was very influential and inspired <a href=""/award_winners/hoare_4622167.cfm"">Tony Hoare</a> to develop a system known as <a href=""https://en.wikipedia.org/wiki/Hoare_logic"" target=""_blank"">Hoare triples</a> that furthered this work.</p>
<p>Bob also invented many important practical algorithms. Best known are those that find the <a href=""https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm"" target=""_blank"">shortest paths</a> through networks, <a href=""https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=9&amp;ved=0CG4QFjAI&amp;url=http%3A%2F%2Fpeople.csail.mit.edu%2Frivest%2Fpubs%2FBFPRT72.pdf&amp;ei=xr0JUKmOGojbqgGsn4TICg&amp;usg=AFQjCNEMZyKkGY8g_vEVS1bAS7ExaXf6bA&amp;sig2=PYEy8c-tofLCbI8Em3IBmA"" target=""_blank"">compute the median of data</a>, and render gray scale images with binary pixels using error diffusion—the <a href=""https://en.wikipedia.org/wiki/Floyd%E2%80%93Steinberg_dithering"" target=""_blank"">Floyd-Steinberg algorithm.</a></p>
<p>Early in his career Floyd met Donald Knuth and they became both collaborators and friends. Floyd was the main proof reader and critic for Knuth’s famous <em>The Art of Computer Programming</em> before it expanded from a single book into a series. The collaboration grew stronger, and Knuth sponsored Floyd’s application for a position at Stanford University. Floyd was appointed as an Associate Professor at Stanford in 1968, which was unusual for someone without a graduate degree. Floyd used to joke that he intended to get his PhD via the “green stamp” method—collecting the envelopes of all the letters he received addressed to “Dr. R. Floyd,” and when he had enough he would trade them for a real degree. He was promoted to Full Professor in 1970, and became Chair of the Department of Computer Science in 1973.</p>
<p>Bob had a strong social conscience and was a leading member of Amnesty International. He used his influence to help release former Chilean Minister of Education <a href=""https://en.wikipedia.org/wiki/Fernando_Flores"" target=""_blank"">Fernando Flores</a> from imprisonment by the military junta. Flores subsequently joined the Stanford computer science department as a researcher.</p>
<p>Bob loved hiking and rock climbing. He was an avid backgammon player, and studied the game carefully. He had his middle name changed to the single letter “W”, but he often wrote it as an abbreviation with a period (“W.”). He was married and divorced twice, and had a daughter and three sons.</p>
<p>When he retired in 1994, he and Richard Biegel published a book <em>The Language of Machines: An Introduction to Computability and Formal Languages</em> [<a href=""/bib/floyd_3720707.cfm#bib_1"">4</a>], describing a machine-based theory of computational complexity. &nbsp;It gave him great satisfaction to see it translated into other languages.</p>
<p>Sometime shortly before his retirement in 1994 Floyd was stricken with a neurodegenerative disease that began to rob him of both his mental and physical facilities. His intellectual abilities were so strong that he managed to continue with his research, but at a slower pace. In a few years his condition had deteriorated to the point that he became unresponsive. He died in 2001.</p>
<p>When he died a memorial resolution was created by his colleagues at Stanford. That resolution includes other details of his life and work and can be found <a href=""/info/floyd_3720707.cfm"">here</a>.</p>
<p>Don Knuth, a colleague and close friend, wrote another memorial piece describing their relations and Bob’s effect on Knuth’s work. It was published as <a href=""https://dl.acm.org/citation.cfm?id=954488"" target=""_blank"">“Robert W Floyd, In Memoriam,”</a> <em>ACM SIGACT News</em>, Volume 34, Issue 4, December 2003, pp. 3-13 and is available <a href=""/p3-knuth.pdf"" target=""_blank"">here <img alt="""" src=""/images/logos/pdf_logo.gif"" style=""width: 16px; height: 16px;"">.</a></p>","<div class=""featured-photo"">
<a href=""/award_winners/floyd_3720707.cfm""><img src=""/images/lg_aw/3720707.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Robert W. Floyd""></a>
<br><br>
<h6 class=""label""><a href=""/photo/floyd_3720707.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>June 8, 1936, New York</p>
<h6><span class=""label"">DEATH: </span></h6>
<p>September 25, 2001, California</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>BA (Liberal Arts, University of Chicago, 1953); BSc (Physics, University of Chicago, 1958).</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Staff member, Armour Research Foundation, now IIT Research Institute (1953-1962)); Senior Project Scientist at Computer Associates (1962-1965); Assistant Professor (Carnegie Mellon University, 1965-1968); Stanford University (Associate Professor, 1968-1970; Full Professor, 1970-1994; Chairman, Computer Science, 1973-1976).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science, and the Association for Computing Machinery; appointed, for one year, as the first Grace Murray Hopper Professor at the Naval Postgraduate School; ACM Turing Award (1978); IEEE Computer Pioneer Award (1992).</p>","","https://dl.acm.org/author_page.cfm?id=81452611368","Robert (Bob) W Floyd","<li class=""bibliography""><a href=""/bib/floyd_3720707.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283934&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/floyd_3720707.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/floyd_3720707.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179043-657","https://amturing.acm.org/award_winners/clarke_1167964.cfm","","<h4>Birth and education</h4>
<p>Edmund Melson Clarke was born on July 27, 1945. He initially studied mathematics, receiving a BA from the University of Virginia in 1967 and an MA from Duke University in 1968. But by the time he enrolled in a doctoral program at Cornell University, he had switched to computing science. At Cornell he studied under <a href=""https://en.wikipedia.org/wiki/Robert_Lee_Constable"" target=""_blank"">Robert Constable</a>, a pioneer in making deep connections between mathematical logic and computing. After graduation Clarke returned to teach at Duke for two years, moving to Harvard University in 1978. He joined Carnegie Mellon University in 1982, where he is currently the FORE Systems University Professor of Computer Science and Professor of Electrical and Computer Engineering.</p>
<p>Clarke’s career has focused on mathematical reasoning about computer systems, with an emphasis on reasoning about the reliability of those systems. Such reasoning is necessary but very hard. A computer system executes simple operations, but those operations can occur in a staggering number of different orders. This makes it impossible for the designer to envision every possible sequence and predict its consequences. Yet every one of those sequences, no matter how infrequently executed, must be correct. If a program executes an incorrect sequence, at the very least it will waste a user’s time, while at the worst it could cause injury or loss of life.</p>
<p>The sequences become even more difficult to envision in systems with multiple programs running at the same time—a feature that has long been present in computer hardware and has become widespread in software since the beginning of the 21st century. Mathematical reasoning, and specifically its expression in formal logic, in principle is sufficient to describe every possible sequence and ensure that all of them are correct, even for simultaneously-running programs. In practice, however, classical mathematical reasoning is awkwardly-matched to describing the many possible execution orderings in a computer system.</p>
<h4>Inventing model checking</h4>
<p>Early researchers addressed this mismatch by developing logical forms better-suited to describing computer systems. One of the first was by <a href=""/award_winners/hoare_4622167.cfm"">Tony Hoare</a>. Hoare’s logic could be used to prove that <em>every possible</em> execution of a system would <em>only </em>execute an acceptable sequence of operations. This opened the possibility that systems could be proven to perform according to specification every time, no matter the circumstances. Clarke’s early research strengthened the foundations of Hoare’s logic and extended his method. Although Hoare’s method worked for smaller systems, it was close to impossible to apply to systems of any real size. The dream of powerful, effective methods for reasoning about all possible orderings of a system remained unfulfilled.</p>
<p>In 1977, <a href=""/award_winners/pnueli_4725172.cfm"">Amir Pnueli</a> introduced temporal logic, an alternative logic for verifying computer systems. As the name implies, temporal logic allows explicit reasoning about time. In temporal logic it is possible to express statements such as, “This condition will remain true until a second condition becomes true”.</p>
<p>Clarke and his student <a href=""/award_winners/emerson_1671460.cfm"">E. Allen Emerson</a> saw an important possibility in temporal logic: it could be directly checked by machine. Whereas Hoare’s logic required the designer to consider every detail of both the system and the argument about the system’s correctness—substantially increasing the designer’s workload—Pnueli’s logic could be implemented in a computer program. The responsibilities could be divided: the designer focused on specifying the system, while the software ensured that the proposed system would always perform correctly.</p>
<p>Clarke and Emerson realized that a program could exhaustively construct every possible sequence of actions a system might perform, and for every action it could evaluate a property expressed in temporal logic. If the program found the property to be true for every possible action in every possible sequence, this proved the property for the system. In the language of mathematical logic, Clarke and Emerson’s program checked that the possible execution sequences form a “model” of the specified property. Working independently, Jean-Pierre Queille and <a href=""/award_winners/sifakis_1701095.cfm"">Joseph Sifakis</a> developed similar ideas. The technology of model checking was born.</p>
<p>A great strength of the model checking approach is that when it detects a problem in a system design, the checker prints out an example sequence of actions that gives rise to the problem. Initial designs always get some details wrong. The error traces provided by model checking are invaluable for designers, because the traces precisely locate the source of the problems.</p>
<h4>Averting the state space explosion</h4>
<p>Although the 1981 paper [<a href=""/bib/clarke_1167964.cfm#bib_2""><u>2</u></a>] demonstrated that the model checking was possible in principle, its application to practical systems was severely limited. The most pressing limitation was the number of states to search. Early model checkers required explicitly computing every possible configuration of values the program might assume. For example, if a program counts the millimeters of rain at a weather station each day of the week, it will need 7 storage locations. Each location will have to be big enough to hold the largest rain level expected in a single day. If the highest rain level in a day is 1 meter, this simple program will have 10<sup>21</sup> possible states, slightly less than the number of stars in the observable universe. Early model checkers would have to verify that the required property was true for every one of those states.</p>
<p>Systems of practical size manipulate much more data than the simple example above. The number of possible states grows as well, at an explosive speed. This rapid growth is called the <em>state space explosion</em>. Although early model checkers demonstrated that the technology was feasible for small systems, it was not ready for wider use.</p>
<p>Clarke and his student Ken McMillan had a fundamental insight: The state space explodes because the number of states a memory location can assume is much, much bigger than the size of the location itself. The memory location is compact because it encodes many <em>potential</em> states but only contains one at a time. Clarke and McMillan observed that this process could be applied in reverse: with the right encoding, many potential states could be represented by a single value. From the literature, McMillan found an encoding that met the twin goals of tersely encoding multiple states while at the same time permitting efficient computation of formulas in temporal logic.</p>
<p>The new representation dramatically reduced the storage required to represent state spaces, in turn reducing the time required to run a model checker on systems of practical size. They called these new systems <em>symbolic model checkers. </em>In 1995, Clarke, McMillan, and colleagues used this approach to demonstrate flaws in the design of an IEEE standard for interconnecting computer components. Before this, the reliability of such standards had only been informally analyzed, leaving many rare cases unconsidered and potential errors undiscovered. This was the first time every possible case of an IEEE standard had been exhaustively checked.</p>
<p>With enhancements such as these, model checking has become a mature technology. It is widely used to verify designs for integrated circuits, computer networks, and software, by companies such as Intel and Microsoft. Model checkers have been used to analyze systems whose state space (10<sup>120)</sup> is substantially larger than the number of atoms in the observable universe (around 10<sup>80</sup>). It is becoming particularly important in the verification of software designed for recent generations of integrated circuits, which feature multiple processors running simultaneously. Model checking has substantially improved the reliability and safety of the systems upon which modern life depends.</p>
<p align=""right""><span class=""callout"">Author: Ted Kirkpatrick</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/clarke_1167964.cfm""><img src=""/images/lg_aw/1167964.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Edmund Clarke""></a>
<br><br>
<h6 class=""label""><a href=""/photo/clarke_1167964.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>July 27, 1945.</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>BA, Mathematics, (University of Virginia, 1967); MA Mathematics (Duke University, 1968);Ph.D., Computer Science (Cornell University, 1976)</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Duke University (1976–1978); Harvard University (1978–1982); Carnegie Mellon University (1982–present, including FORE Systems Professor 1995–present, University Professor 2008–present).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>Technical Excellence Award, Semiconductor Research Corporation (1995); ACM Paris Kanellakis Theory and Practice Award (1998, with Randal&nbsp; Bryant, E. Allen Emerson, and Kenneth L. McMillan); IEEE Harry H. Goode Memorial Award (2004); Elected to National Academy of Engineering (2005); ACM Turing award (2007, with Emerson and Sifakis); International Conference on Automated Deduction (CADE) Herbrand Award for Distinguished Contributions to Automated Reasoning (2008); Logic in Computing Science (LICS) 2010 Test-of-Time Award for his 1990 paper, ""Symbolic model checking…; Elected to the American Academy of Arts and Sciences (2011); Fellow of the ACM and IEEE; Honorary Doctorate, (Vienna University of Technology, 2012);&nbsp;<span style=""line-height: normal;"">Bower Award and Prize for Achievement in Science&nbsp;from the&nbsp;Franklin Institute (2014)</span><span style=""line-height: 1.3;"">.</span></p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100393517","Edmund Melson Clarke","<li class=""bibliography""><a href=""/bib/clarke_1167964.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/clarke_1167964.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/clarke_1167964.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179469-686","https://amturing.acm.org/award_winners/ritchie_1506389.cfm","With Ken Thompson, for their development of generic operating systems theory and specifically for the implementation of the UNIX operating system.","<p><strong>Dennis MacAlistair Ritchie was born September 9, 1941 in Bronxville, New York. </strong>His father worked at AT&amp;T Bell Laboratories, and Dennis grew up in New Jersey.</p>
<p>Ritchie received a BS degree in Physics in 1963 and a PhD in Applied Mathematics in 1967 from Harvard University. While a graduate student, Ritchie had a part-time job at <a href=""/info/ritchie_1506389.cfm#add_1"">MIT Project MAC's</a> Multics timesharing project.</p>
<p>After graduation, Ritchie and <a href=""/award_winners/thompson_4588371.cfm"">Ken Thompson</a> joined the Bell Laboratories Computing Sciences Research Center in Murray Hill, NJ. At the time, staff members of this group had considerable latitude in choosing research topics in computing theory, languages, programming and systems. Since 1964, members of the group had been participating in the design and development of the Multics system, along with developers from MIT's Project MAC and General Electric.</p>
<p>Ritchie worked with others in many different software projects associated with the Multics operating systems or tools. For example, in 1967, Ritchie, Robert Morris and Rudd Canaday ported the programming language <a href=""/info/ritchie_1506389.cfm#add_3"">BCPL</a> from <a href=""/info/ritchie_1506389.cfm#add_4"">CTSS</a> to the Multics and <a href=""/info/ritchie_1506389.cfm#add_5"">GECOS</a> systems.</p>
<p>In 1969 Bell Labs withdrew from the Multics project. The Computing Sciences Research group members searched for other projects, and in particular for a computing environment with an on-line community that avoided the ""big system mentality."" Unix was to provide such an environment. Ritchie wrote:</p>
<p><span class=""callout"" style=""margin-left: 40px;"">""It began in 1969 when Ken Thompson discovered a little-used PDP-7 computer and set out to fashion a computing environment that he liked. Thompson wrote the first version of Unix for a <a href=""/info/ritchie_1506389.cfm#add_6"">Digital Equipment Corporation PDP-7</a> in a month, using a cross-assembler that ran on GECOS. The PDP-7 he used had 4K of 18-bit words. His work soon attracted me; I joined in the enterprise, though most of the ideas, and most of the work for that matter, were his.""</span>The resulting Unix system provided users with interactive remote terminal computing and a shared file system. Source code was provided with the system, and the community of users could share ideas and programs directly and informally. Because Unix ran on a relatively inexpensive minicomputer, small research groups could experiment with it without dealing with computation center bureaucracies.</p>
<p>In 1971, the Bell Laboratories Computing Sciences Research group ported Unix to a <a href=""/info/ritchie_1506389.cfm#add_6"">Digital Equipment Corporation PDP-11</a> to support text processing for the Bell Laboratories Patents Office. By 1972, there were 10 installations of Unix at AT&amp;T.</p>
<p>Ken Thompson also created an interpretive language called B, based on BCPL, which he used to re-implement the non-kernel parts of Unix. Ritchie added types to the B language, and later created a compiler for the C language. Thompson and Ritchie rewrote most of Unix in C in 1973, which made further development and porting to other platforms much easier.</p>
<p>The second ACM Symposium on Operating Systems Principles was held in Elmsford, NY in 1973, and Thompson and Ritchie presented a clear and well-written paper [<a href=""/bib/ritchie_1506389.cfm#link_3"">3</a>] describing Unix. The Unix system presented in the paper was elegant and simple, providing a useful and extensible multi-user programming environment on an affordable machine. The file system and libraries included with the system made it easy to build and share application programs and to augment the system's functions. By the end of 1973, there were over 20 Unix systems running.</p>
<p>Thompson and Ritchie, along with other Computing Sciences Research group members, continued the development of Unix and C at Bell Laboratories, and Unix use spread further within AT&amp;T. The Sixth Edition, released in 1975, began the spread of Unix to university, commercial, and government users of the popular DEC PDP-11 computers. AT&amp;T, forbidden by court decree from selling Unix, licensed it for the cost of media. Enthusiastic users had the source code available, and fed improvements to Unix back to the Bell Labs developers. A 1977 retrospective paper by Ritchie [<a href=""/bib/ritchie_1506389.cfm#link_4"">4</a>] said that there were more than 300 Unix installations running on configurations from a single-user DEC LSI-11 to a 48-user PDP-11/70. By 1978, there were over 600 Unix installations, and Unix had begun to be ported to other minicomputers.</p>
<p>In the late 1970s, John Lions of the University of New South Wales circulated a book [<a href=""/bib/ritchie_1506389.cfm#link_8"">8</a>] on Unix that included the source code and commentaries on it. This book was used to teach Unix in operating systems courses around the world, and created a generation of computer scientists familiar with Unix internals.</p>
<p>In 1983, Thompson and Ritchie received the ACM A. M. Turing Award. The Turing Award selection committee wrote:</p>
<p><span class=""callout"" style=""margin-left: 40px;"">The success of the UNIX system stems from its tasteful selection of a few key ideas and their elegant implementation. The model of the Unix system has led a generation of software designers to new ways of thinking about programming. The genius of the Unix system is its framework, which enables programmers to stand on the work of others.</span></p>
<p>In the mid-1980s, several organizations promoted different technical approaches to Unix on different platforms, with different licensing arrangements. Thompson and Ritchie were honored as the originators of the system but no longer controlled its destiny. They went on to other computing research projects within AT&amp;T.</p>
<p>Ritchie became head of the Bell Laboratories Computing Techniques Research Department in 1990 and, with others, began the Inferno distributed operating system and the Limbo language in 1995. Inferno is designed to support applications such as television set-top boxes and advanced telephones.</p>
<p>Ritchie retired as head of Lucent Technologies' System Software Research Department in 2007, and died in early October, 2011.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Tom Van Vleck</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/ritchie_1506389.cfm""><img src=""/images/lg_aw/1506389.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Dennis M. Ritchie ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/ritchie_1506389.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>September 9, 1941 in Bronxville, New York.</p>
<h6 class=""label"">DEATH:</h6>
<p>October 2011.</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Ph.D. in Physics and Applied Mathematics, Harvard (1967);</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Member of Technical Staff, Bell Laboratories, Murray Hill NJ (Multics project 1967-1969, Creator of C language, Co-creator of Unix operating system, co-creator of Plan 9 From Bell Labs operating system, co-creator of Inferno distributed operating system, Department Head, Bell Laboratories)</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>The following awards were jointly given to both Thompson and Ritchie: ACM Programming Systems and Languages Paper Award (1975); ACM A.M. Turing Award (1983); ACM Software System Award (1983); IEEE Emmanuel R. Piore Award (1983), IEEE Richard W. Hamming Medal (1990); IEEE Computer Pioneer Award (1994); Computer History Museum Fellow Award (1997); National Medal of Technology from President Bill Clinton (1998); ACM SIGOPS Hall of Fame Award (2005). Japan Prize for Information and Communications (2011). Ritchie was elected to the National Academy of Engineering in 1988.&nbsp;<span style=""line-height: 1.3;"">Industrial Research Institute </span><span style=""line-height: 1.3;"">Achievement Award</span><span style=""line-height: 1.3;"">&nbsp;in recognition of his contribution to science and technology, and to society generally, with his development of the Unix operating system (2005).</span></p>","","https://dl.acm.org/author_page.cfm?id=81100458439","Dennis M. Ritchie","<li class=""bibliography""><a href=""/bib/ritchie_1506389.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283939&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/ritchie_1506389.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/ritchie_1506389.cfm""><span></span>Additional<br> Materials</a></li>"
"1573178919-649","https://amturing.acm.org/award_winners/kay_3972189.cfm","For pioneering many of the ideas at the root of contemporary object-oriented programming languages, leading the team that developed Smalltalk, and for fundamental contributions to personal computing.","<p><strong>Alan Curtis Kay was born in Springfield, Massachusetts on May 17, 1940. </strong>His father designed arm and leg prostheses, and his mother, a musician, taught Alan how to play. Kay grew up in an environment of art, literature, and science. He could read by the age of three and had read about 150 books before he started school. His family later moved to New York City where he attended Brooklyn Technical High School.</p>
<p>He started college, but left before graduation to join the air force. There he discovered computers and passed an aptitude test to become an <a href=""https://en.wikipedia.org/wiki/IBM_1401"" target=""_blank"">IBM 1401</a> programmer. He gained experience working with a number of different machines, including the <a href=""http://www.computerhistory.org/collections/accession/X313.84"" target=""_blank"">Burroughs B500</a>. From this air force experience, Kay learned that a program can be designed with procedures that don’t know how the data are represented. This idea supported later development of object-oriented programming languages.</p>
<p>After the air force, Kay went back to the University of Colorado. In 1966, he earned an undergraduate degree in mathematics and molecular biology from the University of Colorado. He also worked as a professional jazz guitarist. He then went to the University of Utah where he was awarded MS in Electrical Engineering and, in 1969, a Ph.D. in Computer Science. Much computer science research there was financed by the Department of Defense’s Advanced Research Projects Agency (ARPA), and Kay was one of the many graduate students who attended ARPA-sponsored conferences and contributed to ARPA research and projects such as time-sharing and the ARPAnet, the forerunner of the Internet.</p>
<p>At the University of Utah, graduate students were encouraged to work on practical computing projects. Kay teamed up with Edward Cheadle, who was working on the design of a small computer for engineers. Together they designed “FLEX” to have sharp graphics and windowing features, and called it a “personal computer.”</p>
<p>While working on FLEX, Kay witnessed <a href=""/award_winners/engelbart_7929781.cfm"">Douglas Engelbart’s</a> demonstration of interactive computing designed to support collaborative work groups. Engelbart’s vision influenced Kay to adopt graphical interfaces, hypertext, and the mouse. Other influences were <a href=""https://en.wikipedia.org/wiki/JOSS"" target=""_blank"">JOSS</a>, a system that supported 12 personal workstations; GRAIL, a project designed to support human-computer communication through modeless computing; Understanding Media, a book written by Marshall McLuhan that describes the internalization of media; Logo, a project designed to help children learn through computers; and flat panel screen displays.</p>
<p>After considering these technologies and ideas, Kay made a cardboard mock-up of a tablet-style personal computer with a flat-panel display screen and a stylus. The technology of the time could not capture Kay’s vision for personal computing, but he knew from Moore’s law that eventually it would. Kay continued working on the FLEX project and finished his doctoral work in 1969. His thesis was called the “Reactive Engine.”</p>
<p>After graduating from Utah, Kay became a researcher at the Stanford Artificial Intelligence Laboratory and developed programming languages. He began to think of a future with book-sized computers. Influenced by the Logo project, he particularly wanted to see how children would use them, and made sketches of “KiddieKomputers”. These ideas were later integrated into the design of the <a href=""https://en.wikipedia.org/wiki/Xerox_Alto"" target=""_blank"">Alto computer</a>.</p>
<p>In 1971 Kay joined the Xerox Palo Alto Research Center (PARC). PARC had been started by the Xerox Corporation in 1970 to do long-term research for “the office of the future.” Kay was hired to run The Learning Research Group, and he established the following goals:</p>
<ol>
<li>Create examples of how small computers could be used in different subject areas;</li>
<li>Examine how small computers could help to expand the user’s visual and auditory skills;</li>
<li>Let children spend time learning about computers and experiment with personal ways to understand computer processes;</li>
<li>Report on children’s unexpected uses of the computer and its software.</li>
</ol>
<p>Kay was a visionary force at Xerox PARC in the development of tools that transformed computers into a new major communication medium. His credo was, “the best way to predict the future is to invent it.” One of his visionary concepts was the <a href=""https://en.wikipedia.org/wiki/Dynabook"" target=""_blank"">Dynabook</a>, a powerful and portable electronic device the size of a three-ring notebook with a touch-sensitive liquid crystal screen and a keyboard for entering information. Kay is recognized for inventing ideas that became the future. Laptops, notebook computers, and tablets have roots in the early concepts of the Dynabook.</p>
<p>Kay also realized that computers could become a “metamedium”—that it could incorporate all other media. As a new medium, computers could have the same impact as the Gutenberg printing press. McLuhan’s ideas about the cultural impact of the printing press influenced Kay’s choice of the name “Dynabook,” because computers produce dynamic representations of information rather than static book pages.</p>
<p>People needed a method for interacting with the new computer medium. To help with this, Kay and the members of his lab created graphical interfaces and the <a href=""https://en.wikipedia.org/wiki/Smalltalk"" target=""_blank"">Smalltalk</a> programming language.</p>
<p>Kay’s philosophy for designing interfaces was based on the learning research of Jerome Bruner, who was influenced by <a href=""https://en.wikipedia.org/wiki/Jean_Piaget"" target=""_blank"">Jean Piaget</a>. Continuing the research, Bruner contended that the mind has multiple intelligences. Using learning theory in interface design helped Kay’s develop computer technology that children could use.</p>
<p>Bruner argued for the existence of different learning mentalities, which suggested to Kay a model for interface design called ‘Doing With Images makes Symbols.” The “doing” was interacting with a mouse, the “images” were icons on the computer screen, and the “symbols” were the SmallTalk programming language.</p>
<p>SmallTalk was originally designed as a graphical programming language. However, it soon became a complete integrated programming environment with a debugger, object-oriented virtual memory, an editor, screen management, and user interface. SmallTalk was the first dynamic object-oriented programming language. It ran on the Alto computer, envisioned by <a href=""/award_winners/lampson_1142421.cfm"">Butler Lampson</a> and designed by <a href=""/award_winners/thacker_1336106.cfm"">Charles P. Thacker</a>&nbsp; (both Turing Award recipients). The Alto was a step in the direction of small powerful personal computers, and it was considered an interim Dynabook.</p>
<p>Kay left Xerox PARC in the early 1980s to move to Los Angeles. In 1983, Kay worked for Atari for a year before joining Apple Computer. While at Apple, his research team developed <a href=""https://en.wikipedia.org/wiki/Squeak"" target=""_blank"">Squeak</a>, an open-source SmallTalk language. In 1997 Kay moved his team to Disney’s Imagineering division to continue his work on Squeak. Five years later, he established <a href=""https://en.wikipedia.org/wiki/Viewpoints_Research_Institute"" target=""_blank"">Viewpoints Research Institute</a>, a nonprofit organization dedicated to supporting educational media for children.</p>
<p>Kay also held the position of Senior Fellow at Hewlett-Packard until 2005. He has taught classes at New York University’s Interactive Telecommunications program, the University of California, Los Angeles, the Kyoto University and the Massachusetts Institute of Technology.</p>
<p>Alan Kay is considered by some as the “father of personal computers” because he envisioned a small computing system in the 1970’s, long before notebook computers were available. The One Laptop per Child program and the Children’s Machine have adopted his concepts about children and learning. His most important contribution to computer science is his commitment to turning the computer into a dynamic personal medium that supports creative thought. He continues to explore ways in which computers can be accessible to children.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Susan B. Barnes</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/kay_3972189.cfm""><img src=""/images/lg_aw/3972189.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Alan Kay""></a>
<br><br>
<h6 class=""label""><a href=""/photo/kay_3972189.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>17 May 1940 in Springfield, Massachusetts</p>
<h6 class=""label"">EDUCATION:</h6>
<p>University of Colorado at Boulder (mathematics and molecular biology); University of Utah MS (computer science), Ph.D. (computer science), 1969.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Xerox PARC; Stanford University; Atari; Apple Inc. (Advanced Technology Group); Walt Disney Imagineering; UCLA; Kyoto University; MIT; Viewpoints Research Institute; Hewlett-Packard.</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Berlin University of the Arts Award for Interdisciplinary Art and Science (2001); J-D Warnier Prix d'Informatique (2001); NEC Computer &amp; Communications Prize (2001); Telluride Tech Festival Award of Technology (2002); ACM Turing Award (2003); Kyoto Prize (2004); Charles Stark Draper Prize (2004); UPE Abacus Award (2012); ACM Systems Software Award; NEC Computers &amp; Communication Foundation Prize; Funai Foundation Prize; Lewis Branscomb Technology Award; ACM SIGCSE Award for Outstanding Contributions to Computer Science Education. Honorary Doctorates: Kungliga Tekniska Högskolan (Royal Institute of Technology) in Stockholm (2002); Georgia Institute of Technology (2005); Columbia College Chicago (2005); Laurea Honoris Causa in Informatica, Università di Pisa (2007); University of Waterloo (2008); Universidad de Murcia (2010); Honorary Professor, Berlin University of the Arts.<br>
Elected Fellow of: American Academy of Arts and Sciences; National Academy of Engineering; Royal Society of Arts; Computer History Museum (1999); Association for Computing Machinery (2008); Hasso-Plattner-Institute (2011).<br>
&nbsp;</p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100544599","Alan Kay","<li class=""bibliography""><a href=""/bib/kay_3972189.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/kay_3972189.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/kay_3972189.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/kay_3972189.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179656-698","https://amturing.acm.org/award_winners/rivest_1403005.cfm","Together with Leonard M. Adleman and Adi Shamir, for their ingenious contribution to making public-key cryptography useful in practice.","<p>Ron Rivest grew up in Niskayuna, New York, a suburb of Schenectady. He attended public schools and graduated from the Niskayuna High School in 1965. He graduated from Yale University in 1969 with a B.A. in mathematics, and from Stanford University in 1973 with a PhD in Computer Science. He learned from the best: his PhD supervisor was Turing Award recipient <a href=""/award_winners/floyd_3720707.cfm"">Robert Floyd</a>, and he worked closely with Turing Award laureate <a href=""/award_winners/knuth_1013846.cfm"">Don Knuth</a>.</p>
<p>Following graduate study he accepted a post-doctoral position at <a href=""https://en.wikipedia.org/wiki/National_Institute_for_Research_in_Computer_Science_and_Control"" target=""_blank"">INRIA</a> in Rocquencourt, France before taking a job at MIT, where he has been ever since. He currently holds the Andrew and Erna Viterbi professorship in the Department of Electrical Engineering and Computer Science.</p>
<p>At MIT he met <a href=""/award_winners/adleman_7308544.cfm"">Leonard M. Adleman</a>&nbsp;and <a href=""/award_winners/shamir_0028491.cfm"">Adi Shamir</a>, who collaborated with Ron on their fundamental advance in cryptography. They were inspired by a 1976 paper [<a href=""/bib/rivest_1403005.cfm#bib_4"">4</a>] by cryptographers Whitfield Diffie and Martin Hellman discussing several new developments in cryptography. It described ways for the sender and receiver of private messages to avoid needing a shared secret key, but it did not provide any realistic way to implement these concepts. Rivest, Shamir, and Adleman presented practical implementations in their 1977 paper, “A method for obtaining digital signatures and public-key cryptosystems,” [<a href=""/bib/rivest_1403005.cfm#bib_1"">1</a>] which showed how a message could easily be encoded, sent to a recipient, and decoded with little chance of it being decoded by a third party who sees it.</p>
<p>The method, known as <a href=""https://en.wikipedia.org/wiki/Public-key_cryptography"" target=""_blank"">Public Key Cryptography</a>, uses two different but mathematically linked keys: one public key used to encrypt the message, and a completely different private key used to decrypt it. The encrypting key is made public by individuals who wish to receive messages, but the secret decrypting key is known only them. The two keys are linked by some well-defined mathematical relationship, but determining the decryption key from its publically available counterpart is either impossible or so prohibitively expensive that it cannot be done in practice. The “RSA” method (from the first letters of the names of the three authors) relies on the fact that nobody has yet developed an efficient algorithm for <a href=""https://en.wikipedia.org/wiki/Prime_factor"" target=""_blank"">factoring</a> very large integers. There is no guarantee, however, that it will be difficult forever; should a large <a href=""https://en.wikipedia.org/wiki/Quantum_computer"" target=""_blank"">quantum computer</a> ever be built, it might be able to break the system.</p>
<p>A detailed discussion of the theory and practice behind RSA can be found <a href=""https://en.wikipedia.org/wiki/RSA_%28algorithm%29"" target=""_blank"">here</a>. The computer code to implement it is quite simple, and as long as suitably large integer keys are used, no one knows how to break an encoded message.</p>
<p>RSA is used in almost all internet-based commercial transactions. Without it, commercial online activities would not be as widespread as they are today. It allows users to communicate sensitive information like credit card numbers over an unsecure internet without having to agree on a shared secret key ahead of time. Most people ordering items over the internet don’t know that the system is in use unless they notice the small padlock symbol in the corner of the screen. RSA is a prime example of an abstract elegant theory that has had great practical application.</p>
<p>After developing the basic method in 1977, the three Turing Award recipients founded <a href=""https://en.wikipedia.org/wiki/RSA_%28security_firm%29"" target=""_blank"">RSA Data Security</a> in 1983. The company was later acquired by Security Dynamics, which was in turn purchased by EMC in 2006. It has done cryptographic research, sponsored conferences, shown how earlier encryption systems could be compromised, and spun off other companies such as <a href=""https://en.wikipedia.org/wiki/VeriSign"" target=""_blank"">Verisign</a>. Rivest thus demonstrated that he could move easily between theory and practice. When the 1983 patent on RSA [<a href=""/bib/rivest_1403005.cfm#bib_2"">2</a>] was about to expire, RSA Data Security published all the details of its implementation so that there would be no question that anyone could create products incorporating the method.</p>
<p>The three Turing Award recipients were not aware that a similar method had been developed years before by British mathematician Clifford Cocks, who extended the even earlier work of James H. Ellis. Cocks was doing his encryption work at the Government Communications Headquarters (GCHQ), so it was classified as secret and not released until 1997, twenty years after Rivest, Adleman, and Shamir had published their independent discovery.</p>
<p>In addition to the well-known RSA scheme, Rivest designed several other encoding methods for special applications. RC2, or “Ron’s Code 2,” was designed in 1987 as an encoding method for Lotus Corporation to use in the international version of their Lotus Notes product. The US National Security Agency (NSA) had prohibited the export of software unless it met restrictions designed to ensure that sensitive technology did not fall into unfriendly hands; RC2 was effective but still met those restrictions. Details of all six (two of which were never released) of Ron’s RC algorithms can be found <a href=""https://en.wikipedia.org/wiki/RC_algorithm"" target=""_blank"">here</a>.</p>
<p>Rivest’s interests in security are not limited to encryption. For example, he is a member of the US government technical committee that develops election guidelines, and he published a 2006 paper [<a href=""/bib/rivest_1403005.cfm#bib_5"">5</a>] describing a novel three-ballot voting scheme. The paper is posted on his MIT website and is modified occasionally to eliminate problems that others have noted. As Rivest notes at the end:</p>
<p style=""margin-left: 40px; "">ThreeBallot is hereby placed in the public domain—I am not filing for any patents on this approach, and we encourage others who work on extensions, improvements, or variations of this approach to act similarly. Our democracy is too important...</p>
<p>Ron is also an inspirational teacher. His co-written influential textbook <em>Introduction to Algorithms</em> [<a href=""/bib/rivest_1403005.cfm#bib_3"">3</a>], based on his undergraduate and graduate courses, has become a classroom standard. More than 500,000 copies were sold in 20 years.&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/rivest_1403005.cfm""><img src=""/images/lg_aw/1403005.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Ronald L Rivest""></a>
</div>
<h6 class=""label""><strong>BIRTH:</strong></h6>
<p>1947, Schenectady, New York, USA</p>
<h6 class=""label""><strong>EDUCATION:</strong></h6>
<p>Niskayuna High School, Niskayuna, New York, USA (1965); BA (Mathematics, Yale University, 1969); Ph.D. (Computer Science, Stanford University, 1973)</p>
<h6 class=""label""><strong>EXPERIENCE:</strong></h6>
<p>INRIA, Rocquencourt, France (post-doctorate position, 1973-1974); MIT (professor of Electrical Engineering and Computer Department, member of MIT's Computer Science and Artificial Intelligence Laboratory, CSAIL, a member of their Theory of Computation Group and a leader of its Cryptography and Information Security Group, from 1974)</p>
<h6 class=""label""><strong>HONORS AND AWARDS:</strong></h6>
<p>Member, National Academy of Engineering (1990);&nbsp;<span style=""line-height: 20.8px;"">Fellow of the ACM (1993);</span><span style=""line-height: 20.8px;"">&nbsp;</span><span style=""line-height: 1.3;"">Member, American Academy of Arts and Sciences (1993); National Computer Systems Security Award (1996); ACM Paris </span>Kanellakis<span style=""line-height: 1.3;""> Theory and Practice Award (1997); IEEE </span>Koji<span style=""line-height: 1.3;""> Kobayashi Computers and Communications Award, with A. </span>Shamir<span style=""line-height: 1.3;""> and L. </span>Adleman<span style=""line-height: 1.3;""> (2000); Secure Computing Lifetime Achievement Award, with A. </span>Shamir<span style=""line-height: 1.3;""> and L. </span>Adleman<span style=""line-height: 1.3;""> (2000); ACM Turing Award, with A. </span>Shamir<span style=""line-height: 1.3;""> and L. </span>Adleman<span style=""line-height: 1.3;""> (2002); Honorary doctorate, University of Rome La </span>Sapienza<span style=""line-height: 1.3;""> (2002); Fellow, International Association for </span>Cryptologic<span style=""line-height: 1.3;""> Research (2004); Member, National Academy of Science (2004); </span>MITX<span style=""line-height: 1.3;""> Lifetime Achievement Award (2005); </span>Marconi<span style=""line-height: 1.3;""> Prize (2007); Computers, Freedom and Privacy Conference ""Distinguished Innovator"" award (2007); Killian Burgess and Elizabeth </span>Jamieson<span style=""line-height: 1.3;""> Award from MIT </span>EECS<span style=""line-height: 1.3;""> Department (2008); honorary doctorate, </span>Louvain<span style=""line-height: 1.3;""> School of Engineering at the </span>Université<span style=""line-height: 1.3;""> </span>Catholique<span style=""line-height: 1.3;""> de </span>Louvain<span style=""line-height: 1.3;""> (2008); Faculty Achievement Award from MIT (2009); NEC C&amp;C Prize, with A. Shamir and L. Adleman (2009); RSA 2011 Conference Lifetime Achievement Award, with A. Shamir and L. Adleman (2011); named an Institute Professor at MIT (2015).</span></p>","","https://dl.acm.org/author_page.cfm?id=81328490215","Ronald (Ron) Linn Rivest","<li class=""bibliography""><a href=""/bib/rivest_1403005.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/rivest_1403005.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/rivest_1403005.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/rivest_1403005.cfm""><span></span>Additional<br> Materials</a></li>"
"1573178886-647","https://amturing.acm.org/award_winners/blum_4659082.cfm","","<p><strong>Manuel Blum was born in Caracas, Venezuela in 1938.</strong> He remembers that as a child he wanted to know how brains work. The reason: he wanted to be smarter. &nbsp;(“I was the opposite of a prodigy,"" he claims). At M.I.T. he studied electrical engineering because he thought electric circuits might hold the answer. As a junior he studied brains in Warren McCulloch's lab. Eventually he became convinced that the limitations of our brain are related to computational complexity, and that is how he embarked on a life-long journey that transformed that subject as well as much of computer science.</p>
<p>By the early 1960s computers were already solving all kinds of problems in business and science. An algorithm for solving a problem expends resources: the time it takes to run, the amount of memory it uses, the energy it consumes, etc. The irresistible question is: Can I do better? Can I find an algorithm that solves the problem faster using fewer resources? You can sort <em>n</em> numbers in a computer's memory with <em>n</em> log <em>n</em> comparisons using Mergesort, Quicksort or a similar sorting algorithm. But can it be done better? You can find the factors of an <em>n</em>-bit integer in time proportional to 2<em><sup>n</sup></em><sup>/2</sup> by checking all candidate divisors up to the square root of the number. But this is exponential in <em>n</em>.&nbsp; Is there a better, truly fast algorithm? You can search a maze with <em>n</em> corridors using only log<sup>2</sup> <em>n</em> memory. Is there a solution utilizing log <em>n</em> memory? This is the stuff of complexity.</p>
<p>Of the three questions above, after half a century of contemplation we know the answer to only the first one, about sorting: An elementary argument discovered in the 1960s establishes that you cannot sort with fewer than <em>n</em> log <em>n</em> comparisons. &nbsp;(This is proved by considering the decision tree of a sorting algorithm and then taking the logarithm of its size). So sorting a list of <em>n</em> numbers requires <em>n</em> log <em>n</em> time.</p>
<p>Computing the average of a list of numbers is, of course, easier than sorting, since it can be done in just <em>n</em> steps. But the <em>median</em> is usually more informative than the average–in connection with incomes, for example. Unfortunately you need to first sort the numbers in order to find the median. Or do you? In the late 1960s, Blum was convinced that computing the median does indeed require <em>n</em> log <em>n</em> steps, just like sorting. He tried very hard to prove that it does, and in the end his labors were rewarded with a most pleasant surprise: in 1971 he came up with an algorithm that finds the median in linear time! (A few other great researchers had in the meantime joined him in this quest <a href=""/award_winners/tarjan_1092048.cfm"">Robert Tarjan</a>, <a href=""/award_winners/rivest_1403005.cfm"">Ronald Rivest</a>, <a href=""/award_winners/floyd_3720707.cfm"">Bob Floyd</a>,&nbsp; and Vaughan Pratt.) [<a href=""/bib/blum_4659082.cfm#link_2""><u>2</u></a>]</p>
<p>But we are getting ahead of our story. In the early 1960s Blum wanted to understand complexity. To do so one must first pick a specific machine model on which algorithms run, and then a resource to account for in each computation. But Blum wanted to discover the essence of complexity–its fundamental nature that lies beyond such mundane considerations. In his doctoral thesis at M.I.T. under <a href=""/award_winners/minsky_7440781.cfm"">Marvin Minsky</a>, he developed a machine-independent theory of complexity that underlies all possible studies of complexity. He postulated that a resource is any function that has two basic properties (since then called the Blum axioms), essentially stating that the amount of the resource expended by a program running on a particular input can be computed—unless the computation fails to halt, in which case it is undefined.</p>
<p>Surprisingly these simple and uncontroversial axioms spawn a rich theory full of unexpected results. One such result is Blum’s speedup theorem, stating that there is a computable function such that any algorithm for this function can be sped up exponentially for almost all inputs [<a href=""/bib/blum_4659082.cfm#link_1""><u>1</u></a>]. That is, any given algorithm for that function can be modified to run exponentially faster, but then the modified algorithm can be modified again, and then again, an infinite sequence of exponential improvements! Blum's theory served as a cautionary tale for complexity theorists: unless resource bounds are restricted to the familiar, well-behaved ones (such as <em>n</em> log <em>n</em>, n<sup>2</sup>, polynomial, exponential, etc.), very strange things can happen in complexity.</p>
<p>By the early 1970s Manuel Blum was a professor at UC Berkeley, where he would teach for over thirty-five years. He had married the notable mathematician Lenore Blum, about whom Manuel wrote a haiku: “Honor her requests as if/Your life depends on it/It does."" They had a son Avrim, now a professor of Computer Science at Carnegie Mellon, where Lenore and Manuel are also now employed.</p>
<p>By the early 1970s the study of complexity had become much more concrete, partly because of the scary phenomena exposed by Blum's model.&nbsp; It had already succeeded in articulating the fundamental <a href=""https://en.wikipedia.org/wiki/P_versus_NP_problem"">P vs NP</a> question, which remains famously unresolved to this day: are there problems which can only be solved by rote enumeration of all possible solutions? At that point, when most students of complexity embarked on a long-term research effort aiming at either taming the beast, or establishing that all such efforts are futile, Blum's thinking about the subject took a surprising turn that would constitute the overarching theme of his work for the coming decades: he decided to make friends with complexity, to turn the tables on it, and use it to accomplish useful things.</p>
<p>One particular such “application” of complexity was emerging just around that time: public key cryptography, which allows secure communication through the use of one-way functions that are easy to compute but hard to reverse. It so happened that in the mid-1970s Blum decided to spend a sabbatical studying number theory—the beautiful branch of mathematics theretofore famously proud of its complete lack of application. Nothing could be more fortuitous.&nbsp; The RSA approach to public key cryptography, which was being invented around that time, exploits important one-way functions inspired by number theory. RSA's inventors <a href=""/award_winners/rivest_1403005.cfm"">Ron Rivest</a>, <a href=""/award_winners/shamir_0028491.cfm"">Adi Shamir</a>, and <a href=""/award_winners/adleman_7308544.cfm"">Len Adleman</a> (the latter a doctoral student of Blum's) won the Turing award in 2003 for this idea.</p>
<p>Blum wondered what else complexity can help us with beyond cryptography. One strange thought came to him: can two people, call them Alice and Bob, settle an argument over the telephone about, say, who is buying drinks next time, by flipping a coin? This seems impossible, because who is going to force the one who flips, for example, Bob, to admit that he lost the toss? Surprisingly, Blum showed [<a href=""/bib/blum_4659082.cfm#link_3""><u>3</u></a>] that it can be done: Alice “calls” the flip by announcing to Bob not her choice, but some data which commits her to her choice. Then Bob announces the outcome of the flip, and they both check whether the committing data reveals a choice that wins or loses the flip. The notion of commitment (Alice creating a piece of data from a secret bit that can later be used, with Alice's cooperation, to reveal the bit in a way that can neither be manipulated by Alice nor questioned by Bob) has since become an important tool in cryptography.</p>
<p>Perhaps the most consequential application of complexity initiated by Manuel Blum relates to randomness. We often use pseudo-random number generators to instill randomness into our computations. But to what extent are the numbers generated truly random, and what does this mean, exactly? Blum's idea was that the answer lies in complexity. Distinguishing the output of a good generator from a truly random sequence seemed be an intractable problem, and many random number generators that were popular at the time fail this test. In 1984, Blum and his student Silvio Micali came up with a good generator [<a href=""/bib/blum_4659082.cfm#link_4""><u>4</u></a>] based on another notoriously difficult problem in number theory, the <a href=""https://en.wikipedia.org/wiki/Discrete_logarithm"">discrete logarithm problem</a>. With Lenore Blum and Michael Shub they found another generator [<a href=""/bib/blum_4659082.cfm#link_5""><u>5</u></a>] }using &nbsp;repeated squaring modulo the product of two large primes. To discover the seed, the adversary must factor the product, and we know that this is a hard problem.</p>
<p>On top of all this, Blum demonstrated that cryptographically strong pseudorandom generators can pay back their intellectual debt to cryptography. He and student Shafi Goldwasser came up in 1986 with a public key encryption scheme [<a href=""/bib/blum_4659082.cfm#link_6""><u>6</u></a>] based on the Blum-Blum-Shub generator [<a href=""/bib/blum_4659082.cfm#link_5""><u>5</u></a>] which, unlike RSA, has been mathematically proven to be as hard to break as factoring.</p>
<p>In the mid-1980s, Blum became interested in the problem of checking the correctness of programs. Given a program that purports to solve a particular problem, can you write a program checker for it: an algorithm that interacts with the program and its input, and eventually decrees (in reasonable time and with great certainty) whether or not the program works correctly. Checkability turns out to be a windfall of complexity, and Manuel and his students Sampath Kannan and Ronitt Rubinfeld were able to come up with some interesting checkers [<a href=""/bib/blum_4659082.cfm#link_7""><u>7</u></a>]. They used the tricks of their trade: randomness, reductions, and algebraic maneuvers reminiscent of coding theory.</p>
<p>All said, this work was not very successful in helping software engineers in making a dent into the practical problem of software testing. Instead, it serendipitously inspired other theoreticians to come up with more clever tricks of this sort, and eventually use them—closing a full circle—in the study of complexity. The concept of <a href=""https://en.wikipedia.org/wiki/Interactive_proof_system"">interactive proof</a> was proposed, whereby an all-powerful prover can convince a distrustful but limited checker that a theorem is true (just as a program interacts with its checkers to convince it of its correctness), and such proof systems were demonstrated for truths at higher and higher spheres of complexity. And, finally, in 1991, this long chain of ideas and results—this intellectual Rube Goldberg structure initiated by Blum's ideas about program checking—culminated with the famed <a href=""https://en.wikipedia.org/wiki/Probabilistically_checkable_proof"">PCP (Probabilistically checkable proof) Theorem</a> stating that any proof can be rewritten in such a way that its correctness can be tested with high confidence by quickly examining for consistency just a few bits of the proof! Importantly, the PCP theorem implies a result which had been long sought by complexity theorists, namely that, assuming that P is not NP, not only can we not solve the notorious NP-complete optimization problems <em>exactly</em> in polynomial time, we cannot even approximately.</p>
<p>In the mid-1990s, Manuel and Lenore Blum went to Hong Kong for a long sabbatical, and soon after that Manuel decided to retire from UC Berkeley and move to CMU. At CMU, Blum envisioned a new use for complexity, motivated by the vagaries of our networked environment: What if one takes advantage of the complexity of a problem—and the difficulty involved in solving it by a computer program—to verify that the solver is <em>not</em> a computer program? This kind of twisted Turing test would be useful, for example, in making sure that the new users signing up for a web service are not mischievous bots with base intentions. In 2000 Blum and his student Von Ahn, with the collaboration of others, came up with the idea of a visual challenge (“what is written on this wrinkled piece of paper?”) as an appropriate test. They called it “Completely Automated Public Turing test to tell Computers and Humans Apart,” or <em>CAPTCHA</em> for short [<a href=""/bib/blum_4659082.cfm#link_8""><u>8</u></a>]. And, as we all know all too well from our encounters with this CAPTCHA device on web pages, the idea spread. Blum is now working on harnessing people's creativity through games, and on a computational understanding of consciousness.</p>
<p>The research career of Manuel Blum is extraordinary. It is not just the sheer number and impact of his contributions, it is the unmistakable style that runs through them. Manuel believes that research results should be surprising, paradoxical, and nearly contradictory. “When you can prove that a proposition is true, and also that the same proposition is false, then you know you are on to something,” he says. A wonderful advisor with an amazing array of former students, he is always delighted to impart wisdom. &nbsp;His “advice to a new graduate student"" essay is a must-read. Quoting Anatole France, he implores students to “know something about everything and everything about something.” He urges them to read books as random access devices, not necessarily from beginning to end. And to write: if we read without writing, he observes, we are reduced to finite state machines; it is writing that makes us all-powerful Turing machines. He advocates the advice of John Shaw Billings: “First, have something to say.&nbsp; Second, say it. Third, stop when you have said it.”</p>
<p align=""right""><span class=""callout"">Author: Christos H. Papadimitriou</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/blum_4659082.cfm""><img src=""/images/lg_aw/4659082.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Manuel Blum""></a>
<br><br>
<h6 class=""label""><a href=""/photo/blum_4659082.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>Caracas, Venezuela, April 26, 1938</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>B.S., Electrical Engineering, MIT (1959); M.S., Electrical Engineering, MIT (1961); Ph.D., Mathematics, MIT (1964).</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Research Assistant and Research Associate for Dr. Warren S. McCulloch, Research Laboratory of Electronics, MIT, (1960-1965); Assistant Professor, Mathematics, MIT (1966-68); Visiting Assistant Professor, Associate Professor, Professor, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, (1968-2001); Associate Chair for Computer Science, U.C. Berkeley (1977-1980); Arthur J. Chick Professor of Computer Science, U.C. Berkeley, (1995-2001); Visiting Professor of Computer Science. City University of Hong Kong (1997-1999); Bruce Nelson Professor of Computer Science, Carnegie Mellon University, 2001-present.</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>ACM Turing Award (1995); <span style=""line-height: 1.3;"">Fellow of&nbsp;</span><span style=""line-height: 20.8px;"">the Institute of Electrical and Electronics Engineers (1982);&nbsp;</span><span style=""line-height: 20.8px;"">the American Association for the Advancement, of Science (1983); and&nbsp;</span><span style=""line-height: 1.3;"">the American Academy of Arts and Sciences (1995).&nbsp;</span><span style=""line-height: 20.8px;"">Member,&nbsp;</span><span style=""line-height: 20.8px;"">National Academy of Engineering;</span><span style=""line-height: 1.3;"">&nbsp;United States National Academy of Sciences (2002).&nbsp;</span></p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100159946","Manuel Blum","<li class=""bibliography""><a href=""/bib/blum_4659082.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""key-words""><a href=""/keywords/blum_4659082.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/interviews/blum_4659082.cfm""><span></span>Video Interview</a></li>"
"1573179615-695","https://amturing.acm.org/award_winners/hennessy_1426931.cfm","For pioneering a systematic, quantitative approach to the design and evaluation of computer architectures with enduring impact on the microprocessor industry.","<div class=""awards-winners__citation-text"">
<p>John L. Hennessy, born in 1952, was raised on Long Island’s north shore in Huntington, New York. His mother was a teacher before retiring to raise six children; his father was an electrical engineer. He was a tinkerer in high school, winning a science fair prize for an automated tic-tac-toe machine. This impressed the mother of his senior prom date, Andrea Berti, a girl he knew from his shelf-stocking job at the local King Kullen grocery store. He enrolled at Villanova University near Philadelphia, earning a bachelor’s degree in electrical engineering (1973). For graduate school, he returned to Long Island, attending Stony Brook University (then S.U.N.Y. Stonybrook), married Andrea in 1974, and garnered a master’s (1975) and Ph.D (1977) in computer science.</p>
<p>Hennessy became an Assistant Professor at Stanford in September, 1977, remaining for virtually his entire career. Coincident with his first major honor, the John J. Gallen Memorial award by Villanova in 1983, he became an Associate Professor at Stanford. In 1986 he became the inaugural holder of the Willard and Inez Kerr Bell endowed chair.</p>
<p>His work centered on computer architecture. In 1980, microcomputers were rapidly advancing in complexity, to challenge the capabilities of minicomputers. The prevailing wisdom was that powerful processors needed very large, very rich instruction sets. As Hennessy observed in his Turing award lecture, “Microcomputers were competing on crazy things like here’s my new instruction to do this kind of thing … rather than saying here’s a set of standard benchmarks, and my machine is faster than your machine….”<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn1"" name=""_ednref1"" title="""">[1]</a></p>
<p>Hennessy won fame by challenging this mindset with his work on reduced instruction set computer architectures (RISC), along with David Patterson, a Berkeley professor. They first met at a microprocessor conference in 1980 where each was presenting similar micro-coding concepts. Hennessy recalled that “like Dave at Berkeley, we started with a clean slate with our graduate student class that was sort of a brainstorming class. We had a unique perspective. People were ignoring basic performance implications completely. It was an efficiency argument from the very beginning…. We both built prototypes of our design, and we could see that the advantages were clear. These were academic prototypes built by graduate students.”</p>
<p>Building on the original RISC work of John Cocke at IBM, in 1983, Hennessy’s Stanford team developed a prototype chip named MIPS (Microprocessor without Interlocked Pipeline Stages). The first MIPS chip used 25,000 transistors and ran at a slightly faster clock speed than a similar Berkeley chip called RISC-2 (40,760 transistors).<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn2"" name=""_ednref2"" title="""">[2]</a> To advance and commercialized this technology he co-founded MIPS Computer Systems in 1984, during a sabbatical from Stanford. He served eight years as their chief scientist, and six more as chief architect. MIPS was later acquired by Silicon Graphics, where its processors, combined with custom graphics developed by James Clark at Stanford, powered the high-performance graphics workstations relied on by Hollywood in the late 1980s and 1990s.</p>
<p>Patterson recalled that: “There is this remarkable point in time when it was clear that a handful of grad students at Berkeley or Stanford could build a microprocessor that was arguably better than what industry could build—faster, cheaper, more efficient…. RISC was very controversial, it was heretical… and John and I were on the RISC side while all the other people were on the CISC side…. We had a hard time convincing people of that.” <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn3"" name=""_ednref3"" title="""">[3]</a></p>
<p>While others argued about the relative merits of the Hennessy and Patterson variants of RISC, they recognized that the much larger contest was between RISC ideas embodied in both of their chips versus the CISC (Complex Instruction Set Computing) architectures then used throughout the industry from mainframes to personal computers. The two began a partnership, creating a systematic quantitative approach for designing faster, lower power and reduced complexity microprocessors, co-authoring two books that became landmark textbooks for the discipline. The first, <em>Computer Architecture: A Quantitative Approach</em>, now in its sixth edition, established enduring principles for generations of architects. <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn4"" name=""_ednref4"" title="""">[4]</a></p>
<p>Patterson quantified the impact of this work in his Turing lecture, given jointly with Hennessy: “Our colleagues at Intel … had great technology…. They got up to 350 million chips per year, not only dominating the desktop, but servers as well…. But the Post-PC era, starting with the iPhone in 2007 totally changed things… valuing area and energy as much as performance. Last year there were more than 20 billion chips with 32-bit processors in them. [Intel compatible] chips peaked in 2011 with dropping sales of PCs, and there are only 10 million chips in the cloud, so 99% of the processors today are RISC.” <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn5"" name=""_ednref5"" title="""">[5]</a></p>
<p>Hennessy’s career at Stanford led him from research to administrative leadership. Within five years of becoming department chair in 1994 he was appointed Provost, working with his former colleague, Jim Clark (founder of Silicon Graphics) to arrange a record-setting donation to create a biological engineering and sciences center. Clark said of Hennessy: “The most lasting impression was how good he was with students, how hard he worked and how helpful he was with my project."" <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn6"" name=""_ednref6"" title="""">[6]</a> In another year he rose to the top of a pool of five hundred candidates to became president of Stanford, helped by his exceptional connections to Silicon Valley’s high-tech industry. He co-founded Atheros as well as MIPS, and he served many years on the Cisco Systems Board of Directors, and subsequently on the Google Board, where in 2016 he became chairman of Alphabet, Google’s parent company. Under his leadership Stanford’s fundraising brought in $13 billion,” <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn7"" name=""_ednref7"" title="""">[7]</a> including a five-year campaign from 2007-2011 that $6.23 billion, 60% more than the previous record for any university.<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn8"" name=""_ednref8"" title="""">[8]</a></p>
<p>During sixteen years as president, Hennessey reshaped Stanford’s buildings, the campus, its research profile, and its financial resources. An appreciative article in <em>Stanford Magazine</em> catalogued his accomplishments of his term: “70 building projects,” a cultural shift on campus to “a deep commitment to interdisciplinary collaboration,” and the “deft and decisive handling of” of the challenges of a major recession. Maybe most importantly, and surprisingly to many, was Hennessy’s devotion to students, to interdisciplinary studies, to the humanities, and the arts. Hennessy pushed for world-class performance and exhibition spaces, drawing on a comment from Itzhak Perlman that “Mr. President, Stanford is a great university, but you have terrible performance facilities.” Hennessy called this complaint “a gift to a president, because there’s a story I can repeat from an expert.”&nbsp;<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn9"" name=""_ednref9"" title="""">[9]</a> Since retiring as president in 2016 he has been the inaugural director of the Knight-Hennessy Scholars program.</p>
<p>Fittingly, for the two RISC champions who took on the computer establishment in the 1980s, Hennessy and Patterson have returned to their first love—computing architectures—as they savor their joint selection as the 2017 ACM Turing award winners. Their Turing address challenged the idea that potential processor performance has little scope for dramatic improvement of the kind seen in previous decades. Not so: “innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development” will multiple current system throughput “tens, hundreds, thousands of times—up to 62,000 times.” Their audience was listening as intently as ever.<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_edn10"" name=""_ednref10"" title="""">[10]</a></p>
<p>Hennessy has received numerous regional, national, and international awards, plus eleven honorary doctorates. His computing architecture awards include Fellows of IEEE (1991), American Academy of Arts and Sciences (1995), ACM (1997), and the UK Royal Academy of Engineering (2017). He received the Seymour Cray Computer Engineering award in 2001, and he was honored with IEEE’s highest honor, the Medal of Honor, in 2012, ""for pioneering the RISC processor architecture and for leadership in computer engineering and higher education.""</p>
<p>Hennessy and Patterson have won a number of joint awards, including the John von Neumann Medal (IEEE, 2000), the Eckert-Mauchly ACM/IEEE award in 2001; Fellows for the Computer History Museum in 2007, and the ACM Turing Award in 2017.</p>
<address style=""margin-left: 3.5in; text-align: right;"">Author: Charles H. House</address>
<div><br clear=""all"">
<hr align=""left"" size=""1"" width=""33%"">
<div id=""edn1"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref1"" name=""_edn1"" title="""">[1]</a> Hennessy, John L. and David A. Patterson, “A new golden age for computer architecture: domain-specific hardware/software co-design, enhanced security, open instruction sets, and agile chip development,” 2017 ACM A.M.Turing Award lecture, 45<sup>th</sup> ISCA (International Symposium of Computer Architecture), Los Angeles, June 4, 2018 <a href=""https://www.acm.org/hennessy-patterson-turing-lecture"">https://www.acm.org/hennessy-patterson-turing-lecture</a></p>
</div>
<div id=""edn2"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref2"" name=""_edn2"" title="""">[2]</a> Hennessy, John L.; Forest Baskett; et al, “MIPS, A Microprocessor Architecture,” ACM SIGMICRO Newsletter, 13:4; 1983</p>
</div>
<div id=""edn3"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref3"" name=""_edn3"" title="""">[3]</a> Patterson, David A., “A New Golden Age for Computer Architecture: History, Challenges, and Opportunities,” UC Berkeley ACM Turing Laureate Colloquium lecture, October 10, 2018; <a href=""https://eecs.berkeley.edu/turing-colloquium/schedule/patterson"">https://eecs.berkeley.edu/turing-colloquium/schedule/patterson</a></p>
</div>
<div id=""edn4"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref4"" name=""_edn4"" title="""">[4]</a> Hennessy, J. L. and Patterson, D. A. <a href=""http://www.mkp.com/books_catalog/1-55860-329-8.asp"">Computer Architecture: A Quantitative Approach</a>. 1990. Morgan Kaufmann Publishers, Inc. San Mateo, CA. Second edition 1995, Third edition, 2002. Fourth Edition, 2007, Fifth Edition, 2011, Sixth Edition, 2018. Also Patterson, D.A. and Hennessy, J.L.,&nbsp;<a href=""http://www.mkp.com/books_catalog/catalog.asp?ISBN=1-55860-428-6"">Computer Organization and Design: The Hardware/Software Interface.</a>&nbsp;1993. San Mateo, CA: Morgan Kaufmann Publishers. Second Edition, 1998, Third Edition 2005.</p>
</div>
<div id=""edn5"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref5"" name=""_edn5"" title="""">[5]</a> Hennessy and Patterson, 2017 ACM A.M.Turing award lecture, <em>op. cit.</em></p>
</div>
<div id=""edn6"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref6"" name=""_edn6"" title="""">[6]</a> Swanson, Doug, “Favorite Son,” <u>Stanford Magazine</u>, May-June 2000; <a href=""https://stanfordmag.org/contents/favorite-son"">https://stanfordmag.org/contents/favorite-son</a></p>
</div>
<div id=""edn7"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref7"" name=""_edn7"" title="""">[7]</a> Antonucci, Mike, “Where he took us,” <u>Stanford Magazine</u>, May-June 2016; <a href=""https://stanfordmag.org/contents/where-he-took-us"">https://stanfordmag.org/contents/where-he-took-us</a></p>
</div>
<div id=""edn8"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref8"" name=""_edn8"" title="""">[8]</a> Kiley, Kevin, “Stanford raises $6.2B in five-year campaign,” <u>Inside Higher Ed</u>, February 8, 2012; <a href=""https://www.insidehighered.com/quicktakes/2012/02/08/stanford-raises-62-billion-five-year-campaign"">http://www.insidehighered.com/quicktakes/2012/02/08/stanford-raises-62-billion-five-year-campaign</a></p>
</div>
<div id=""edn9"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref9"" name=""_edn9"" title="""">[9]</a> Antonucci, Mike, “Where he took us,” <em>op. cit.</em></p>
</div>
<div id=""edn10"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Hennessy%20v3.docx#_ednref10"" name=""_edn10"" title="""">[10]</a> Hennessy and Patterson, 2017 ACM A.M.Turing award lecture, <em>op. cit. </em>Also Hennessy, John L. and David A. Patterson, “A New Golden Age for Computer Architecture,” <u>Communications of the ACM</u> (62:2) February 2019, pp. 48-60</p>
</div>
</div>
<p>&nbsp;</p>
</div>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/hennessy_1426931.cfm""><img src=""/images/lg_aw/1426931.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""John L Hennessy""></a>
</div>
<h6><a href=""/stonebraker_1172121.pdf"" target=""_blank"" title=""PDF format"">BIRTH:</a></h6>
<p>September 22, 1952.</p>
<h6><a href=""/stonebraker_1172121.pdf"" target=""_blank"" title=""PDF format"">EDUCATION:</a></h6>
<p>Bachelor’s degree in Electrical Engineering (Villanova University, 1973); M.Sc. in Computer Science (S.U.N.Y. Stonybrook, 1975); Ph.D. in Computer Science (S.U.N.Y. Stonybrook, 1977).</p>
<h6>EXPERIENCE</h6>
<p>Assistant Professor of Electrical Engineering (Stanford University, 1977-1983), Associate Professor (1983-1986),&nbsp;Willard and Inez Kerr Bell Endowed Professor of Electrical Engineering and Computer Science (1986-present), Dean (1996-1999), Provost (1999-2000), President (2000-2016),&nbsp;Director Knight-Hennessy Scholars Program, (2016-present).&nbsp;Concurrent industrial roles included&nbsp;Co-founder and Chief Scientist, MIPS Computer Systems (1984-1992); Chief Architect, Silicon Graphics Computer Systems (1992-1998).</p>
<h6>HONORS AND AWARDS (SELECTED):</h6>
<p>John J.&nbsp;Gallen&nbsp;Memorial Award, Villanova University (1983); Presidential Young Investigator, National Science Foundation (1984); Fellow, Institute of Electrical and Electronics Engineers (1991); Member, National Academy of Engineering (1992); IEEE&nbsp;Emannuel&nbsp;R.&nbsp;Piore&nbsp;Award (1994); Fellow, American Academy of Arts and Sciences (1995); Fellow, Association for Computing Machinery (1997); John Von Neumann Medal (jointly with D. Patterson), IEEE (2000); Eckert-Mauchly Award, Association for Computing Machinery and IEEE Computer Society (2001); Seymour Cray Computer Engineering Award,(2001); Member, National Academy of Sciences (2002); NEC Computers and Communications Prize (2004); Founders Award, American Academy of Arts and Sciences (2005); Fellow, Computer History Museum (2007); Member, American Philosophical Society (2008); Morris Chang Exemplary Leadership Award, Global Semiconductor Alliance (2010); IEEE Medal of Honor (2012); Fellow of the Royal Academy of Engineering, United Kingdom (2017); ACM A.M. Turing Award&nbsp;(jointly with D. Patterson),&nbsp;2017. At least 11 honororary doctorates as of 2018.</p>","","https://dl.acm.org/author_page.cfm?id=81100207767","John L Hennessy","<li class=""bibliography""><a href=""/bib/hennessy_1426931.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/hennessy_1426931.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/hennessy_1426931.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179341-678","https://amturing.acm.org/award_winners/kahan_1023746.cfm","For his fundamental contributions to numerical analysis. One of the foremost
experts on floating-point computations. Kahan has dedicated himself to ""making the world safe for
numerical computations""!","<p><strong>William Kahan was born in Canada in 1933 and grew up around Toronto in a family of Jewish immigrants.</strong> His mother created dress designs for a successful factory run by his father. His family called him ""Velvel"", or ""little wolf,"" a nickname still favored by his friends over the Anglicized ""William."" As a young man Kahan loved to fix mechanical and electronic devices, an inclination that gained him a summer job repairing electronic equipment used by Canadian Armed Forces for a while after WW-II. He has retained this love of tinkering throughout his life.He has retained this love of tinkering throughout his life. He kept a pair of customized 1984 Peugeot 505s running for more than&nbsp;thirty years, and long eschewed laser printers in favor of a dot matrix printer&nbsp;he had&nbsp;modified&nbsp;to rapidly print mathematical texts.</p>
<p>Kahan learned to program during the summer of 1953 as an undergraduate majoring in mathematics at the University of Toronto. The university was a center of early computer development and use, and owned <a href=""https://en.wikipedia.org/wiki/Ferranti_Mark_1"" target=""_blank"">FERUT</a>, one of the world’s first commercially manufactured computers –the second Manchester Ferranti Mark I ever built. Remaining at Toronto as a graduate student, he focused his studies on numerical analysis and explored the new possibilities in applied mathematics that&nbsp;were opened up by the use of computers. He made extended visits to two other computing centers: he spent the summer of 1957 working on the <a href=""https://en.wikipedia.org/wiki/ILLIAC"" target=""_blank"">ILLIAC</a> I at the University of Illinois, and after completing his Ph.D. in 1958 spent two years with the EDSAC-2 at Cambridge University's Mathematical Laboratory.</p>
<p>He returned to Toronto in 1960 as a faculty member, where his research focused on the error-analysis of numerical computations. For Kahan this involved not just determination of the accuracy of calculated results, but also the design of new software and architectural features to improve accuracy while maintaining high performance. He created an integrated system of mathematical routines, compiler tweaks and operating system modifications for the university’s IBM 7094 computer to help programmers create accurate, high performance <a href=""/info/kahan_1023746.cfm"">floating point&nbsp;</a>code. Kahan played a leading role in the numerical analysis subgroup of the IBM computer user group <a href=""https://en.wikipedia.org/wiki/SHARE_%28computing%29"" target=""_blank"">SHARE</a>, and spearheaded its successful campaign in 1966-1967 to force IBM to fix design flaws in the arithmetic of its new System /360 computers.</p>
<p>Kahan left Toronto in 1968 for the University of California, Berkeley, to bolster its newly created department of computer science. (Another Turing Award winner, <a href=""/award_winners/karp_3256708.cfm"">Richard M. Karp</a>,&nbsp;joined the same year as Kahan). Kahan credits the move to a combination of the attractions of Northern California for his children, who had been struck by its beauty during his recent sabbatical at Stanford University, and his dwindling faith in the future of high performance computing in Canada following the collapse of the country’s high technology defense sector.</p>
<p>Numerical analysis is at the intersection of computing and applied mathematics. Within that community Kahan is recognized as a researcher and theorist of exceptional talent. He contributed to several widely used algorithms, including the <a href=""https://en.wikipedia.org/wiki/Bidiagonalization"" target=""_blank"">Golub-Kahan </a>variant of the QR algorithm for singular value decomposition (used for matrix calculations fundamental to many kinds of computation) and&nbsp;a<a href=""https://en.wikipedia.org/wiki/Kahan_summation_algorithm"" target=""_blank""> Compensated Summation algorithm</a> to offset the worst rounding errors in trajectory computations.</p>
<p>Throughout his tenure at Berkeley, Kahan’s work involved the creation of practical tools as well as papers, algorithms and theorems. Together with his students he produced the widely used <em>fdlibm</em> mathematics library distributed with BSD Unix and used to implement mathematical computations for machines supporting the new IEEE 754 floating-point standard. Another widely used work was <a href=""https://en.wikipedia.org/wiki/William_Kahan"" target=""_blank"">paranoia</a>, a program that tests floating point arithmetic implementations for errors. He was also a committed, if demanding, teacher and invested considerable time tutoring undergraduates&nbsp;to solve problems of the kind encountered in the&nbsp;<a href=""https://en.wikipedia.org/wiki/William_Lowell_Putnam_Mathematical_Competition"" target=""_blank"">William Lowell Putnam Mathematics Competition</a>.</p>
<p>At Berkeley, Kahan began to consult for computer manufacturers. From 1974 to 1984 he assisted Hewlett Packard with the mathematical functioning of its calculators, improving the accuracy and performance of many models, including the classic HP34C and HP12C, and adding widely used functions such as <em>integrate</em> and <em>solve</em> to package complex mathematical operations in a convenient and easy to use form. The continuing popularity of the HP12C financial calculator is testified to by its 30th anniversary edition, released in 2011.</p>
<p>During a long and productive relationship with Intel he specified the design for its floating-point arithmetic on several chips starting with the 8087, released in 1980, and then the 80387 and 486DX. Thanks to the success of IBM's Personal Computer and its direct descendants, those chips were used on millions of computers. When Intel redesigned its floating point implementation for the Pentium processor, launched in 1994, it introduced a high profile ""Divide bug""; subsequently Kahan supplied a test program to prevent that from happening again.</p>
<p>In 1977 Kahan became active in the fledgling IEEE effort initiated by Robert Stewart to define a standard for computer arithmetic. Variations in floating-point arithmetics implemented on different kinds of computers often caused programs written in a standard language such as FORTRAN to give very different results when recompiled and run on another computer. The sudden proliferation of microprocessors and workstation systems threatened to worsen diversity and render uneconomical the development, testing and wide distribution of engineering and even some business software.</p>
<p>Most people expected the IEEE committee to endorse the existing approach of some particular manufacturer, such as DEC or IBM. Instead, Kahan persuaded the committee to endorse an entirely new design modeled on his then-secret work for Intel. The committee's doubts about efficient implementation were eventually overcome in what Kahan termed ""late-night educational sessions."" By the time the standard was officially adopted as IEEE 754 in 1985, it had already been implemented widely based on drafts published from 1979 onward with the help of his then graduate student Jerome Coonen. Kahan won the ACM’s Turing Award in 1989 for his work in creating that IEEE 754 standard, and he has often been called “The Father of Floating Point”.</p>
<p>Kahan takes personal credit for just two aspects of the 8087 and the IEEE standard that evolved from it: its “inexact flag” that programmers could test to see if a result had been approximated, and the provision of a NaN (Not a Number) code to report the creation of a result for which no numerical representation is possible (such as the quotient 0/0). These novelties, if supported by the programming language, let a programmer suspend judgment about speculatively executed operations because any serious anomalies could be detected by comparatively few tests conducted when the code was run. “Everything else” in the design, he told me, “was deduced” based on technical necessity and his decades of experience with existing computer arithmetic units and the needs of technical computation users.<a href=""/award_winners/kahan_1023746.cfm#link_1""><sup>1</sup></a></p>
<p>Adoption of the standard did a great deal to improve the robustness of floating point arithmetic and improve consistency of results across different computing platforms. Kahan himself regrets that IEEE 754 was generally seen only as a hardware standard, rather than as a specification for a computing environment in which hardware, compilers and application programs would interact. Some hardware features were neglected by compiler writers and language designers, which deprived programmers of their benefits and made it less likely that hardware producers would implement them effectively.</p>
<p>In recent decades Kahan has continued to articulately and bluntly warn of the shortcomings in the floating point implementations of environments as popular as Java and Matlab. The title of one paper, “<a href=""https://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf"" target=""_blank"">How Java’s Floating-Point Hurts Everyone Everywhere</a>”, gives a good idea of his willingness to play the part of curmudgeon in pointing out the deficiencies of fashionable ideas, products or technologies.</p>
<p>After retirement Kahan became Professor Emeritus at UC Berkeley. His career has been marked by faith in rigorous analysis and in the power of superior designs to win over skeptics. Kahan would rather be right than popular, and even as a young man he showed little deference when he thought someone was mistaken. When visiting Cambridge as a postdoctoral fellow he challenged the competence of Maurice Wilkes, the leading figure in British computing, to lecture on Numerical Analysis. Kahan’s version of the story ends with him informing Wilkes that “there is a difference between elementary and superficial”.<a href=""/award_winners/kahan_1023746.cfm#link_1""><sup>1</sup></a> He mentioned to me that Wilkes would tell the same story to make a different point.</p>
<p>In March 2016 he&nbsp;summarized his part in the later revision of the standard by proposing the following text: ""Starting in 2000, Kahan participated initially in an obligatory revision of IEEE 754 subjected to an official demand that the revision cover decimal arithmetic as well as binary. As the committee grew, so did centrifugal demands for the inclusion of options that have compounded complexity. He regrets that the result released in 2008 has become so long and unreadable that it discourages the wide adoption in hardware of decimal floating-point, which is more humane than binary albeit less liked by mathematical error-analysts like him.""</p>
<p>Unsurprisingly, Kahan finds little inherent reward in the politics and bureaucracy of committee work, so his commitment to the IEEE standards project over many years surprised some colleagues. It reflected his determination to apply the force of his intellect to end wasted effort and unnecessary frustration. Kahan’s success shows that commitment to uncompromising technical rigor and academic excellence, coupled with more than a little luck, can transform real world computer hardware and software. Sometimes an idealistic refusal to accept the deficiencies of established ways of doing things really does pay off.</p>
<p>Kahan is married to Sheila K. Strauss. They have two sons and four granddaughters.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Thomas Haigh</span></p>
<p>Notes:</p>
<p><a name=""link_1""><sup>1</sup></a> William Kahan, Oral History Interview With Thomas Haigh, 5-8 August 2005. <a href=""http://history.siam.org/oralhistories/kahan.htm"">Available online from SIAM.</a></p>","<div class=""featured-photo"">
<a href=""/award_winners/kahan_1023746.cfm""><img src=""/images/lg_aw/1023746.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""William Kahan""></a>
<br><br>
<h6 class=""label""><a href=""/photo/kahan_1023746.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>June 5, 1933, in Toronto, Ontario, Canada</p>
<h6 class=""label"">EDUCATION:</h6>
<p>B.Sc. 1954; M.Sc. 1956; Ph.D. 1958 (all in Mathematics, University of Toronto); Honorary Doctor of Mathematics, University of Waterloo, Canada, 1998; Honorary Doctor of Mathematics, Chalmers Inst., Goteborg, Sweden, 1993.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Postdoctoral fellow, Cambridge University Mathematical Laboratory (UK) 1958-1960; Assistant/Associate Professor of Mathematics, University of Toronto 1960-1968; Professor of Mathematics/Electrical Engineering &amp; Computer Science (currently Emeritus Professor), University of California Berkeley 1968.</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>First ACM George. E. Forsythe Memorial Award (1972); ACM Turing Award, 1989; ACM Fellow (1994); SIAM John von Neumann Lecture (1997); IEEE Emanuel R. Piore Award (2000); Foreign (Canadian) Associate, National Academy of Engineering (2005). Holds honorary doctorates from Goteborg, Sweden (1993) and from University of Waterloo, Canada (1998).<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100645838","William (“Velvel”) Morton Kahan","<li class=""bibliography""><a href=""/bib/kahan_1023746.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""key-words""><a href=""/keywords/kahan_1023746.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/kahan_1023746.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/kahan_1023746.cfm""><span></span>Video Interview</a></li>"
"1573179732-704","https://amturing.acm.org/award_winners/yao_1611524.cfm","In recognition of his fundamental contributions to the theory of computation, including the complexity-based theory of pseudorandom number generation, cryptography, and communication complexity.","<p>Andrew Chi-Chih Yao was born in Shanghai, China, on December 24, 1946. After moving with his family to Hong Kong for two years he immigrated to Taiwan. In 1967 he received a B.S. in Physics from the National University of Taiwan. He then started graduate studies in Physics at Harvard University, where he received an A.M. in 1969 and a Ph.D. in 1972 under the supervision of Sheldon Glashow, winner of the 1979 Nobel Prize in Physics. He subsequently entered the Ph.D. program in Computer Science at the University of Illinois Urbana-Champaign, and received his degree just two years later, in 1975. Yao completed his dissertation, <em>A Study of Concrete Computational Complexity, </em>under the supervision of <a href=""https://en.wikipedia.org/wiki/Chung_Laung_Liu"" target=""_blank"">Chung Laung Liu</a>.</p>
<p>After a year as an Assistant Professor in the Mathematics Department at MIT, Yao joined the Computer Science Department at Stanford University as an Assistant Professor in 1976. Over the next five years there he made a number of fundamental contributions to the theory of algorithms.</p>
<p>His 1977 paper “Probabilistic computations: toward a unified measure of complexity,” [<a href=""/bib/yao_1611524.cfm#bib_1"">1</a>] introduced what is now known as Yao’s min-max principle, which uses von Neumann’s <a href=""https://en.wikipedia.org/wiki/Minimax"" target=""_blank"">minimax</a> theorem from game theory to relate average-case complexity for deterministic algorithms to worst-case complexity for randomized algorithms. Yao proved that the expected running time of any randomized algorithm on worst-case input is equal to the average-case running time of any deterministic algorithm for the worst-case distribution of inputs. Yao’s principle has become a fundamental technique for reasoning about randomized algorithms and complexity, and has also been applied in areas such as property testing and learning theory.</p>
<p>Around this time Yao also made fundamental contributions to the theory of data structures. His 1978 paper, “Should tables be sorted?” [<a href=""/bib/yao_1611524.cfm#bib_2"">2</a>] introduced the cell-probe model, an abstract model of data structures where the cost of a computation is measured by the total number of memory accesses. This model has been widely used in creating lower bound proofs of algorithms.</p>
<p>Yao spent a year as a Professor in the Computer Science Division of the University of California, Berkeley, and subsequently returned to Stanford as a full Professor in 1982. During the early 1980’s, Yao produced a number of papers which had a lasting impact on the foundations of cryptography, computer security, computational complexity and randomized computation. This work was significant not only for results obtained, but also for the introduction of problems, models and techniques which are now considered fundamental in their respective areas.</p>
<p>His 1981 paper with Danny Dolev, “On the security of public-key protocols,” [<a href=""/bib/yao_1611524.cfm#bib_8"">8</a>] introduced a formal model for symbolic reasoning about security protocols. Since its introduction, this “<a href=""https://en.wikipedia.org/wiki/Dolev-Yao_model"" target=""_blank"">Dolev-Yao model</a>” has been the starting point for most work done on symbolic security, including recent work on the security of complexity-based cryptography, This continues to be an active area of research. Yao also made significant contributions to cryptography and complexity-based approaches to security. In 1982 he published “Theory and applications of trapdoor functions” [<a href=""/bib/yao_1611524.cfm#bib_7"">7</a>] and “Protocols for secure computations” [<a href=""/bib/yao_1611524.cfm#bib_5"">5</a>]. These works, which were introduced at the same conference, stand as seminal contributions in cryptography and secure computation.</p>
<p>The first of these papers addresses the then newly-emerging field of <a href=""https://en.wikipedia.org/wiki/Public-key_cryptography"" target=""_blank"">public-key cryptography</a> from a theoretical perspective, lays the foundation for a theory of computational randomness, and initiates a study of its relationship to computational hardness. Yao provides a definition of <a href=""https://en.wikipedia.org/wiki/Pseudorandom_number_generator"" target=""_blank"">pseudorandom number generator</a> which is based on computational complexity, and proposes a definition of “perfect”—in current terminology, “pseudorandom”—probability distribution ensembles. (An ensemble is perfect if it cannot be distinguished from a truly random ensemble by any feasible distinguisher, where such distinguishers are formalized using the notion of a polynomial time randomized algorithm.) Yao relates his notion of pseudorandomness to the idea of a statistical test, a notion already used in the study of pseudorandom number generators, and shows that one particular test, known as the <a href=""https://en.wikipedia.org/wiki/Next-bit_test"" target=""_blank"">next-bit test</a>, is adequate for characterizing pseudorandomness. Having defined perfect ensembles, Yao then defined a pseudorandom number generator as an efficient randomized algorithm which uses a limited number of truly random bits in order to output a sample from a perfect distribution whose size is polynomial in the number of random bits used.</p>
<p>The next fundamental contribution of the paper addresses the question of what computational assumptions are adequate for the existence of pseudorandom number generators. Advances in public-key cryptography indicated that secure encryption could be based on the assumed hardness of certain computational problems such as <a href=""https://en.wikipedia.org/wiki/Quadratic_residuosity_problem"" target=""_blank"">quadratic residuosity</a>, or problems related to factoring integers. Yao asked whether one could make a general assumption about computational hardness which could be used to obtain pseudorandomness and hence, through standard techniques, cryptographic security. For this he formalized the notion of a one-way function that is easy to compute but hard to invert for a large fraction of inputs. He proved that one-way functions with certain properties may be used to construct pseudorandom number generators. This inspired a series of important results that refined Yao’s work, and it continues to be an area of active research. The contributions of this paper to pseudorandomness form an essential component of modern cryptography. In addition, the paper proposes a new field of computational information theory which refines <a href=""https://en.wikipedia.org/wiki/Information_theory"" target=""_blank"">Shannon’s theory</a> by taking computational resources into account. Yao gives a definition of computational entropy, and uses it to give a characterization of encryption security. This definition of entropy is now important in areas such as <a href=""http://researcher.watson.ibm.com/researcher/view_project_subpage.php?id=2664"" target=""_blank"">leakage-resilient cryptography</a>.</p>
<p>The second paper introduces a new paradigm for secure function evaluation, and introduces the famous “<a href=""https://en.wikipedia.org/wiki/Yao%27s_Millionaires%27_Problem"" target=""_blank"">Millionaires’ Problem</a>”. Yao gives a protocol which allows two parties, each holding a number, to determine who has the larger number without revealing the actual values. The Millionaires’ Problem is a two-party instance of a more general class of secure multiparty computation problems which are now essential to the study of secure cryptographic protocols. With the advent of wide-scale distributed computing and the ubiquity of cryptographic protocols, Yao’s contributions in this area have had a significant impact on networked computing.</p>
<p>In the 1980’s, Professor Yao introduced models and techniques whose ramifications are still being felt in research in complexity, computational randomness, cryptography, and security. Some of his most influential ideas were disseminated in lectures building on his published results. One example is the XOR-lemma, which uses computational hardness to produce pseudorandomness. Yao addressed whether the hardness of a problem may be amplified by combining multiple instances of the problem, in this case through the use of the bitwise exclusive-OR operation. While interesting in its own right, the XOR-lemma is an essential technique in the area of <a href=""https://en.wikipedia.org/wiki/Randomized_algorithm"" target=""_blank"">derandomization</a>, which seeks generic methods for eliminating the need for randomness in the efficient solution of algorithmic problems. &nbsp;More generally, it helps determine whether certain classes of problems that are solved efficiently with randomization can instead be solved deterministically.</p>
<p>A second example is the garbled circuit technique, an important tool in secure multiparty computation which was used implicitly by Yao in his 1982 secure computation paper as well as in a 1986 paper “How to generate and exchange secrets” [<a href=""/bib/yao_1611524.cfm#bib_6"">6</a>]. Recent advances in computing power have made the garbled circuit technique practical for large-scale computational problems, for example in privacy-preserving matching in DNA databases.</p>
<p>In 1986, Yao moved to Princeton University, where he became the William and Edna Macaleer Professor of Engineering and Applied Science. During this period he continued his work on the foundations of cryptography. He also built on previous work in areas such as decision tree and communication complexity. Yao made substantial contributions to the theory of lower bounds for algebraic decision trees, an area he established in a 1982 <em>Journal of Algorithms </em>paper [<a href=""/bib/yao_1611524.cfm#bib_9"">9</a>] co-authored with J.M. Steele. This work exploited deep relationships between algebraic decision trees and mathematical results in algebraic geometry. He also investigated the use of randomization in decision trees. Professor Yao introduced the theory of <a href=""https://en.wikipedia.org/wiki/Communication_complexity"" target=""_blank"">communication complexity</a> in a 1979 paper “Some complexity questions related to distributive computing” [<a href=""/bib/yao_1611524.cfm#bib_3"">3</a>]. &nbsp;In his 1993 paper, “Quantum circuit complexity”[<a href=""/bib/yao_1611524.cfm#bib_4"">4</a>] he extended communication complexity to quantum computing. Starting in the 1990’s, Professor Yao began to work extensively on quantum computing, communication and information theory. He continues to make significant contributions in these areas.</p>
<p>Andrew Yao became a Professor in the Center for Advanced Study and Director of the Institute for Theoretical Computer Science at Tsinghua University, Beijing, in 2004. Since 2005 he has also been Distinguished Professor-at-Large of the Chinese University of Hong Kong. His recent contributions include work in security protocols, universally composable secure computation, quantum computing, and the theory of algorithms.</p>
<p>Yao is active in graduate supervision, and has mentored over twenty Ph.D. students. He is married to Professor Frances Yao, a computer scientist and leading researcher in computational geometry, algorithms and cryptography.</p>
<p>&nbsp;</p>
<p style=""text-align: right; ""><em>Author: Bruce Kapron</em></p>","<div class=""featured-photo"">
<a href=""/award_winners/yao_1611524.cfm""><img src=""/images/lg_aw/1611524.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Andrew C Yao""></a>
</div>
<h6 class=""label""><strong>BIRTH:</strong></h6>
<p>1946, Shanghai, China</p>
<h6 class=""label""><strong>EDUCATION:</strong></h6>
<p>B.S. (Physics, National University of Taiwan, 1967); A.M. (Physics, Harvard University, 1969); Ph.D. (Physics, Harvard University, 1972); Ph.D. (Computer Science, University of Illinois Urbana-Champaign, 1975).</p>
<h6 class=""label""><strong>EXPERIENCE:</strong></h6>
<p>Assistant Professor (Mathematics Department, Massachusetts Institute of Technology, 1975-1976); Assistant Professor (Computer Science Department, Stanford University, 1976-1981); Professor (Computer Science Division, University of California, Berkeley, 1981-1982); Professor (Computer Science Department, Stanford University, 1982-1886); William and Edna Macaleer Professor of Engineering and Applied Science, (Princeton University, 1986-2004); Professor (Center for Advanced Study, Tsinghua University, from 2004); Director, Institute for Theoretical Computer Science, Tsinghua University, from 2004); Distinguished Professor-At-Large (The Chinese University of Hong Kong, from 2005).</p>
<h6 class=""label""><strong>HONORS AND AWARDS:</strong></h6>
<p><a href=""http://www.siam.org/prizes/sponsored/polya.php"" target=""_blank"">George Polya Prize</a> (1987); Guggenheim Fellowship (1991); Fellow, Association for Computing Machinery (1995); <a href=""https://en.wikipedia.org/wiki/Knuth_Prize"" target=""_blank"">Donald E. Knuth Prize</a> (1996); Member, US National Academy of Sciences (1998), and American Academy of Arts and Sciences (2000), Academia Sinica (2000); A.M. Turing Award (2000); Pan Wen-Yuan Research Award (2003); Honorary Doctor of Science (City University of Hong Kong, 2003); Fellow, American Association for the Advancement of Science (2003); Honorary Doctor of Engineering (Hong Kong University of Science and Technology, 2004); Foreign Member, Chinese Academy of Sciences (2004); Alumni Award for Distinguished Service, College of Engineering, University of Illinois (2004); Honorary Doctor of Science (Chinese University of Hong Kong, 2006); Honorary Doctor of Mathematics (University of Waterloo, 2009); Fellow, International Association for Cryptologic Research (2010).</p>","","https://dl.acm.org/author_page.cfm?id=81100545139","Andrew Chi-Chih Yao","<li class=""bibliography""><a href=""/bib/yao_1611524.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""key-words""><a href=""/keywords/yao_1611524.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179486-687","https://amturing.acm.org/award_winners/wilkinson_0671216.cfm","For his research in numerical analysis to facilitiate the use of the high-speed digital computer, having received special recognition for his work in computations in linear algebra and ""backward"" error analysis.","<p>&nbsp;</p>
<p><strong>James (Jim) Hardy Wilkinson was a British mathematician who became the leading expert in a new, and important, field that emerged after World War II.</strong> It goes under two names, <em>matrix computations</em> and (more pompously) <em>numerical linear algebra</em>.</p>
<p>Jim came from a humble family in the dairy business and was the third of five children. He showed a great interest in mathematical problems as a youngster and won a Foundation Scholarship to the Sir Joseph Williamson’s Mathematical School when he was eleven—a fortunate occurrence because the family business had fallen on hard times and they could not have paid for such an elite school.</p>
<p>Wilkinson graduated from Trinity College, Cambridge, the mecca of mathematics in Britain, in 1939 after a stellar undergraduate career. He was expected to become a pure analyst and continue with graduate work at Cambridge, but the outbreak of World War II put an end to that plan. He began working at the British Government Ministry of Supply doing research on topics such as thermodynamics of explosions, ballistics, and similar military interests. This work during the war changed his focus radically.</p>
<p>At the end of the war, rather than returning to Cambridge, he joined the National Physical Laboratory (NPL) staff where he worked with Alan Turing on problems associated with Turing’s proposal to build an electronic computer. When Turing left the NPL to go to Manchester University, Wilkinson (with others) took over the development of Turing’s computer—the <a href=""https://amturing.acm.org/photo/wilkinson_0671216.cfm"">Pilot ACE</a> (operational in 1950).</p>
<p>This essay will attempt to explain, in as simple a way as possible (although some knowledge of mathematics might be helpful) the concerns of this field of matrix computations and why Wilkinson won the ACM A. W. Turing Award in 1970.</p>
<p>The field would not exist without the availability of fast digital computers. Although a few classical mathematicians, such as Carl Friedrich Gauss (1777-1855) and Karl G. J. Jacobi (1804-1851), had engaged in matrix computations by hand (a labor of Hercules) there was no shared field of study.</p>
<p>Let us consider these electronic computing machines for a moment.</p>
<p>People are awed at the prodigious speeds at which they execute primitive arithmetic operations such as addition and multiplication. Yet this speed is achieved at a price, almost every answer is wrong! Not many people are aware of this property. It is not due to difficulties in approximation. It is due to an iron law: all numbers used in a computer shall have a fixed number of digits. More precisely, the numbers we are talking about are usually floating point numbers that have two parts, a fraction and an exponent. It is the fractional parts with which we are concerned here. The exponents are also constrained and this can cause a condition of overflow or underflow (the numbers become too big or too small and cannot be represented in the machine) which is vexing but not our concern in this essay. To get a feel for the constraint on the fractional parts, consider them as restricted to 16 digits (this figure will vary depending on the computer in use). In Wilkinson's first machine the figure was 9 digits. That seems generous enough for practical purposes, even too generous, and for many calculations it is overkill. Nevertheless the product of two 16 digit fractions needs 32 digits to represent it and the computer always throws away the last (least significant) 16 of them. The error, more precisely the relative error, is miniscule. For calculations involving physical quantities, such as pressure or velocity, the given precision is usually adequate but in applications such as astronomy it may not be. The name for these miniscule errors is, appropriately, <a href=""https://en.wikipedia.org/wiki/Round-off_error"">roundoff error</a>, the computer “rounds off” the results of all arithmetic operations. The study of the effect of these errors is called roundoff error analysis.</p>
<p>A dramatic demonstration of the catastrophic effect of roundoff error can be obtained with a simple hand held calculator. Enter a positive integer between 2 and 9 (say 5). Press the square root key a large number of times (20 to 40 depending on the sophistication of the calculator) and then press the key for squaring an equal number of times. The original integer would be returned in exact arithmetic but the calculator will usually (depending on how many decimal digits it retains after each calculation) return either1or an approximation to 5 (e.g., 4.9998) instead. The differences are due to accumulating roundoff error. No subtractions are involved and each arithmetic error is tiny in a relative sense, but they accumulate if enough operations are performed.</p>
<p>Wilkinson pioneered, but did not invent, successful error analyses of all the matrix algorithms of his day. More on that topic later. Before leaving this description of round off error it should be stressed that in most of scientific and engineering computations roundoff error is only one, and a minor one at that, of the sources of error in the output. Other types of error are data uncertainty, approximation error, and discretation error when dealing with derivatives. However matrix computations are distinguished by the fact that for the so-called direct methods roundoff error is the only source of error.</p>
<p>Another striking feature of matrix computations is that it deals with objects that are not natural physical quantities such as pressure or temperature. Determinants and polynomials are wild. It was only gradually appreciated that the value of a polynomial of degree 1000 or more overflows in the computer unless the evaluation point is very close to a zero of the polynomial. Numbers with a large range of exponent occur very easily.</p>
<p>With that background to computers and to roundoff error let us return to the state of the pioneers in the use of these new devices. Long before the computer age scientists had approximated derivatives by difference quotients. After all a derivative is just the limit of a sequence of divided differences, (f(<em>x</em>+<em>h</em>) - f(<em>x</em>))/((<em>x</em>+<em>h</em>) – <em>x</em>), for small <em>h</em>. These details are not important but the great reward for making these approximations is that differential equations (difficult) turn into difference equations and these are disguised forms of algebraic equations. When the original differential equation is linear these difference equations turn into systems of linear algebraic equations (which are usually considered easy). In modern language a set of linear algebraic equations in unknowns called <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ... is called a matrix equation and written <em>Ax</em> = <em>b</em>, where the vector <em>b</em> holds the right hand sides, the vector <em>x</em> holds the unknowns and <em>A</em> is a matrix. For our purposes a matrix is a rectangular array of numbers. Readers who have not been exposed to matrices should just tell themselves that they are an important notational invention allowing a mass of detail to be compressed in a few symbols. The purpose of this paragraph is to hint at how a host of diverse scientific and engineering problems boil down to solving a few standard matrix problems. Matrix computations are almost never of interest for their own sake—they usually occur as intermediate steps in a bigger process.</p>
<p>It was immediately apparent to all involved in building automatic digital computers that it if these machines could solve systems of equations with, say, hundreds of unknowns then engineering models could be made much more realistic with huge benefits to technology. Direct methods for solving <em>Ax</em> = <em>b</em> had been refined for humans with handheld computing devices since the 1930s and it did not seem too much of a challenge to automate these methods. The only cloud on the horizon was the threat of all those roundoff errors. Might they undermine the hopes inspired by the new devices? Mathematicians of the greatest power (e.g., John von Neumann in the U.S.A. and Alan Turing in England), thought hard about these problems. Remember that no human being had ever solved 50 equations in 50 unknowns by a direct method. Here was unknown territory. A very senior statistician, Harold T. Hotelling, came up with a very pessimistic analysis; it seemed that the errors could accumulate at an exponential rate. Turing was pessimistic. In the U.S.A. von Neumann and Herman H. Goldstine came up with a seminal analysis and a clever, but complicated solution. In technical terms they proposed solving the normal equations instead of <em>Ax</em> = <em>b</em>. It is not essential to know what the normal equations are, but for those who have been exposed to matrix theory they are <em>A'Ax</em> = <em>A'b</em> where <em>A'</em> is the transpose of <em>A</em>.</p>
<p>By this time Wilkinson was the chief assistant to Alan Turing at the National Physical Laboratory which was charged with building one of Britain's earliest automatic general purpose digital computer, the Pilot ACE (Automatic Computing Engine), and of course they were giving much thought to the problems it should grapple with initially. In fact Wilkinson designed (and built) the multiplication unit for the Pilot ACE. And experimented by writing numerical programs (in an attempt to see what problems presented themselves) for the machine even before it was built.</p>
<p>Wilkinson had one great advantage over von Neumann and Turing. He had been obliged to solve 18 equations in 18 unknowns, with a hand cranked mechanical calculator, during WWII. He had seen how amazingly accurate the direct method was. That arduous exercise helped Wilkinson think in a different way from these other experts in accounting for those troublesome roundoff errors: stop obsessing over how large the error in the final output might be and ask instead how little could the data be changed so that the output is exactly correct? This is the fruitful question.</p>
<p>Another task where Wilkinson made major contributions was the <a href=""https://en.wikipedia.org/wiki/Eigenvalue""><em>eigenvalue</em></a> problem. Here the matrix <em>A</em> is square (same number of rows as columns) and the task is to find those few (very special) values <em>e</em> so that <em>A</em> - <em>eI</em> is not invertible. Here <em>I</em> is the identity matrix whose special property is that <em>Iv</em> = <em>v</em> for all vectors <em>v</em>. Formally this problem is related to the every old problem of finding the zeros of a given polynomial. Wilkinson showed that it is most unwise to reduce the eigenvalue problem to the problem of finding the zeros of its characteristic polynomial despite the fact that this reduction is the natural approach of a mathematician. Once grasped the reason is simple; the zeros of a polynomial are, almost always, extremely sensitive to tiny changes in the coefficients of the polynomial. Indeed a big effect of the arrival of automatic computing devices on mathematics, and engineering, was to boost the importance of <a href=""https://en.wikipedia.org/wiki/Perturbation_theory"">Perturbation Theory</a> to an essential feature of the field of Matrix Computations. The invention of algorithms to solve the matrix eigenvalue problem is an interesting topic but a bit too technical for this essay. Wilkinson guided its development in its first three decades (1950 - 1980). A key idea is to change the given matrix, to make it closer to triangular, without changing the eigenvalues. That requirement severely constrains the changes that can be made to the given matrix.</p>
<p>The great rivals to direct methods for solving <em>Ax</em>=<em>b</em> are iterative methods. These methods, developed with great success by Richard Southwell during WWI, do not aim to get the exact answer, instead they aim to improve an approximate solution at each step. The process continues until the approximation <em>z</em> is deemed satisfactory because its residual <em>b</em> - <em>Az</em> is small enough for the user's needs. Often these needs were modest and 3 correct decimal digits would suffice. So the experts were inclined to put their money on these iterative methods. The serious flaw in iterative methods is that they were not applicable to the general case; they required special properties of the matrix <em>A</em>.</p>
<p>The 1950s were the time of exploration. Programming was an ordeal until compilers came along late in the decade. However Wilkinson was able to try various methods for various matrix problems on the Pilot ACE and learn very valuable lessons.</p>
<p>Without too much distortion we could terminate this essay by saying that Wilkinson won the Turing prize for showing that the fears of the experts were unfounded, for understanding precisely the role of roundoff error in matrix computations, and for showing a way to make it all look rather easy. His tool was the so-called <a href=""https://en.wikipedia.org/wiki/Error_analysis"">backward error analysis</a>. Instead of concentrating on the size of the error, find the problem which the computed solution solved exactly! For the <em>Ax</em> = <em>b</em> problem let the computer output be <em>z</em>. Look for a small matrix <em>E</em> and a small vector <em>e</em> such that (<em>A</em> + <em>E</em>)<em>z</em> = <em>b</em> + <em>e</em>. If the bounds on <em>E</em> and <em>e</em> are really tiny should we not be satisfied? After all the entries in <em>A </em>and <em>b</em> may well be uncertain. If the error is still large then the problem itself, i.e. the pair (<em>A</em>, <em>b</em>), must be close to being <a href=""https://en.wikipedia.org/wiki/Well-posed_problem"">illposed</a>.</p>
<p>In fact Wilkinson did a great deal more than has been indicated above. He appreciated that there were often subtle but important differences in the way that a method is implemented. For example the order in which the squares of the entries of a vector are added up (top down or bottom up) can make a difference to the computed norm of the vector.</p>
<p>Here is a very simple example of an issue which is of no importance whatsoever in exact arithmetic but can make a significant difference when a computer is used: for numbers <em>a</em> and <em>b</em>, <em>a</em><sup>2</sup> – <em>b</em><sup>2</sup> = (<em>a</em> - b)(<em>a</em> + <em>b</em>) in exact arithmetic but not in the computer. For example, if <em>a</em> and <em>b</em> are very close in value then, a computer performing the operation (<em>a</em> – <em>b</em>) might calculate the result as zero (because the computer can only store a limited number of digits for each number). This would result in the right hand side of the above equation being (0)(<em>a </em>+ <em>b</em>) which is always zero. On the other hand, first calculating <em>a</em><sup>2</sup> and then <em>b</em><sup>2</sup>, then doing the subtraction, might well result in a non-zero value. Which form should one use when programming?</p>
<p>By the time he retired, Jim had been given the rank of Special Merit Chief Scientific Officer within the British Civil Service, a category which is very rarely used and allows one to continue with their research without having to worry about other duties.</p>
<p>In addition to his technical expertise Wilkinson was an excellent expositor with an informal style.</p>
<p>In 1945 Jim married Heather Nora Ware (also a mathematician) and they had a son and a daughter.</p>
<p align=""right""><span class=""callout"">Author: Beresford Neill Parlett</span></p>
<p>&nbsp;</p>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/wilkinson_0671216.cfm""><img src=""/images/lg_aw/0671216.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""J. H. Wilkinson ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/wilkinson_0671216.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>September 27, 1919, Strood, England</p>
<h6><span class=""label"">DEATH:</span></h6>
<p>October 5, 1986, Teddington, England</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>Sir Joseph Williamson’s Mathematical School, Rochester, England; Trinity College Cambridge (First Class Honors, 1939)</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>British Government Ministry of Supply (1940–1946); National Physical Laboratory (1946–retirement in 1980); he also held several visiting positions at major American universities</p>
<h6><span class=""label"">HONORS AND AWARDS:</span></h6>
<p>DSc, Cambridge (1963); Fellow of the Royal Society (1969); SIAM John von Neumann Lecturer (1970); ACM Alan M. Turing Award (1970); American Mathematical Society Chauvenet Prize (1987). The&nbsp;J. H. Wilkinson Prize for Numerical Software is named in his honor.&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81341498480","James Hardy (""Jim"") Wilkinson","<li class=""bibliography""><a href=""/bib/wilkinson_0671216.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283925&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/wilkinson_0671216.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/wilkinson_0671216.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179405-682","https://amturing.acm.org/award_winners/hopcroft_1053917.cfm","With Robert E Tarjan, for fundamental achievements in the design and analysis of algorithms and data structures.","<p>
<strong>John Hopcroft was born into a working class family on October 7, 1939 in Seattle Washington.</strong> His father was a British veteran of the First World War who moved to Canada because he was unable to find employment in Britain. He eventually worked his way to the west coast and finally to Seattle, where he met and married John’s mother and worked as a janitor.</p>
<p>
John grew up in Seattle and attended the local schools. Neither of his parents were high school graduates, but they instilled in him a love of learning and worked hard to ensure that he had the chance to obtain more education than either of them had. At an early age John was fascinated by technology – trains at first, and later mathematics and logic, particularly those aspects relating to electrical engineering.</p>
<p>
He claims that because of the lack of family experience with higher education, it never occurred to him to look at other than the local Seattle University. He graduated with a BS in Electrical Engineering in 1961. He was a fine student and was quickly admitted to graduate studies at Stanford University, where he received a MS in 1962 and a PhD in 1964, both in Electrical Engineering.</p>
<p>
John’s fist academic job was as an Assistant Professor of Electrical Engineering at Princeton. He became a computer scientist almost by accident, when his department head asked him to teach a class in computing science. Since he had no experience in the subject, he had to ask what subjects should be included. The department head recommended a few papers, and John managed to create and teach this first course to six bright students who in turn motivated him. He taught that class for several years, but he says that first year was the most enjoyable because he and his few students were feeling their way into unfamiliar territory together.</p>
<p>
One of those first students was Jeff Ullman, who would become a major collaborator in his work. Jeff left Princeton to work at Bell Laboratories and teach an evening course at Columbia University. This was the era when there were few computer science texts, particularly for the theoretical aspects of the subject. Hopcroft and Ullman took John’s course notes and expanded them into one of the earliest and most influential books on the subject [<a href=""/bib/hopcroft_1053917.cfm#bib_1"">1</a>].&nbsp; This volume and its many translations educated an entire generation of computer scientists. Its successor [<a href=""/bib/hopcroft_1053917.cfm#bib_3"">3</a>] is still in use today.</p>
<p>
John’s first computer science research efforts were in an area that became known as <a href=""https://en.wikipedia.org/wiki/Formal_language_theory"" target=""_blank"">formal language theory</a>. Linguists had developed formal grammars for the study of human languages, and similar tools could be used for the developing high level programming languages. The earliest compilers had not been based on a theoretical foundation like that; they were simply a collection of ad-hoc routines that analyzed the statements in a high level language and created the equivalent set of instructions in computer machine code. This process sufficed for the earliest compliers but quickly became unworkable as languages increased in complexity. John made fundamental advances in formal grammars before turning his attention to the study of algorithms.</p>
<p>
In the early days of computers, algorithms tended to be compared based on total running time, which depends on the speed of the computer, the programming efficiency, the constant overhead of the algorithm, and the growth rate of the running time as the input gets larger. John recognized that as computers became faster and worked on larger problems, the most important part to understand was the last: the growth rate of the running time. His focus on this “asymptotic complexity” set the direction for the new field of analysis of algorithms.</p>
<p>
Much of this work dealt with the algorithms used to manipulate <a href=""https://en.wikipedia.org/wiki/Graph_theory"" target=""_blank"">graphs</a>, which are made from a set of points (vertices) connected by lines (edges). While this may appear to be a very academic and abstract subject, many practical problems can be described using graphs. Thus any improvements in the general algorithms used to manipulate graphs can provide practical improvements to real life problems. Such problems span the range from linguistics (for grammar and syntactic analysis), to chemistry (for modelling of atoms and molecules to compute their properties), to network analysis (for determining the flow of traffic in the World Wide Web or in highways).</p>
<p>
Hopcroft’s work on graph algorithms was not just theoretical analysis, it also included synthesis. He explored efficient structures for storing data in a computer, and created efficient algorithms for solving the problems they could represent. This work, new and exciting at the time, is now part of the standard computer science curriculum.</p>
<p>
His work on formal languages and the analysis of algorithms has made John Hopcroft one of the handful of pioneering computer scientists who put the discipline on a firm theoretical foundation. His co-authored texts on formal languages and their relation to automata [<a href=""/bib/hopcroft_1053917.cfm#bib_1"">1</a>] and on the design and analysis of algorithms [<a href=""/bib/hopcroft_1053917.cfm#bib_2"">2</a>] became the standards for a generation of computer scientists.</p>
<p>
His current work, which again uses graph models, explores ways to track social networks and build the search engines of the future.</p>
<p>
John is not only a respected researcher, but also an inspiring teacher. Each year the top 20 graduates at Cornell are asked to name the professor that had the most influence on their education. John has been chosen twice, a record of which he is justly proud.</p>
<p>
He believes that students should be required to take fewer specialist courses. They should be allowed acquire a broad education and have more free time to learn things on their own. He admits that his success in changing university curricula requirements has been limited, but he’s still trying.</p>
<p>
Hopcroft has been an influential and inspiring PhD advisor to at least 34 students since 1967. His advisees learned how to do research from him, but they also absorbed his sense of discrimination, his standard of excellence, and his commitment to community service. Many of them now serve in influential positions in academia and industry. Among his PhD students are the president of an Israeli university, a MacArthur Fellow, a vice president of a computing research firm, and many chairs of academic departments.</p>
<p>
Throughout his career, Hopcroft has helped the field by serving on national and international committees. The respect for his views and his work are evident from the many honors he has received.</p>","<div class=""featured-photo"">
<a href=""/award_winners/hopcroft_1053917.cfm""><img src=""/images/lg_aw/1053917.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""John E Hopcroft""></a>
</div>
<h6><span class=""label"">BIRTH:</span></h6>
<p>October 7, 1939, Seattle Washington, USA</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>B.S., Seattle University (1961, Electrical Engineering); M.S., Stanford University (1962 Electrical Engineering); Ph.D., Stanford University (1964, Electrical Engineering).</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>Assistant Professor, Princeton University (1964-1967); Cornell University (Associate Professor, 1967-1971; Professor, 1972-85; Joseph C. Ford Professor, College of Engineering, 1985-1994; Chair, Dept. of Computer Science, 1987-92; Associate Dean for College Affairs, 1992-1993; Joseph Silbert Dean, College of Engineering, 1994-2001; Professor, Department of Computer Science, 2001-2004; IBM Professor of Engineering and Applied Mathematics (2004 and onwards).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>National Science Foundation Graduate Fellow, 1961-1964; Association for Computing Machinery A.M. Turing Award (shared with R.E. Tarjan), 1986; Fellow of the American Academy of Arts and Sciences, 1987; Fellow of the American Association for the Advancement of Science, 1987; Fellow of the Institute of Electrical and Electronics Engineers (IEEE), 1987; Member of the National Academy of Engineering, 1989; Fellow of the Association for Computing Machinery, 1994; Life Fellow of IEEE, 2004; Cornell College of Engineering Michael Tien ’72 Excellence in Teaching Award, 2004; IEEE Harry Goode Memorial Award, 2005; Assoc. of Computer Science Undergraduates Faculty of the Year Award, 2006; CRA Distinguished Service Award, 2007; ACM Karl V. Karlstrom Outstanding Educator Award, 2008;&nbsp;<span style=""line-height: 20.8px;"">Honorary professorship, Beijing Institute of Technology, 2008;</span><span style=""line-height: 1.3;"">&nbsp;Fellow of Society for Industrial and Applied Mathematics, 2009; Member of the National Academy of Sciences, 2009; Einstein professor Chinese Academy of Sciences, 2010; IEEE von Neumann Medal, 2010, 2011; Ralph S. Watts 72 Excellence in Teaching Award, 2011; Recognized by the </span>Societe<span style=""line-height: 1.3;""> </span>Mathematique<span style=""line-height: 1.3;""> de </span>Tunisie<span style=""line-height: 1.3;""> (</span>SMT<span style=""line-height: 1.3;"">) for “notable services and outstanding contributions in the application of mathematical theories in theoretical computer science”, March 2010; Honorary professorship, </span>Yunnan<span style=""line-height: 1.3;""> University, 2010; Honorary professorship, </span>Chongqing<span style=""line-height: 1.3;""> University, 2011; Honorary professorship, </span>Jiao<span style=""line-height: 1.3;""> Tong University, 2011;&nbsp;</span><span style=""line-height: 20.8px;"">Honorary degrees from: Seattle University; University of Sydney; Saint Petersburg State University of Information Technologies, Mechanics &amp; Optics; HKUST; Beijing Institute of Technology; and National College of Ireland.</span></p>","","https://dl.acm.org/author_page.cfm?id=81100008350","John E Hopcroft","<li class=""bibliography""><a href=""/bib/hopcroft_1053917.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283943&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/hopcroft_1053917.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/hopcroft_1053917.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/hopcroft_1053917.cfm""><span></span>Video Interview</a></li>"
"1573179229-671","https://amturing.acm.org/award_winners/karp_3256708.cfm","For his continuing contributions to the theory of algorithms including the
development of efficient algorithms for network flow and other combinatorial optimization problems,
the identification of polynomial-time computability with the intuitive notion of algorithmic efficiency,
and, most notably, contributions to the theory of NP-completeness. Karp introduced the now standard
methodology for proving problems to be NP-complete which has led to the identification of many
theoretical and practical problems as being computationally difficult.","<p>
<strong>Richard Karp was born in Boston, Massachusetts on January 3, 1935.</strong> He attended Boston Latin School, and received the A.B, S.M. and Ph.D. degrees in Mathematics and Applied Mathematics from Harvard University in 1955, 1956 and 1959.</p>
<p>
Karp then joined the research staff of the IBM Watson Research Center in Yorktown Heights, NY, where he worked until 1968. During that time, Karp did foundational work on models of parallel computation, which involves the simultaneous use of multiple coordinated computers to solve a single problem. Roughly twenty years later, he returned to the study of parallel computation and continues to work in that area.</p>
<p>
Motivated by the desire to teach and ""to be where the action is,"" in 1968 Karp moved to the University of California, Berkeley, with appointments in Computer Science and in Operations Research. His dedication to teaching won him the UC Berkeley Distinguished Teaching Award in 1986.</p>
<p>
Karp's initial research at Berkeley focused on <a href=""https://en.wikipedia.org/wiki/Heuristic"" target=""_blank"">heuristic algorithms</a> for hard combinatorial problems, particularly for the notoriously difficult <a href=""https://en.wikipedia.org/wiki/Travelling_salesman_problem"" target=""_blank""><em>Traveling Salesmen Problem</em></a> (TSP). Like many combinatorial problems, there was no efficient TSP algorithm that could be proved correct for all input, and whose worst-case running time is bounded by a polynomial function of the input size. Also lacking was an explanation of why an efficient algorithm had been so elusive. In one of the most significant paper of his career, Karp's 1972 <a href=""https://www.cs.berkeley.edu/~luca/cs172/karp.pdf"" target=""_blank""><em>Reducibility Among Combinatorial Problems</em></a> provided a convincing explanation for the elusiveness of an efficient solution. In that seminal paper, Karp expanded on the concept (first introduced by Stephen Cook in 1971, and independently by Leonid Levin) of NP-completeness and proved that if any of twenty-one well-known difficult problems (including TSP) could be solved by an efficient algorithm, then all of the problems could be efficiently solved.&nbsp; <a href=""https://en.wikipedia.org/wiki/NP-complete"" target=""_blank"">NP-completeness</a> shows, in an important sense, that these 21 problems are all equivalent. The collective failure by many talented people to find efficient algorithms for specific NP-complete problems strongly suggests that no efficient algorithm exists for any of them. Importantly, the paper established a framework for adding additional problems to the class of NP-complete problems, which has since grown to include many thousands. Today, only one or two classic combinatorial problems remain unclassified as to whether they are NP-complete or not. While most of the classified classic problems are NP-complete, several have been shown to be efficiently solvable.</p>
<p>
Karp's work on NP-completeness substantially motivated the discussion&nbsp; of the famous unsolved <a href=""https://en.wikipedia.org/wiki/P_versus_NP_problem"" target=""_blank""><em>P = NP </em>question</a> and its further examination by mathematicians and computer scientists. In essence, P = NP asks whether finding a solution to a problem is inherently more difficult than verifying that a proposed solution is correct. Not only is the problem considered the most important open question in theoretical computer science, it is also one of the most important open questions in mathematics.</p>
<p>
The year after his paper on NP-completeness, Karp co-authored with Jack Edmonds and <a href=""/award_winners/hopcroft_1053917.cfm"">John Hopcroft</a> (himself a Turing Award recipient) two very significant papers on efficient algorithms for network flow and <a href=""https://en.wikipedia.org/wiki/Bipartite_graph"" target=""_blank"">bipartite graph matching</a>. The network flow problem is to compute the maximum steady-state amount of material (for example, liquids in a pipe or bits in a communication network) that can be transported from a source to a destination in a network, where each edge (pipe, wire, road etc.) in the network can have a different but bounded capacity. This is one of the most widely studied problems in optimization and algorithmic computer science, and is not limited to physical networks as the same problems arise in, for example, biological cycles. The bipartite matching problem is a particularly useful special case of network flow that has a huge number of non-physical applications. Karp and Hopcroft showed in 1973 that the bipartite matching problem can be solved more efficiently than the general network flow problem [<a href=""/bib/karp_3256708.cfm#link_4"">4</a>] .</p>
<p>
Another major theme in Karp's research has been the use of probability in both the design and analysis of efficient algorithms. Karp's interest in the probabilistic analysis of algorithms began in the 1980s, when he examined questions of the average, rather than the worst-case, running times of particular algorithms. In probabilistic analysis, the algorithm is fixed and <a href=""https://en.wikipedia.org/wiki/Deterministic_algorithm"" target=""_blank"">deterministic</a>, but its input is assumed to be drawn from a space universe of possible inputs according to a well-specified probabilistic model. The analysis finds the expected running time (or other characteristic) for that distribution of inputs. Karp later became active in the design of <a href=""https://en.wikipedia.org/wiki/Randomized_algorithm"" target=""_blank"">randomized algorithms</a>, where the algorithm itself, rather than the input to the algorithm, has a random component. Surprisingly, randomizing the behavior of an algorithm can often significantly reduce its expected running time for any input.</p>
<p>
Karp's most recent research has been in computational biology. This work began in the 1990s, as the field began to grow rapidly under the influence of the Human Genome Project. Karp first examined physical mapping of genes, devising algorithms to help determine the physical location of genes in a genome given the limited indirect experimental information about gene location. Such algorithms were especially important before affordable full DNA sequencing became available. From that beginning, Karp has examined a wide range of algorithmic issues in biology, such as:</p>
<ol>
<li>
determining protein interactions</li>
<li>
designing and analyzing methods for gene mapping&nbsp;under different data collection technologies</li>
<li>
designing and analyzing methods for problems that arise in population genetics and family pedigree analysis</li>
<li>
finding repeating patterns and structures in gene sequences</li>
<li>
constructing and analyzing networks that represent biological interactions, such as protein-protein contact, gene regulation, and metabolic pathways.</li>
</ol>
<p>
Much of Karp's work in computational biology uses techniques from combinatorial optimization, but more recently his work has also involved <a href=""https://en.wikipedia.org/wiki/Stochastic"" target=""_blank"">stochastic approaches</a> from the field of machine learning. A new direction for Karp, this recent work on machine learning is related to his continuing interest in the use of probability in the design of algorithms and in the analysis of algorithmic behavior.</p>
<p>
In 1995 Karp left Berkeley for the Department of Computer Science at the University of Washington, where he was also an Adjunct Professor of Molecular Biotechnology. He returned to Berkeley in 1999 with appointments in Computer Science, Mathematics, and Bioengineering. In addition, Karp is a Senior Research Scientist at the International Computer Science Institute in Berkeley, and was a principal in the creation of the Institute. Karp is currently a University Professor at the University of California, a prestigious title that has been awarded only thirty-eight times. Other major awards include election to the National Academy of Sciences in 1980, election to the National Academy of Engineering in 1992, the U.S. National Medal of Science in 1996, and the Kyoto Prize in 2008.</p>
<p>
During his years at Berkeley and Washington, Karp mentored almost forty Ph.D. students. In addition to his professional accomplishments, Karp is an avid reader and chess player.</p>
<p style=""text-align: right;"">
<br>
<span class=""callout"">B. Simons and D. Gusfield</span><br>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/karp_3256708.cfm""><img src=""/images/lg_aw/3256708.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Richard Karp""></a>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>January 3, 1935, Boston Massachusetts</p>
<h6 class=""label"">EDUCATION:</h6>
<p>AB, Harvard 1955; SM Harvard 1956; Ph.D., Harvard 1959, Applied Mathematics; and eight honorary degrees.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>1959-1968 Research Staff Member, IBM Watson Research Center, Yorktown Heights, New York; 1964-1965 Visiting Associate Professor, Electrical Engineering, University of Michigan; 1968-1994 Professor of Computer Science and of Industrial Engineering and Operations Research, University of California, Berkeley; 1973-1975 Associate Chairman for Computer Science, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; 1980-1994 Professor of Mathematics, University of California, Berkeley; 1985-1986 Co-chair, Program in Computational Complexity, Mathematical Sciences Research Institute, Berkeley; 1988-1995 Research Scientist, International Computer Science Institute, Berkeley, California; 1995-1999 Professor of Computer Science and Adjunct Professor of Molecular Biotechnology, University of Washington; 1999- University Professor, University of California, Berkeley, Computer Science, Mathematics, and Bioengineering; 1999- Research Scientist, International Computer Science Institute, Berkeley, California; 2001-2003 Chair, Board of Governors, Institute for Mathematics and Its Applications; 2001-2004 Founding Chair, Section 34, National Academy of Sciences.</p>
<h6 class=""label""><br>
HONORS AND AWARDS:</h6>
<p>Lanchester Prize in Operations Research, 1977 (co-winner); Fulkerson Prize in Discrete Mathematics, 1979; Miller Research Professor, Berkeley, 1980-81; Faculty Research Lecturer, Berkeley, 1981-82; Einstein Fellowship and Lady Davis Fellowship, Technion, Haifa, Israel, 1983; ACM Turing Award, 1985; Distinguished Teaching Award, University of California at Berkeley, 1986; Doctor of Science (honoris causa), University of Pennsylvania, 1986; John von Neumann Lecturer, SIAM, 1987; Doctor of Science (honoris causa), Technion, 1989; Class of 1939 Chair, University of California at Berkeley, 1989; ORSA/TIMS von Neumann Theory Prize, 1990; Doctor of Science (honoris causa), University of Massachusetts, 1990; Doctor of Humane Letters (honoris causa), Georgetown University, 1992; Babbage Prize, 9th International Parallel Processing Symposium 1995; National Medal of Science, 1996; Centennial Medal, Harvard University, 1997; Kyoto Prize, 2008; Harvey Prize; Benjamin Franklin Medal.<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81361598854","Richard (""Dick"") Manning Karp","<li class=""bibliography""><a href=""/bib/karp_3256708.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283942&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/karp_3256708.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/interviews/karp_3256708.cfm""><span></span>Video Interview</a></li>"
"1573178999-654","https://amturing.acm.org/award_winners/berners-lee_8087960.cfm","For inventing the World Wide Web, the first web browser, and the fundamental protocols and algorithms allowing the Web to scale.","<div class=""awards-winners__citation-text"" style=""color: rgb(34, 34, 34); line-height: 1.2; font-family: Verdana, Geneva, Tahoma, sans-serif; font-size: 0.87rem; margin-bottom: 1.25rem; box-sizing: border-box; background-color: rgb(255, 255, 255);"">
<p>Tim Berners-Lee grew up in London. Both of his parents (Mary Lee Woods and Conway Berners-Lee) were mathematicians, who had worked on the Ferranti Mark 1, a pioneering effort to commercialize the early Manchester computer. He inherited their interests, playing with electronics as a boy, but choosing physics for his university studies. After earning a degree from Queen’s College, Oxford in 1976 he worked on programming problems at several companies, before joining the European physics lab CERN in 1984. His initial job was in the data acquisition and control group, working to capture and process experimental data.</p>
<p><strong>Inventing the Web</strong></p>
<p>CERN’s business was particle smashing, not computer science, but its computing needs were formidable and it employed a large technical staff. Its massive internal network was connected to the Internet. In March 1989 Berners-Lee began to circulate a document headed “Information Management: A Proposal,” which proposed an Internet-based hypertext publishing system. This, he argued, would help CERN manage the huge collections of documents, code, and reports produced by its thousands of workers, many of them temporary visitors.</p>
<p>Berners-Lee later said that he had been dreaming of a networked hypertext system since a short spell consulting at CERN in 1980. Ted Nelson, who coined the phrase “hypertext” back in the 1960s, had imagined an online platform to replace conventional publishers. Authors could create links between documents, and readers would follow them from one document to another. By the late-1980s hypertext was flourishing as a research area, but in practice was used only in closed systems, such as the Microsoft Windows help system and the Macintosh Hypercard electronic document platform.</p>
<p>Mike Sendall, Berners-Lee’s manager, wrote “vague but exciting” on his copy of the proposal. In May 1990, he authorized Berners-Lee to spend some time on his idea, justifying this as a test of the widely hyped NeXT workstation. This was a high-end personal computer with a novel Unix-based operating system that optimized the rapid implementation of graphical applications. Berners-Lee spent the first few months working out specifications attempting to interest existing hypertext software companies in his ideas. By October 1990, he had begun to code prototype Web browser and server software, finishing in December. On 6, August, 1991, after tests and further development inside CERN, he used the Internet to announce the new “World Wide Web” and to distribute the new software.</p>
<p><strong>Elements of the Web</strong></p>
<p>The World Wide Web was ambitious in some ways, as its name reflects, but cautious in others. Berners-Lee’s initial support from CERN did not consist of much more than a temporary release from his other duties. So he leveraged existing technologies and standards everywhere in the design of the WWW. He remembers CERN as “chronically short of manpower for the huge challenges it had taken on.” There was no team of staff coders standing by to implement any grand plans he might come up with.</p>
<p>The Web, like most of the Internet during this era, was intimately tied in with the Unix operating system (for which Dennis M. Ritchie and Ken Thompson won the 1983 Turing Award). For example, the first Web server (and most since) have run as background processes on Unix-derived operating systems. URLs use Unix conventions to specify file paths within a website. To develop his prototype software, Berners-Lee used the NeXT workstation. More fundamentally, Berners-Lee’s whole approach reflected the distinctive Unix philosophy of building new system capabilities by recombining existing tools.</p>
<p>The Web also followed the Internet philosophy of achieving compatibility through communications protocols rather than standard code, hardware, or operating systems. His specifications for the new system led to three new Internet standards.</p>
<p>Web pages displayed text.&nbsp;<strong>HTML</strong>&nbsp;(Hyper Text Markup Language) specified the way text for a Web page should be tagged, for example as a hyperlink, ordinary paragraph, or level 2 heading. It was an application of the existing SGML (Standard Generalized Markup Language) markup language definition standard.</p>
<p><strong>HTTP</strong>&nbsp;(Hyper Text Transfer Protocol) specified the interactions through which Web browsers could request and receive HTML pages from Web servers. HTML was, in computer science terms, stateless – users did not log into websites and each request for a Web page or other file was treated separately. This made it a file transfer protocol, which was easy to design and implement because existing Internet standards and software, most importantly TCP/IP (for which Vinton Cerf and Robert E. Kahn won the 2004 Turing award), provided the infrastructure needed to pipe data across the network from one program to another. Berners-Lee later called this use of Internet protocols “politically incorrect” as European officials at the time were supporting a transition to the rival ISO network protocols. A few years later it was the success of the Web that put the final nail in their coffin.</p>
<p>Consider a Web address like&nbsp;<a href=""https://amturing.acm.org/award_winners/berners-lee_8087960.cfm"" target=""_blank"">http://amturing.acm.org/award_winners/berners-lee_8087960.cfm</a>.&nbsp;This is a&nbsp;<strong>URL</strong>&nbsp;or Uniform Resource Locator (Berners-Lee originally called this a Universal Resource Identifier). The “<a href=""https://amturing.acm.org/"" target=""_blank"">amturing.acm.org</a>” part identified the computer where the resource was found. This was nothing new – Internet sites had been using this Domain Name System since the mid-1980s. The novelty was the “http://” which told Web browsers, and users, to expect a Web server. Information after the first single “/” identified which page on the host computer was being requested. Berners-Lee also specified URL formats for existing Internet resources, including file servers, gopher servers (an earlier kind of Internet hypertext system), and telnet hosts for terminal connections. In 1994, Berners-Lee wrote that “The fact that it is easy to address an object anywhere in the Internet is essential for the system to scale, and for the information space to be independent of the network and server topology.”</p>
<p>The URL was the simplest of the three inventions, but was crucial to the early spread of the Web because it solved the “chicken and egg” problem facing any new communications system. Why set up a Web page when almost nobody has a Web browser? Why run a Web browser when almost nobody has set up a Web server to visit? The URL system made Web browsers a convenient way to access existing resources, cataloged on Web pages. In 1992, the Whole Internet Catalog and User’s Guide stated that “the World Wide Web hasn’t really been exploited yet… Hypertext is used primarily as a way of organizing resources that already exist.”</p>
<p><strong>The Web Takes Off</strong></p>
<p>CERN found some resources to support the further development of the Web – about 20 person years of work in total, mostly from interns. More importantly, it made it clear that others were free to use the new standards and prototype code to develop new and better software. Robert Cailliau, of the Office Computing Systems group, played an important role as a champion of the project within CERN. In 1991 CERN produced a simple text-based browser that could easily be accessed over the Internet and a Macintosh browser, essential to the initial spread of the Web as NeXT workstations remained very rare.</p>
<p>Over the next few years others implemented faster and more robust browsers with new features such as graphics in pages, browser history, and forward and back buttons. Mosaic, released in 1993 by the National Center for Supercomputer Applications of the University of Illinois, brought the Web to millions of users. In April, 1994 CERN, which was still trying to maintain a comprehensive list of Web servers, cataloged 829 in its “Geographical Registry.”</p>
<p>Berners-Lee later attributed his success largely to “being in the right place at the right time.” He succeeded where larger and better funded teams had failed, setting the foundation for a global hypertext system that quickly became a universal infrastructure for online communication and the foundation for many new industries. Yet the ACM’s 1991 Hypertext conference had rejected Berners-Lee’s paper describing the World Wide Web. From a research viewpoint, the Web seemed to sidestep many thorny research problems related to capabilities that Ted Nelson thought essential for a public hypertext publication system. If a Web page was moved, then links pointing to it stopped working. If the target page was changed, then it might no longer hold the content the link promised. Links went only one way – one couldn’t see which other pages linked to a document. There was no central, searchable index of websites and their content. Neither did the Web itself provide any way for publishers to get paid when people read their work.</p>
<p>Berners-Lee had only a few months at his disposal, which may have been a hidden blessing: Nelson worked for decades without coming close to finishing his system. Rather than attack intractable problems, Berners-Lee used proven technologies as the building blocks of a system intended to be powerful and immediately useful rather than perfect.</p>
<p>The Web’s reliance on existing technologies was appealing to early users and eased deployment – setting up a Web server on a computer already connected to the Internet just involved downloading and installing a small program. This technological minimalism made the Web easy to scale, with no indexing system or central database to overload. After the Web took off, whole new industries emerged to fill in some of the missing capabilities needed for large scale and commercial use, eventually leading, for example, to the rise of Google as the dominant provider of Internet search.</p>
<p>One crucial feature that Berners-Lee built into his prototype Web software was left out of its successors. His browser allowed users to edit pages, and save the changes back on the server. His 1994 article in&nbsp;<em>Communications of the ACM</em>&nbsp;noted that “The Web does not yet meet its design goal of being a pool of knowledge that is as easy to update as to read.” Editing capabilities were eventually added in other ways – first through separate HTML editing software, and later with the widespread adoption of content management systems where the software used to edit Web pages is itself accessed through a Web browser.</p>
<p><img alt="""" src=""/images/Tim Burners Lee ACM Turing winner image insert.jpg"" style=""width: 650px; height: 446px;""></p>
<p><em>A screenshot of Berners-Lee’s Web browser software running on his NeXT computer. Note the Edit menu to allow changes, and the Style menu which put decisions over fonts and other display details in the hands of the reader rather than Webpage creators. Since 2014 this computer has been exhibited at the Science Museum in London</em>.</p>
<p>Berners-Lee feels that his original design decisions have held up well, with one exception: the “//” in URLs which make addresses longer and harder to type without adding any additional information. “I have to say that now I regret that the syntax is so clumsy” he wrote in 2009.<a href=""https://www.w3.org/People/Berners-Lee/FAQ.html#etc""><sup>[1]</sup></a></p>
<p><strong>Standardizing the Web</strong></p>
<p>Mosaic’s successor, the commercial browser Netscape, was used by hundreds of millions and kickstarted the “.com” frenzy for new Internet stocks. By 2000 there were an estimated 17 million websites online, used for commercial transactions such as online shopping and banking as well as document display. In the process, HTML was quickly given many clunky and incompatible extensions so that Web pages could be coded for things like font styles and page layout rather than its original focus on document structure.</p>
<p>In 1994 Berners-Lee left CERN for a faculty job at MIT. This let him establish the World Wide Web Consortium (W3C), to standardize HTML and other, newer, elements of the Web. Berners-Lee had been frustrated in 1992 in an initial attempt to work with the Internet Engineering Task Force, the group that developed and standardized other Internet protocols. The consortium followed a different model, using corporate memberships to support the work of paid staff members. With its guidance the Web has remained open during its growth, so that users can choose their preferred Web browser while still accessing the full range of functionality found on modern websites. It also played a crucial role in adoption of the XML data description language. As of 2017, his primary appointment remains at MIT where he holds the Founders Chair in the MIT Computer Science and Artificial Intelligence Laboratory and continues to direct W3C.</p>
<p><strong>The Semantic Web</strong></p>
<p>Since the late 1990s Berners-Lee’s primary focus has been on trying to get Web publishers and technology companies to add a set of capabilities he called the “Semantic Web.” Berners-Lee defined his idea as follows: “The Semantic Web is an extension of the current Web in which information is given well-defined meaning, better enabling computers and people to work in cooperation.""</p>
<p>Document metadata was largely left off the original Web, in contrast to traditional online publishing systems, which made it hard for search engines to determine basic information such as the date on which an article was written or the person who wrote it. The Semantic Web initiative covered a hierarchy of technologies and standards that would let the creators of Web pages tag them to make their conceptual structure explicit, not just for information retrieval but also for machine reasoning.</p>
<p><strong>Legacy and Recognition</strong></p>
<p>The success of the Web drove a massive expansion in Internet access and infrastructure – indeed most Internet users of the late-1990s experienced the Internet primarily through the Web and did not clearly separate the two. Berners-Lee has been widely honored for this work, winning a remarkable array of international prizes. Sir Tim, as he been known since the Queen knighted him in 2004, has been recognized as one of the public faces of British science and technology. In 2012 he appeared with a NeXT computer during the elaborate opening ceremony of the London Olympic Games.</p>
<p>He has been increasingly willing to use this public influence to impact the ways in which governments and companies are shaping the Web. In 2009 he set up the World Wide Web Foundation, which lobbies for “digital equality” and produces rankings of Web freedom around the world. More recently, Berners-Lee has championed protection for personal data, criticized the increasing dominance of proprietary social media platforms, and bemoaned the prevalence of fake news online.</p>
<p style=""text-align: right;""><span style=""display: inline !important; float: none; background-color: transparent; color: rgb(26, 26, 31); font-family: Georgia,serif; font-size: 12.8px; font-style: italic; font-variant: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: right; text-decoration: none; text-indent: 0px; text-transform: none; -webkit-text-stroke-width: 0px; white-space: normal; word-spacing: 0px;"">Author: Thomas Haigh</span></p>
<p><a href=""https://mail.google.com/mail/u/0/?shva=1#m_-4016731291391036510__ftnref1"" name=""m_-4016731291391036510__ftn1"" style=""color: rgb(17, 85, 204); font-family: Calibri, sans-serif; font-size: 13.3333px; background-color: rgb(255, 255, 255);"" title=""""><span class=""m_-4016731291391036510MsoFootnoteReference"" style=""vertical-align: super;"">[1]</span></a><span style=""font-family: Calibri, sans-serif; font-size: 13.3333px;"">&nbsp;</span><a data-saferedirecturl=""https://www.google.com/url?hl=en&amp;q=https://www.w3.org/People/Berners-Lee/FAQ.html%23etc&amp;source=gmail&amp;ust=1507076656030000&amp;usg=AFQjCNF2IxUfwlxC_s_sEx4zkOhhk9VG1g"" href=""https://www.w3.org/People/Berners-Lee/FAQ.html#etc"" style=""color: rgb(17, 85, 204); font-family: Calibri, sans-serif; font-size: 13.3333px;"" target=""_blank"">https://www.w3.org/People/<wbr>Berners-Lee/FAQ.html#etc</a></p>
</div>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/berners-lee_8087960.cfm""><img src=""/images/lg_aw/8087960.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Sir Tim Berners-Lee""></a>
</div>
<p><span style=""color:#ff8c00;"">Birth</span>: 8 June, 1955.</p>
<p><span style=""color:#ff8c00;"">Education</span>: Bachelor’s degree in Physics (Queen’s College, Oxford, 1976).</p>
<p><span style=""color:#ff8c00;"">Experience</span>: Principal Engineer (Plessy, 1976-78), Programmer (D.G. Nash, 1978-80), Contractor (CERN, 1980), Technical manager (Image Computer Systems, 1981-1984), Technical staff (CERN, 1986-1994), Professor (MIT, 1994-Present. 3Com Founders Chair since 1999). Director &amp; Founder of: The World Wide Web Consortium (1994-present) and World Wide Web Foundation (2008-present). Secondary appointments as Professor at Southampton University (2004-present); Professor at Oxford University in Computer Science (2016-Present).</p>
<p><span style=""color:#ff8c00;"">Honors and Awards (selected)</span>: ACM Software System Award&nbsp;(1992); Distinguished Fellow of the British Computer Society (1995); Fellow of the Royal Society (2001), Fellow of the American Academy of Arts and Sciences (2001), Royal Society of Arts Albert Medal (2002), Japan Prize (2002), Computer History Museum Fellow (2003); Knight Commander of the Order of the British Empire (KBE); Order of Merit (2007); Draper Prize (2007); Fellow of the IEEE (2008); UNESCO Niels Bohr Gold Medal (2010), Queen Elizabeth II Prize for Engineering (2013); ACM A.M. Turing Award (2016).&nbsp;19 honorary doctorates as of 2014.</p>","","https://dl.acm.org/author_page.cfm?id=81100026375","Sir Tim Berners-Lee","<li class=""bibliography""><a href=""/bib/berners-lee_8087960.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/berners-lee_8087960.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/berners-lee_8087960.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/berners-lee_8087960.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179077-660","https://amturing.acm.org/award_winners/lamport_1205376.cfm","","<p style=""margin-left:27.0pt;""><em>If we could travel back in time to 1974, perhaps we would have found Leslie Lamport at his busy local neighborhood bakery, grappling with the following issue. The bakery had several cashiers, but if more than one person approached a single cashier at the same time, that cashier would try to talk to all of them at once and become confused. Lamport realized that there needed to be some way to guarantee that people approached cashiers one at a time. </em><em>This problem reminded Lamport of an issue which has been posed in an earlier article by computer scientist Edsger Dijkstra on another mundane issue: how to share dinner utensils around a dining table. One of the coordination challenges was to guarantee that each utensil was used by at most one diner at a time, which came to be generalized as the <strong>mutual exclusion</strong> problem, exactly the challenge Lamport faced at the bakery. </em></p>
<p style=""margin-left:27.0pt;""><em>One morning in 1974, an idea came to Lamport on how the bakery customers could solve mutual exclusion among themselves, without relying on the bakery for help. It worked roughly like this: people choose numbers when they enter the bakery, and then get served at the cashier according to their number ordering. To choose a number, a customer asks for the number of everyone around her and chooses a number higher than all the others. </em></p>
<p style=""margin-left:27.0pt;""><em>This simple idea became an elegant algorithm for solving the mutual exclusion problem without requiring any lower-level indivisible operations. It also was a rich source of future ideas, </em><em>since many issues had to be worked out. For example, some bakery customers took a long time to check other numbers, and meanwhile more customers arrived and selected additional numbers. Another time, the manager of the bakery wanted to get a snapshot of all the customer numbers in order to prepare enough pastries. </em><em>Lamport later said ""For a couple of years after my discovery of the bakery algorithm, everything I learned about concurrency came from studying it.""&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_1"">1</a>] </em></p>
<p style=""margin-left:27.0pt;""><em>The Bakery Algorithm and Lamport's other pioneering works -- many with amusing names and associated parables -- have become pillars of computer science. His collection forms the foundation of broad areas in concurrency, and has influenced the specification, development, and verification of concurrent systems. </em></p>
<p>After graduating with a PhD in math from Brandeis University in 1972, Lamport worked as a computer scientist at Massachusetts Computer Associates from 1970 to 1977, at SRI International from 1977 to 1985, and at Digital Equipment Corporation Systems Research Center (later owned by Compaq) from 1985 to 2001. In 2001 he joined Microsoft Research in Mountain View, California.</p>
<p>Spending his research career in industrial research environments was not an accident. ""I like working in an industrial research lab, because of the input"", Lamport said. ""If I just work by myself and come up with problems, I’d come up with some small number of things, but if I go out into the world, where people are working on real computer systems, there are a million problems out there. When I look back on most of the things I worked on—Byzantine Generals, Paxos—they came from real-world problems.”&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_2"">2</a>]</p>
<p>His works shed light on fundamental issues of concurrent programs, for which there was no formal theory at the time. He grappled with fundamental concepts such as causality and logical time, atomic and regular shared registers, sequential consistency, state machine replication, Byzantine agreement and wait-freedom. He worked on algorithms which have become standard engineering practice for fault tolerant distributed systems. He has also developed a substantial body of work on the formal specification and verification of concurrent system, and has contributed to the development of automated tools applying these methods. We will touch on only some of his contributions.</p>
<p><strong>1. Mutual Exclusion solutions and the Bakery algorithm</strong></p>
<p>Lamport's influential works from the 1970's and 1980's came at a time there was only a little understanding of the fundamental issues about programming for multiple concurrent processors.</p>
<p>For example, it was known that correct execution may require parallel activities to exclude one another during periods in ""critical sections"" when they manipulate the same data, in order to prevent undesired interleaving of operations. The origins of this <em>mutual exclusion </em>problem are from Edsger Dijkstra's pioneering work, which includes his solution. [<a href=""/bib/lamport_1205376.cfm#bib_3"">3</a>] Dijkstra's algorithm, while correct, depends on shared memory accesses being atomic – that one processor reading when another is writing will be made to wait, rather than returning a possibly garbled value. In a sense, it constructs a high-level solution out of low-level mutual exclusion already implemented by the hardware.</p>
<p>Lamport's remarkably elegant and intuitive ""Bakery Algorithm""&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_4"">4</a>] doesn't do that. &nbsp;His solution arranges contending processes in an implicit queue according to their arrival order, much like a wait-queue in a Bakery. Yet it doesn't matter if a processor reading data that is being updated by another processor gets garbage. The algorithm still works.</p>
<p>The Bakery algorithm has become textbook material, and most undergraduates in computer science encounter it in the course of his or her studies.</p>
<p><strong>2. Foundations of Concurrent programming</strong></p>
<p>Several important new concepts emanated from the Bakery Algorithm work, a trend which recurred several times in Lamport's career. The experience of devising concurrent algorithms and verifying correctness caused him to focus on the basic foundations that would make multiprocessors behave in a manner that programmers can reason mathematically about. While working on a solution for a specific concrete problem, Lamport invented abstractions and general rules needed to reason about its correctness, and these conceptual contributions then became theoretical pillars of concurrent programming.</p>
<p><strong>Loop-freedom: </strong>The Bakery Algorithm work introduced an important concept called ""loop freedom"". &nbsp;Some obvious solutions that come to mind for the mutual exclusion problem pre-assign `turns' in rotation among the processes. But this forces processes to wait for others that are slow and have not yet even reached the point of contention. Using the bakery analogy, it would be akin to arriving to an empty bakery and being asked to wait for a customer who hasn't even arrived at the bakery yet. In contrast, loop-freedom expresses the ability of processes to make progress independent of the speed of other processes. Because the Bakery Algorithm assigns turns to processes in the order of their arrival, it has loop-freedom.&nbsp; This is a crucial concept which has been used in the design of many subsequent algorithms, and in the design of memory architectures. <em>Wait-freedom</em>, a condition requiring independent progress despite failures, has its clear roots in the notion of loop-freedom and the Bakery doorway concept. It was later extensively explored by others, including Maurice Herlihy&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_5"">5</a>].</p>
<p><strong>Sequential consistency:</strong> Working with a multi-core architecture that had distributed cache memory led Lamport to create formal specifications for coherent cache behavior in multiprocessor systems. That work brought some order to the chaos of this field by inventing <em>sequential consistency</em> [<a href=""/bib/lamport_1205376.cfm#bib_6"">6</a>], which has become the gold standard for memory consistency models. This simple and intuitive notion provides just the right level of “atomicity” to allow software to work. Today we design hardware systems with timestamp ordering or partial-store ordering, with added memory fence instructions, which allows programmers to make the hardware appear sequentially consistent. Programmers can then implement algorithms that provide strong consistency properties. This is key to the memory consistency models of Java and C++. Our multicore processors run today based on principles described by Leslie Lamport in 1979.</p>
<p><strong>Atomic and regular registers: </strong>The Bakery Algorithm also led Lamport to wonder about the precise semantics of memory when multiple processes interact to share data. It took almost a decade to formalize, and the result is the abstraction of regular and atomic registers [<a href=""/bib/lamport_1205376.cfm#bib_7"">7</a>].</p>
<p>His theory gives each operation on a shared register an explicit duration, starting with an invocation and ending with a result. The registers can be implemented by a variety of techniques, such as replication. Nevertheless, the interactions of processes with an atomic register are supposed to “look like” serial accesses to actual shared memory. The theory also includes weaker semantics of interaction, like those of a regular register. A regular register captures situations in which processes read different replicas of the register while it is being updated. At any moment in time, some replicas may be updated while others are not, and eventually, all replicas will hold the updated value. Importantly, these weaker semantics suffice to support mutual exclusion: the Bakery algorithm works correctly if a reader overlapping a writer obtains back any arbitrary value.</p>
<p>This work initiated a distinct subfield of research in distributed computing that is still thriving. Lamport’s atomic objects supported only read and write operations, that is, they were atomic registers. The notion was generalized to other data types by Maurice Herlihy and Jeannette Wing [<a href=""/bib/lamport_1205376.cfm#bib_8"">8</a>], and their term ""linearizability"" became synonymous with atomicity. Today, essentially all non-relational storage systems developed by companies like Amazon, Google, and Facebook adopt linearizability and sequential consistency for their data coherence guarantees.&nbsp; &nbsp;</p>
<p><strong>3. Foundations of Distributed Systems</strong></p>
<p>A special type of concurrent system is a distributed system, characterized by having processes that use messages to interact with each other. Leslie Lamport has had a huge impact on the way we think about distributed system, as well as on the engineering practices of the field.</p>
<p><strong>Logical clocks:</strong> Many people realized that a global notion of time is not natural for a distributed system. &nbsp;Lamport was the first to make precise an alternative notion of ""logical clocks"", which impose a partial order on events based on the causal relation induced by sending messages from one part of the system to another&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_9"">9</a>]. His paper on ""Time, Clocks, and the Ordering of Events in a Distributed System"" has become the most cited of Lamport’s works, and in computer science parlance logical clocks are often nicknamed <em>Lamport timestamps</em>. His paper won the 2000 <em>Principles of Distributed Computing Conference</em> Influential Paper Award (later renamed the Edsger W. Dijkstra Prize in Distributed Computing), and it won an ACM SIGOPS Hall of Fame Award in 2007.</p>
<p>To understand why that work has become so influential, recognize that at the time of the invention there was no good way to capture the communication delay in distributed systems except by using real time. Lamport realized that the communication delay made those systems very different from a shared-memory multiprocessor system. The insight came when reading a paper on replicated databases&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_10"">10</a>] and realizing that its logical ordering of commands might violate causality.</p>
<p>Using ordering of events as a way of proving system correctness is mostly what people do today for intuitive proofs of concurrent synchronization algorithms.&nbsp; Another powerful contribution of this work was to demonstrate how to replicate a state machine using logical clocks, which is explained below.</p>
<p><strong>Distributed Snapshots:</strong> &nbsp;Once you define causal order, the notion of consistent global states naturally follows. That led to another insightful work. &nbsp;Lamport and Mani Chandy invented the first algorithm for reading the state (taking a `snapshot’) of an arbitrary distributed system&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_11"">11</a>]. This is such a powerful notion that others later used it in different domains, like networking, self-stabilization, debugging, and distributed systems. This paper received the 2013 ACM SIGOPS Hall of Fame Award.</p>
<p><strong>4. Fault tolerance and State Machine Replication</strong></p>
<p>``A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable'' is a famous Lamport quip. Much of his work is concerned with fault tolerance.</p>
<p><strong>State Machine Replication (SMR):</strong> Perhaps the most significant of Lamport's many contributions is the State Machine Replication paradigm, which was introduced in the famous paper on ``Time, Clocks, and the Ordering of Events in a Distributed System', and further developed soon after&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_12"">12</a>]. The abstraction captures any service as a centralized state machine -- a kind of a universal computing engine similar to a Turing machine. It has an internal state, and it processes commands in sequence, each resulting in a new internal state and producing a response. Lamport realized that the daunting task of replicating a service over multiple computers can be made remarkably simple if you present the same sequence of input commands to all replicas and they proceed through an identical succession of states.</p>
<p>This insightful SMR paradigm underlies many reliable systems, and is considered a standard approach for building replicated distributed systems due to its elegance. But before Lamport developed a full solution using for SMR, he needed to address a core ingredient, agreement, which was tackled in his next work.</p>
<p><strong>Byzantine Agreement: </strong>While state machine approaches that are resilient to crash faults are sufficient for many applications, more mission-critical systems, such as for avionics, need an even more extreme model of fault-tolerance that is impervious to nodes that might disrupt the system from within. &nbsp;&nbsp;</p>
<p>At Stanford Research Institute (later called SRI International) in the 1970's, Lamport was part of a team that helped NASA design a robust avionics control system. Formal guarantees were an absolute necessity because of the mission-critical nature of the task. Safety had to be guaranteed against the most extreme system malfunction one could imagine.&nbsp; One of the first challenges the team at SRI was asked to take on was to prove the correctness of a cockpit control scheme, which NASA had designed, with three computer systems that use majority-voting to mask any faulty component.</p>
<p>The result of the team's work was several foundational concepts and insights regarding these stringent types of robust systems. It included a fundamental definition of robustness in this setting, an abstraction of the coordination problem which underlies any replicated system to this date, and a surprising revelation that systems with three computers can never safely run a mission critical cockpit!</p>
<p>Indeed, in two seminal works (""LPS"") published with Pease and Shostak&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_13"">13</a>]&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_14"">14</a>], the team first identified a somewhat peculiar vulnerability. LPS posited that ""a failed component may exhibit a type of behavior that is often overlooked -- namely, sending conflicting information to different parts of the system"". More generally, a malfunctioning component could function in a manner completely inconsistent with its prescribed behavior, and might appear almost malicious.</p>
<p>The new fault model needed a name. &nbsp;At the time there was a related classical challenge of coordinating two communicating computers, introduced in a 1975 paper [<a href=""/bib/lamport_1205376.cfm#bib_15"">15</a>] and referred to by Jim Gray in [<a href=""/bib/lamport_1205376.cfm#bib_16"">16</a>] as the ""The Two Generals Paradox "". This led Lamport to think of the control computers in a cockpit as an army of Byzantine Generals, with the army trying to form a coordinated attack while inside traitors sent conflicting signals. The name ""Byzantine Failures"" was adopted for this fault model, and a flurry of academic work followed. The Byzantine fault model is still in use for capturing the worst kind of mishaps and security flaws in systems.</p>
<p>Byzantine Failures analyze the bad things which may happen. But what about the good things that need to happen? LPS also gave an abstract formulation of the problem of reaching coordination despite Byzantine failures; this is known as the ""Byzantine Agreement"" problem. This succinct formulation expresses the control coordination task as the problem of forming an agreement decision for an individual bit, starting with potentially different bits input to each component. Once you have agreement on a single bit, it is possible to repeatedly use it in order to keep an entire complex system coordinated. The paper shows that four computers are needed to form agreement on a single bit in face of a single malfunction. Three are not enough, because with three units, a faulty unit may send conflicting values to the other two units, and form a different majority with each one. More generally, they showed that 3F+1 units are needed in order to overcome F simultaneously faulty components. To prove this, they used a beautiful symmetry argument which has become known as the `hexagon argument'. This archetype argument has found other uses whenever one argues that a malfunctioning unit that sends conflicting information to different parts of the system looks indistinguishable from a symmetrical situation in which the correct and faulty roles are reversed.</p>
<p>LPS also demonstrated that 3F+1 units are enough, and they presented a solution for reaching Byzantine Agreement among the 3F+1 units in F+1 synchronous communication rounds. They also showed that if you use digital signatures, just 2F+1 units are sufficient and necessary.</p>
<p>The Byzantine Agreement problem and its solutions have become the hallmark of fault tolerant systems. Most systems constructed with redundancy make use of it internally for replication and for coordination. Lamport himself later used it in forming the State Machine Replication paradigm discussed next, which gives the algorithmic foundation of replication.</p>
<p>The 1980 paper was awarded the 2005 Edsger W. Dijkstra Prize in Distributed Computing, and the 1982 paper received the Jean-Claude Laprie Award in Dependable Computing.</p>
<p><strong>Paxos:</strong> With a growing understanding of the agreement problem for distributed computing, it was time for Lamport to go back to State Machine Replication and address failures there. The first SMR solution he presented in his 1978 paper assumed there are no failures, and it makes use of logical time to step replicas through the same command sequence. In 1989, Lamport designed a fault tolerant algorithm called Paxos&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_17"">17</a>]&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_18"">18</a>]. Continuing his trend of humorous parable-telling, the paper presents the imaginary story of a government parliament on an ancient Greek island named Paxos, where the absence of any number of its members, or possibly all of them, can be tolerated without losing consistency.</p>
<p>Unfortunately the setting as a Greek parable made the paper difficult for most readers to comprehend, and it took nine years from submission to publication in 1998. But the 1989 DEC technical report did get noticed. Lamport's colleague Butler Lampson evangelized the idea to the distributed computing community&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_19"">19</a>]. Shortly after the publication of Paxos, Google's Chubby system&nbsp;and Apache's open-source ZooKeeper offered State Machine Replication as an external, widely-deployed service.</p>
<p>Paxos stitches together a succession of agreement decisions into a sequence of state-machine commands in an optimized manner. Importantly, the first phase of the agreement component given in the Paxos paper (called Synod) can be avoided when the same leader presides over multiple decisions; that phase needs to be performed only when a leader needs to be replaced. This insightful breakthrough accounts for much of the popularity of Paxos, and was later called <em>Multi-Paxos </em>by the Google team&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_20"">20</a>]. Lamport's Paxos paper won the ACM SIGOPS (Special Interest Group on Operating Systems) Hall of Fame Award in 2012.&nbsp;</p>
<p>SMR and Paxos have become the de facto standard framework for designing and reasoning about consensus and replication methods. Many companies building critical information systems, including Google, Yahoo, Microsoft, and Amazon, have adopted the Paxos foundations.</p>
<p><strong>5. Formal specification and verification of programs</strong></p>
<p>In the early days of concurrency theory the need surfaced for good tools to describe solutions and prove their correctness. Lamport has made central contributions to the theory of specification and verification of concurrent programs.&nbsp; For example, he was the first to articulate the notions of safety properties and liveness properties for asynchronous distributed algorithms. These were the generalization of “partial correctness” and “total correctness” properties previously defined for sequential programs. Today, safety and liveness properties form the standard classification for correctness properties of asynchronous distributed algorithms.</p>
<p>Another work, with Martin Abadi [<a href=""/bib/lamport_1205376.cfm#bib_21"">21</a>], Introduced a special abstraction called prophecy variables to an algorithm model, to handle a situation where an algorithm resolves a nondeterministic choice before the specification does. Abadi and Lamport pointed out situations where such problems arise, and developed the theory needed to support this extension to the theory. Moreover, they proved that whenever a distributed algorithm meets a specification, where both are expressed as state machines, the correspondence between them can be proved using a combination of prophecy variables and previous notions such as history variables. This work won the 2008 LICS Test-Of-Time award.</p>
<p><strong>Formal Modeling Languages and Verification Tools: </strong>In addition to developing the basic notions above, Lamport has developed the language TLA (Temporal Logic of Actions) and the TLA+ toolset, for modeling and verifying distributed algorithms and systems.</p>
<p>TLA and TLA+ support specification and proof of both safety and liveness properties, using notation based on temporal logic. Lamport has supervised the development of verification tools based on TLA+, notably the TLC model checker built by Yuan Yu. TLA+ and TLC have been used to describe and analyze real systems. For example, these tools were used to find a major error in the coherence protocol used in the hardware for Microsoft’s Xbox 360 prior to its release in 2005. At Intel, it was used for the analysis of a cache-coherence protocol of the Intel Quick Path Interconnect as implemented in the Nehalem core processor. To teach engineers how to use his formal specification tools, Lamport has written a book [<a href=""/bib/lamport_1205376.cfm#bib_22"">22</a>]. More recently, Lamport has developed the PlusCAL formal language and tools for use in verifying distributed algorithms; this work builds upon TLA+.</p>
<p><strong>6. LaTeX</strong></p>
<p>When creating such a vast collection of impactful papers, it is natural to wish for a convenient typesetting tool. Lamport did not just wish for one, he created one for the entire community. Outside the field of concurrency is Lamport’s Latex system&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_23"">23</a>], a set of macros for use with Donald Knuth’s TeX typesetting system&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_24"">24</a>]. LaTeX added three important things to TeX:</p>
<ol>
<li>The concept of ‘typesetting environment’, which had originated in Brian Reid’s Scribe system.</li>
<li><span style=""line-height: 1.3;"">A strong emphasis on structural rather than typographic markup.</span></li>
<li><span style=""line-height: 1.3;"">A generic document design, flexible enough to be adequate for a wide variety of documents.</span></li>
</ol>
<p>Lamport did not originate these ideas, but by pushing them as far as possible he created a system that provides the quality of TeX and a lot of its flexibility, but is much easier to use. Latex became the de facto standard for technical publishing in computer science and many other fields.</p>
<p style=""margin-left: 40px;""><em>There are many other important papers by Leslie Lamport -- too many to describe here. They are listed in chronological order on Lamport's </em><em>home page&nbsp;[<a href=""/bib/lamport_1205376.cfm#bib_1"">1]</a></em><em>, accompanied by historical notes that describe the motivation and context of each result. </em></p>
<p style=""margin-left: 40px;""><em>Any time you access a modern computer, you are likely to be impacted by Leslie Lamport's algorithms. And all of this work started with the quest to understand how to organize a queue at a local bakery.</em></p>
<p align=""right""><span class=""callout"">Author: Dahlia Malkhi</span><br>
<span class=""callout"">Additional contributors: Martin Abadi, Hagit Attiya, Idit Keidar,<br>
Nancy Lynch, Nir Shavit,<br>
George Varghese, and Len Shustek</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/lamport_1205376.cfm""><img src=""/images/lg_aw/1205376.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Leslie Lamport""></a>
<br><br>
<h6 class=""label""><a href=""/photo/lamport_1205376.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6>BIRTH:</h6>
<p>7 February 1941 in New York, New York</p>
<h6>EDUCATION:<span style=""line-height: 1.3;""> </span></h6>
<p><span style=""line-height: 1.3;"">Bronx High School of Science (1957); B.S. (Massachusetts Institute of Technology, Mathematics, 1960);&nbsp; M.S (Brandeis University, Mathematics, 1963);&nbsp; PhD (Brandeis University, Mathematics, 1972).</span></p>
<h6>EXPERIENCE:</h6>
<p><span style=""line-height: 1.3;"">Massachusetts Computer Associates, 1970-1977; SRI&nbsp; International, 1977-1985; Digital Equipment Corporation and Compaq, 1985-2001; Microsoft Research, from 2001.</span></p>
<h6>HONORS AND AWARDS:</h6>
<p>Dijkstra<span style=""line-height: 1.3;""> Prize, for the paper ""Time, clocks, and the ordering of events in a distributed system"" (2000); Honorary Doctorate, University of </span>Rennes<span style=""line-height: 1.3;""> (2003); Honorary Doctorate, Christian </span>Albrechts<span style=""line-height: 1.3;""> University of Kiel (2003);&nbsp; Honorary Doctorate, </span>École<span style=""line-height: 1.3;""> </span>Polytechnique<span style=""line-height: 1.3;""> </span>Fédérale<span style=""line-height: 1.3;""> de Lausanne (2004); IEEE Emanuel R. </span>Piore<span style=""line-height: 1.3;""> Award (2004); </span>Dijkstra<span style=""line-height: 1.3;""> Prize, for the paper ""Reaching Agreement in the Presence of Faults "" (2005); Honorary Doctorate, University of </span>Lugano<span style=""line-height: 1.3;""> (2006);&nbsp; Honorary Doctorate, </span>Nancy-Université<span style=""line-height: 1.3;""> (2007); IEEE John von Neumann Medal (2008); Member, US National Academy of Sciences (2011); ACM Fellow (2014).</span></p>
<h6>&nbsp;</h6>","","https://dl.acm.org/author_page.cfm?id=81100244989","Leslie Lamport","<li class=""bibliography""><a href=""/bib/lamport_1205376.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/lamport_1205376.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/lamport_1205376.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/lamport_1205376.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/lamport_1205376.cfm""><span></span>Video Interview</a></li>"
"1573179567-692","https://amturing.acm.org/award_winners/nygaard_5916220.cfm","With Ole-Johan Dahl, for ideas fundamental to the emergence of object oriented programming, through their design of the programming languages Simula I and Simula 67.","<p><strong>Kristen Nygaard is internationally acknowledged as the co-inventor with&nbsp;<a href=""/award_winners/dahl_6917600.cfm"">Ole-Johan Dahl</a>&nbsp;of&nbsp;<em><a href=""https://en.wikipedia.org/wiki/Object-oriented_programming"" target=""_blank"">object-oriented programming</a></em> and the programming language <a href=""https://en.wikipedia.org/wiki/Simula"" target=""_blank"">SIMULA</a>.</strong> In addition he was a pioneer of <em>participatory design</em> and the ""Scandinavian school of systems development"".</p>
<p>Nygaard’s original field was operations research. Early in his career he realized that computer simulations would be a useful tool. He collaborated closely with Ole-Johan Dahl who, in Kristen’s words, “had an exceptional talent for programming”. This collaboration led to the first Simula language, SIMULA I, based on the language <a href=""https://en.wikipedia.org/wiki/ALGOL_60"" target=""_blank"">ALGOL-60</a>. SIMULA I was originally considered a system description and simulation language, not a general programming language. Dahl &amp; Nygaard quickly realized that its simulation concepts could be applied for programming in general. As a result of this insight they designed Simula 67, later just called SIMULA. It is a general purpose programming language and, like SIMULA I, it contains Algol-60 as a subset.</p>
<p>SIMULA contains the core of the concepts now available in mainstream object-oriented languages such as C++, Eiffel, Java, and C#:</p>
<ol>
<li><a href=""https://en.wikipedia.org/wiki/Class_%28computer_programming%29"" target=""_blank"">Class and object</a>. The class concept as a template for creating instances (objects).</li>
<li>Subclass. Classes may be organized in a classification hierarchy by means of subclasses.</li>
<li><a href=""https://en.wikipedia.org/wiki/Virtual_function"" target=""_blank"">Virtual methods</a>. A SIMULA class may define virtual methods that can be redefined in subclasses.</li>
<li><a href=""https://en.wikipedia.org/wiki/Active_object"" target=""_blank"">Active objects</a>. An object in SIMULA may be the head of an active thread; technically it is a <a href=""https://en.wikipedia.org/wiki/Coroutine"">co-routine</a>.</li>
<li>Action combination. SIMULA has an inner-construct for combining the action-parts of a class and its subclass.</li>
<li>Processes and schedulers. SIMULA makes it easy to write new concurrency abstractions, including schedulers.</li>
<li><a href=""https://en.wikipedia.org/wiki/Class_%28computer_programming%29"" target=""_blank"">Frameworks</a>. SIMULA provided the first object-oriented framework in the form of Class Simulation—the mechanism it used to implement its simulation.</li>
<li>Automatic memory management, including garbage collection.</li>
</ol>
<p>One exception to the broad adoption of SIMULA concepts is the notion of an active object with its own action sequence, which, strangely enough, has not been adapted to other languages. For Dahl &amp; Nygaard, having active objects was an essential facility to be able to simulate concurrent processes from the real world.</p>
<p>Before the concept of object-orientation became popular, SIMULA influenced the development of <a href=""https://en.wikipedia.org/wiki/Abstract_data_type"" target=""_blank"">new abstract data types</a>. As a result of these ideas, Simula was extended with constructs such as public, private and protected modifiers, originally proposed by <a href=""http://people.dsv.su.se/~jpalme/"" target=""_blank"">Jakob Palme</a>.</p>
<p>One unexpected result was that people often found making a model in Simula to be more useful than the actual simulation results. The process of describing the application provided a valuable insight in itself. This led Kristen to formulate one of his favorite aphorisms: <em>To program is to understand.</em> He thought programming should not be considered a low-level technical discipline designed just to accomplish a specific task, but that writing a program should enhance understanding of the problem domain and the solutions.</p>
<p>Nygaard’s next advance was the development, with Petter Håndlykken and Erik Holbæk-Hansen, of DELTA. It was not a programming language, but rather a system description language used to aid in modeling real world systems. It was intended for collaborative use by developers and users together—<em>delta</em> means<em> participate</em> in Norwegian. DELTA was based on SIMULA, but extended with equations for describing both discrete state changes and continuous changes over time.</p>
<p>When Kristen was a visiting professor in Aarhus, Denmark, he initiated work with Bent Bruun Kristensen, Ole Lehrmann Madsen and Birger Møller-Pedersen on the <a href=""https://en.wikipedia.org/wiki/BETA"" target=""_blank"">BETA</a> programming language. BETA is a language for describing models of the real world, but, in the tradition of SIMULA, it was also to be useful as an implementation language. In the design of the BETA programming language, a criterion for adding any new construct to the language was that it be meaningful both for modeling and for programming.</p>
<p>In the late sixties, the Norwegian Iron and Metal Workers Union contacted Kristen for help with new computing technologies. Kristen and others developed courses and books about information technologies for the Union. The project developed the first “data agreement” between a union and a company, and elements from this were later included in Norwegian legislation.</p>
<p>The Iron and Metal project was the first of a series of Scandinavian research projects involving users in the design of IT systems. Nygaard was motivated by a desire to empower users to have more influence on these designs. Introducing user participation in systems development was, for many years, considered to be political. Today, however, companies realize that by directly involving users in the design process, the resulting systems are often better. Methods for involving users in the design of IT systems are now known as participatory design, and are taught and practiced by many groups around the world.</p>
<p>Kristen was also one of the few philosophers of informatics, stemming from his early work on simulation. To be able to create a model of real-world phenomena, it is necessary to have a strong conceptual framework to understand and organize knowledge.&nbsp; Modeling capabilities were always central to his design of languages. One famous example of this is the subclass mechanism, developed to represent domain concept specialization hierarchies.</p>
<p>Kristen was thrilled by the enormous success of the object-oriented approach to programming. During a visit to Xerox PARC, <a href=""/award_winners/kay_3972189.cfm"">Alan Kay</a> demonstrated the <a href=""https://en.wikipedia.org/wiki/Smalltalk"" target=""_blank"">Smalltalk system</a>, and Kirsten was very impressed to see how the ideas from SIMULA had inspired the Smalltalk-team. He was happy to see these ideas further adapted by languages such as Flavours, Loops, C++, Eiffel, Java, C# and many others. He never participated in the critique of possible shortcomings of other languages; on the contrary, he respected their creators and acknowledged their influence on the development of object-orientation.</p>
<p>Hundreds of books were written on object-orientation, but Kristen found that most of these books did not do a good job in teaching the fundamental concepts. He was very concerned with education, but frustrated by what he considered a lack of quality. The goal of his last project (COOL: Comprehensive Object-Oriented Learning) was to develop first-class teaching material on object-oriented programming. He had just set up an international team of participants and was ready to start the work when he died in 2002.</p>
<p>Kristen initiated research on participatory design and object-oriented programming at the University of Aarhus. At the University of Oslo he initiated research on participatory design; Ole-Johan Dahl had already established a research group on object-orientation. His work on system development and on the social impacts of computing technology became the foundation of the Scandinavian School in System Development and the field of participatory design.</p>
<p>But Kristen was not just a pioneer and researcher in informatics. He was an engaged social and political citizen, involved in several aspects of society, including politics. During the intense political fight before the 1972 Referendum on whether Norway should become member of the European Common Market, he worked as coordinator for the large majority of youth organizations that worked against membership. He also was the leader (1990-1995) of Norway's <em>No to the EU </em>movement, which argued against Norwegian membership of the European Union and led to victory in the 1994 referendum.</p>
<p>Further information on Kirsten Nygaard can be obtained from the following:</p>
<ol>
<li><a href=""http://heim.ifi.uio.no/~gisle/in_memoriam_kristen/"" target=""_blank"">Memorial site</a> for Kristen Nygaard has further information.</li>
<li>Meyer, Bertrand (ed.), “In memory of Ole-Johan Dahl and Kristen Nygaard,” <em>Journal of Object-Technology,</em> Vol. 1, Num. 4, September-October 2002.</li>
<li>Berntsen, Drude, Knut Elgsaas, and Håvard Hegna, “The Many Dimensions of Kristen Nygaard, Creator of Object-Oriented Programming and the Scandinavian School of System Development,” <em>History of Computing: Learning from the Past, Proceedings of IFIP WG 9.7 International Conference, HC 2010,</em> held as Part of World Computer Congress 2010, Brisbane, Australia, September 20-23, 2010, Tatnall, Arthur (Ed.) ISBN: 978-3-642-15198-9, Springer Berlin Heidelberg New York 2010.</li>
<li>Bent Bruun Kristensen, Ole Lehrmann Madsen, Birger Møller-Pedersen: <em>The When, Why and Why Not of the BETA Programming Language,</em> ACM History of Programming Languages III, San Diego, June 2007.</li>
</ol>
<p style=""text-align: right;""><br>
<span class=""callout"">Author: Ole Lehrman Madsen</span><br>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/nygaard_5916220.cfm""><img src=""/images/lg_aw/5916220.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Kristen Nygaard ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/nygaard_5916220.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>August 27, 1926, Oslo Norway</p>
<h6 class=""label"">DEATH:</h6>
<p>August 10, 2002</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Nygaard got his master's degree in mathematics at the University of Oslo in 1956. His thesis on abstract probability theory was entitled ""Theoretical Aspects of Monte Carlo Methods.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>1948 to 1960, Norwegian Defense Research Establishment doing computing and programming (1948–1954) and operational research (1952–1960); 1957 to 1960, head of the operations research group in the Norwegian Defense Establishment; 1959–1964, cofounder and first chairman of the Norwegian Operational Research Society; 1960 on, Norwegian Computing Center (NCC), becoming its Director of Research in 1962; 1975–1976, visiting professor at the University of Aarhus, Denmark, remaining associated with that University until his death in 2002; 1977, professor at University of Oslo; 1987, visiting professor at Stanford University, visiting scientist at Xerox PARC in Palo Alto and a consultant and lecturer at Apple’s Advanced Technology Group.</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Honorary doctorate from Lund University, 1990; first individual to be given an honorary doctorate by Aalborg University, Denmark, 1991; Member of the Norwegian Academy of Sciences; Computer Professionals for Social Responsibility Norbert Wiener Award for Social and Professional Responsibility, 1990; became (together with Ole-Johan Dahl) the first to receive the Rosing Prize (awarded by the Norwegian Data Association for exceptional professional achievements), 1999; awarded an Honorary Fellowship for his originating of object technology concepts by the Object Management Group, of the International Organization for Standardization, 2000;&nbsp;<span style=""line-height: 20.8px;"">made Commander of the Royal Norwegian Order of St.&nbsp;</span>Olav<span style=""line-height: 20.8px;"">&nbsp;by the King of Norway, 2000;&nbsp;</span><span style=""line-height: 1.3;"">he and Dahl awarded the Institute of Electrical and Electronic Engineers (for design and implementation of </span>SIMULA<span style=""line-height: 1.3;""> 67), 2001; together with Ole-Johan Dahl awarded the ACM A. M. Turing Award, 2001; IEEE John von Neumann Medal, 2002;&nbsp;</span>AITO<span style=""line-height: 1.3;""> established an annual prize in the name of Ole-Johan Dahl and Kristen </span>Nygaard<span style=""line-height: 1.3;""> to honor their pioneering work on object-orientation, 2004.</span></p>
<p>The University of Oslo and the University of Aarhus have both named a building after Kristen Nygaard.</p>","","https://dl.acm.org/author_page.cfm?id=81100392381","Kristen Nygaard","<li class=""bibliography""><a href=""/bib/nygaard_5916220.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""key-words""><a href=""/keywords/nygaard_5916220.cfm""><span></span>Research<br> Subjects</a></li>"
"1573178822-643","https://amturing.acm.org/award_winners/feigenbaum_4167235.cfm","For pioneering the design and construction of large scale artificial
intelligence systems, demonstrating the practical importance and potential commercial impact
of artificial intelligence technology.","<p><strong>Edward Feigenbaum and <a href=""/award_winners/reddy_6247682.cfm"">Raj Reddy</a> have been seminal leaders in defining the emerging field of applied artificial intelligence, and in demonstrating its technological significance.</strong></p>
<p>Edward Albert Feigenbaum is widely known as&nbsp;<em>the father of expert systems</em>. <a href=""https://en.wikipedia.org/wiki/Expert_system"" target=""_blank"">Expert systems</a> are computer programs that act intelligently by using the specially encoded knowledge of experts in fields as diverse as chemistry, medical diagnosis and therapy, geologic exploration, hardware and software trouble-shooting and business practice.</p>
<p>Feigenbaum was born on January 20, 1936 in Weehawken, New Jersey. He learned to read very early, and as a young child became quite skilled in using his stepfather’s Monroe calculator. His stepfather took him on frequent visits to the Hayden Planetarium of the American Museum of Natural History, from which he developed an early interest in astronomy, and in science generally. His favorite pre-college courses were math, physics, and chemistry.</p>
<p>He received a scholarship to attend the Carnegie Institute of Technology (now Carnegie Mellon University) in Pittsburgh, Pennsylvania. Among the courses he took was one taught by <a href=""/award_winners/simon_1031467.cfm"">Herbert Simon</a> (1975 Turing Award recipient) called “Mathematical Models in the Social Sciences.” His first introduction to computers was digesting the manual Simon gave him for the IBM 701, an early vacuum-tube computer.</p>
<p>Feigenbaum stayed at Carnegie Tech and did a PhD dissertation under Simon’s supervision, on a computer model that simulates how humans learn nonsense syllables. He called the computer program implementing the model EPAM: Elementary Perceiver and Memorizer. EPAM is still considered a leading theory of memory organization in cognitive science. It was also one of the first programs to demonstrate that a computer could learn. The basic mechanism of EPAM, a <a href=""https://en.wikipedia.org/wiki/Decision_tree"" target=""_blank"">decision tree</a> (which Feigenbaum called a “Discrimination Net”), is one of the most important computational structures for storing and indexing data. It is used extensively in machine learning research, and in <a href=""https://en.wikipedia.org/wiki/Data_mining"" target=""_blank"">data mining</a>. At Carnegie, Feigenbaum also participated in the implementation of <a href=""https://en.wikipedia.org/wiki/Information_Processing_Language"" target=""_blank"">IPL-V</a>, the first publicly available list-processing language.</p>
<p>After completing his dissertation in 1960, he accepted a position at the University of California, Berkeley, where he taught courses on organization theory, and on computer simulation of intelligent behavior. For the latter, which included topics in what now is called artificial intelligence, he and colleague Julian Feldman distributed photocopies of early papers on computer intelligence. In 1963, these papers, plus some others specially commissioned, were published in an influential volume titled <em>Computers and Thought,</em> which was republished in 1995 by AAI Press in conjunction with MIT Press.[<a href=""/bib/feigenbaum_4167235.cfm#link_1"">1</a>]</p>
<p>After five years at Berkeley, Feigenbaum joined the faculty at Stanford University. He was motivated primarily by the desire to shift away from the science of how humans think, which occupied much of his time at Berkeley, to the technology of getting computers to think—a technology that was the focus of <a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy</a>, who had recently moved to Stanford from MIT.</p>
<p>Feigenbaum was interested in the problem of “induction”, and in particular how to get computers to create theories from data− theories that not only explained the particular data on which the theory was based, but could also make predictions about new data. He thought progress could best be made by finding and working on a specific problem. As he later put it,<em> I needed a ‘task environment—a sandbox in which to specifically work out ideas in detail.</em> Joshua Lederberg, a Nobel Prize winner in genetics at Stanford was doing work on analyzing mass spectrograms of amino acids, and suggested to Feigenbaum the problem of inducing the three-dimensional structure of these chemicals from their mass spectrograms.</p>
<p>Following up on Lederberg’s suggestion, Feigenbaum and colleagues developed <em>Heuristic DENDRAL,</em> a computer program that could guess the geometrical structure of complex chemical compounds given their chemical formulae and their mass spectrogram data. <em>Heuristic DENDRAL</em> discovered some previously unknown structures, and these discoveries were published in a series of papers in the <em>Journal of the American Chemical Society.</em> The program used rules, elicited from chemists, about how a mass spectrometer fragmented compounds into sub-structures. From knowledge of the sub-structures, <em>Heuristic DENDRAL</em> was able to deduce the most plausible overall structure of the compound. Later, his <em>META-DENDRAL</em> program was even able to automatically deduce new rules from chemical data that <em>Heuristic DENDRAL</em> could use to improve its performance.</p>
<p>From his work on <em>Heuristic DENDRAL,</em> Feigenbaum became convinced about the importance of endowing computer programs with <em>knowledge</em> in the form of rules and procedures to guide the process of problem solving. He is credited with inventing and being the first to use the phrases “Knowledge is Power,” “Expert Systems,” “Knowledge Engineering,” and “Expertise” in connection with AI programs. Many researchers in artificial intelligence had previously focused on formal “reasoning” methods. Feigenbaum shifted the emphasis to “knowledge,” and that shift was critically important to future successes in artificial intelligence.</p>
<p>After their work on chemical structures, Feigenbaum’s laboratory went on to develop expert-system programs in medicine <em>(MYCIN, PUFF, ONCOCIN),</em> molecular genetics <em>(MOLGEN)</em>, X-ray crystallography <em>(CHRYSALIS),</em> and analysis of pulmonary function<em> (PUFF)</em>. It also developed the first transportable general-purpose expert system “shell” <em>(EMYCIN).</em> With the aid of computer scientists, experts in different subject areas could populate <em>EMYCIN</em> with their specialized knowledge in the form of rules, and then the knowledge-augmented system could be applied to different problem areas.</p>
<p>Feigenbaum and his wife Penny Nii also did important work for the U.S. Defense Department on applying expert-system technology to the problem of interpreting data from ocean-based hydro-acoustic sensors, resulting in a computer program called <em>HASP.</em></p>
<p>As one would expect from an academic scholar engaged in leading edge research, Feigenbaum guided a large number of graduate students into successful scientific, technical, and business careers. Among these were Edward (Ted) Shortliffe, Randall Davis, Peter Friedland, Mark Stefik, Bill Van Melle, and Jan Aikens. He also had a number of important collaborators, including Joshua Lederberg, Bruce Buchanan, Carl Djerassi, Robert Engelmore, Robert Lindsay, and Georgia Sutherland. His personal style always involved collaborating with extremely capable colleagues and students, guiding and inspiring the work with key ideas.</p>
<p>Feigenbaum has also been involved in administrative and professional activities. He was the director of Stanford University’s Computer Center during the 1960s, and served as the chair of Stanford’s Department of Computer Science from 1976 to 1981. He played a key role in the formation of <em>SUMEX</em>—a national computing resource at Stanford supported by, and aiding, the U. S. National Institutes of Health.</p>
<p>Feigenbaum is a Fellow and Past President of the American Association for Artificial Intelligence (now called the Association for the Advancement of Artificial Intelligence). He has served on the National Science Foundation Computer Science Advisory Board, on the National Research Council’s Computer Science and Technology Board, and as a member of the Board of Regents of the National Library of Medicine. He has taught at Stanford’s Kyoto campus and has lectured frequently in Japan and Europe about artificial intelligence and its applications. He maintains strong links with Japanese universities. From 1994 to 1997, he served as Chief Scientist of the U. S. Air Force.</p>
<p>Feigenbaum co-founded three companies involved in applied artificial intelligence, <em>IntelliCorp, Teknowledge</em>, and <em>Design Power Inc</em>. He continues as an adviser to companies employing AI and related computer technology.</p>
<p>His activities in teaching, research, scholarship, business, and public service have had lasting impact in technical, business, and government communities. His accomplishments have led to many awards and honors. In addition to being a co-recipient (with Raj Reddy) of the ACM Turing Award in 1994, he is a Fellow of the American College of Medical Informatics, of the American Institute of Medical and Biological Engineering, and of the American Association for the Advancement of Science. He is a member of the National Academy of Engineering and of the American Academy of Arts and Sciences.</p>
<p>For an article based on an interview with Feigenbaum, see Len Shustek, “An Interview with Ed Feigenbaum” <em>Communications of the ACM</em>, Vol. 53 No. 6, pp. 41-45, June 2010. Available online <a href=""https://cacm.acm.org/magazines/2010/6/92472-an-interview-with-ed-feigenbaum/fulltext"" target=""_blank"">here</a>.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Nils J. Nilsson</span><br>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/feigenbaum_4167235.cfm""><img src=""/images/lg_aw/4167235.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Edward A Feigenbaum""></a>
<br><br>
<h6 class=""label""><a href=""/photo/feigenbaum_4167235.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>January 20, 1936, Weehawken, New Jersey.</p>
<h6 class=""label"">EDUCATION:</h6>
<p>B.S., Electrical Engineering, Carnegie Institute of Technology, 1956; Ph.D., Graduate School of Industrial Administration, Carnegie Institute of Technology (now Carnegie-Mellon University), Pittsburgh, Pennsylvania, September 1960; D.Sc., Aston University, England (honorary), 1989.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Research Appointment, Center for Research in Management Science, University of California, Berkeley, 1960-1964; Assistant Professor, School of Business Administration, University of California, Berkeley, 1960-1963; Research Appointment, Center for Human Learning, University of California, Berkeley, 1961-1964; Associate Professor, School of Business Administration, University of California, Berkeley1964-1965; Stanford University, Associate Professor of Computer Science, 1965-1968: Professor of Computer Science1969-1995, 1995-2000: Kumagai Professor of Computer Science, Stanford University (post 2000, Professor Emeritus); Principal Investigator, Heuristic Programming Project; Director and then Co-Director, Knowledge Systems Laboratory, Stanford University, 1965-2000; Director, Stanford Computation Center, 1965-1968; Principal, then Co-Principal Investigator, SUMEX-AIM Project, a national computer resource for application of artificial intelligence to medicine and biology, Stanford University, 1978-1992; Professor (by Courtesy), Department of Psychology, Stanford University, 1976-1983; Chairman, Computer Science Department, Stanford University, 1976-1981; Chief Scientist of the Air Force, 1994-1997; Founder and Co-Director, Stanford Software Industry Project, 1993-1998; Senior Scientist, Air Force Office of Scientific Research, 2000-2001.</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Fulbright Research Scholarship to Great Britain, 1959-1960; Elected Fellow, American Association for the Advancement of Science, 1983; Elected Fellow, American College of Medical Informatics, 1984; Elected Member, National Academy of Engineering, 1986; Elected to Productivity Hall of Fame, Republic of Singapore, 1986; D.Sc., Aston University, England (honorary), 1989; Elected Fellow, American Association for Artificial Intelligence, 1990; Elected Member, American Academy of Arts and Sciences, 1991; Feigenbaum Medal, first recipient of an award established in his honor by the World Congress on Expert Systems, 1991; Elected Fellow, American Institute of Medical and Biological Engineering, 1994; Association for Computing Machinery Turing Award recipient (jointly with Raj Reddy), 1994; United States Air Force Exceptional Civilian Service Award, 1997; United States Air Force Meritorious Civilian Service Award, 1999; Okawa Foundation Research Award, 2004; Hall of Fame, Heinz Nixdorf Museum, Paderborn, Germany, 2004; American Association for Artificial Intelligence, Distinguished Service Award, 2006; Named a member of IEEE Intelligent System's inaugural “AI Hall of Fame”, 2011; Fellow, Computer History Museum, 2012; IEEE Computer Society Pioneer Award, 2013.<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100346299","Edward A (""Ed"") Feigenbaum","<li class=""bibliography""><a href=""/bib/feigenbaum_4167235.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283951&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/feigenbaum_4167235.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/feigenbaum_4167235.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/feigenbaum_4167235.cfm""><span></span>Video Interview</a></li>"
"1573179213-670","https://amturing.acm.org/award_winners/corbato_1009471.cfm","For his pioneering work organizing the concepts and leading the
development of the general-purpose, large-scale, time-sharing and resource-sharing computer
systems, CTSS and Multics.","<p><strong>Fernando José Corbató, known to everyone as “Corby”, was born July 1, 1926 in Oakland, California.</strong> His parents were graduate students at the University of California, Berkeley. When Corby was five years old his family moved to Los Angeles, where his father became a professor of Spanish literature at UCLA. World War II broke out while Corby was in high school in Los Angeles, so he graduated early and enlisted in the US Navy at age 17, becoming an electronics technician. After the war he entered the California Institute of Technology and received a BS in Physics in 1950.</p>
<p>Corby then went to the Massachusetts Institute of Technology for graduate school. While a graduate assistant at MIT, he was encouraged by Prof. Philip M. Morse to become expert in the use of the <a href=""/info/corbato_1009471.cfm#link_1"">Whirlwind</a> computer for physics computations. One project he worked on was the computation of wave functions, later published in a book by MIT Press [<a href=""/bib/corbato_1009471.cfm"" name=""link_1"">1</a>].</p>
<p>After receiving his Ph.D. in Physics in 1956, Corby became a member of Prof. Morse's research staff in the newly formed MIT Computation Center, which then had an <a href=""/info/corbato_1009471.cfm#link_2"">IBM 704</a> computer. He served as Deputy Director of the Computation Center from 1958 to 1965. As computer use increased rapidly at MIT in the late 1950s, Corby became familiar with their limitations.&nbsp;</p>
<p>Many users were dissatisfied with the way access to computers was managed. Computers like Whirlwind were used by one person at a time, and required signups in advance.&nbsp; The Computation Center's IBM 704 was run in “batch mode”, where professional operators ran previously-submitted computational jobs. Both approaches required the programmer to wait for hours or days, and to wait again if the job had to be re-run due to an error.</p>
<p>A new idea called ""time-sharing"" was proposed at the end of the 1950s by several people, among them MIT Profesor <a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy</a> (Turing Award winner in 1971). Several users would be connected to the computer at the same time.&nbsp; It would switch rapidly between their programs, running one for a short time, then another. Each user would interact with his or her program using a Teletype-like device called a <a href=""/info/corbato_1009471.cfm#link_5"">terminal</a>.</p>
<p>In order for time-sharing to work, the computer would have to be able to interrupt a running job, save its state, find and restore another job, and start it up where it had been interrupted. Process control computers had such abilities at the time, but scientific machines like the Computation Center's IBM 709 did not. Furthermore, the supervisor program that switched between users had to be protected from being overwritten by a misbehaving user program.</p>
<p>In 1961, Corby proposed a small project on the IBM 709 that would demonstrate such interactive computing. With Bob Daley and Marjorie Merwin-Daggett, he built the initial version of the Compatible Time-Sharing System (CTSS) and demonstrated it in November of 1961.&nbsp; It saved suspended programs onto four magnetic tape drives, providing simultaneous access for four time-sharing users who used modified <a href=""/info/corbato_1009471.cfm#link_5"">Flexowriter</a> terminals. CTSS was described in a paper presented by Corby, Merwin-Daggett and Daley at the 1962 Spring Joint Computer Conference [<a href=""/bib/corbato_1009471.cfm#link_2"">2</a>].</p>
<p>CTSS was ""compatible"" in the sense that binary object programs that ran in batch mode could also be run interactively, and also in the sense that traditional batch processing could share the computer with time-sharing users. The system provided each user with a virtual <a href=""/info/corbato_1009471.cfm#link_3"">IBM 709</a> computer that could execute user programs and system commands in response to command lines entered at a terminal.</p>
<p>CTSS development continued throughout 1962 and 1963. The Computation Center upgraded to an IBM 7090 (which used transistors instead of vacuum tubes) and added disks, drums, and an IBM 7750 terminal controller to its configuration.&nbsp; The computer also had special modifications from IBM to support time-shared operation, and to protect the supervisor from errant user programs. CTSS was accessible from telephone dial-up terminals located in MIT offices and in the homes of researchers and staff members. CTSS evolved rapidly, and a manual for the system was published by MIT Press in 1963 [<a href=""/bib/corbato_1009471.cfm#link_3"">3</a>].</p>
<p>In the Fall of 1962, J.C.R. Licklider and MIT Prof. Robert M. Fano initiated Project MAC, an interdepartmental laboratory at MIT focused on time-sharing and funded by the Advanced Research Projects Agency of the US Department of Defense. An important initial activity of Project MAC was a 1963 Summer Study, where invited computer scientists from many institutions came to MIT and used CTSS. It demonstrated that time-sharing not only used computers more efficiently, it also helped build a community of users. The Summer Study also motivated the MIT programmers to make CTSS a reliable service, not just an experiment.</p>
<p>In the fall of 1963, Project MAC acquired an IBM 7094 and CTSS began operating as a general service for MIT research projects.&nbsp; The MIT Computation Center also upgraded its machine from an <a href=""/info/corbato_1009471.cfm#link_4"">IBM 7090</a> to an <a href=""/info/corbato_1009471.cfm#link_4"">IBM 7094</a>, and continued to provide both CTSS and batch operation for academic users.</p>
<p>The plan for Project MAC included the development of a second-generation replacement for CTSS called Multics: Multiplexed Information and Computing Service. It was to be a prototype of a ""computer utility"" that provides computing and storage service to a large user community. Project MAC selected General Electric as the computer vendor, and enlisted Bell Telephone Laboratories to participate in the software design and implementation. CTSS was used as the tool for developing the new system.</p>
<p>A special session at the 1965 Fall Joint Computer Conference was devoted to six papers describing Multics, written jointly by authors from MIT, General Electric, and Bell Laboratories [<a href=""/bib/corbato_1009471.cfm#link_4"">4</a>]. Multics was to be implemented on a new computer, the <a href=""/info/corbato_1009471.cfm#link_6"">General Electric 645</a>, which provided architectural support specifically for time-sharing.</p>
<p>Besides its utility-like orientation, the most significant feature of Multics was the integration of memory <a href=""/info/corbato_1009471.cfm#link_7"">segmentation and paging</a> (both of which existed in other systems) into a virtual memory system that provided a single level store for a shared memory multiprocessor. The virtual memory was organized into rings of protection, and special attention was given to making the system secure. Multics was implemented in the high-level language PL/I. Many industry watchers regarded the project as too ambitious.</p>
<p>Corby was chosen to lead Project MAC's Computer System Research group. He managed a group of about 30 people, coordinated with similar sized teams from Bell Laboratories and General Electric, which established design directions and priorities for Multics. One of his important contributions was creating a development culture that emphasized open communication, thorough review, and iterative improvement. He led Multics development from 1963 until the mid-1970s.</p>
<p>Corby was appointed Associate Professor of Electrical Engineering at MIT in 1962, and Professor in 1965. Prof. Corbató served as Associate department head for Computer Science and Engineering in the MIT Electrical Engineering and Computer Science Department for the years 1974-1978 and 1983-1993. He also served as MIT Director of Information Services and MIT Network Czar in the mid-70s. He retired in 1996.</p>
<p>CTSS ran at Project MAC until 1968, and at the MIT Computation Center until 1973. Multics became available for general use at MIT in October 1969, and Multics systems were sold by General Electric and Honeywell from 1970 to 1987. The last Multics system was shut down in 2000.</p>
<p>Bell Laboratories withdrew from the Multics project in 1969.&nbsp; Former Multics contributors <a href=""/award_winners/thompson_4588371.cfm"">Ken Thompson</a>&nbsp; and <a href=""/award_winners/ritchie_1506389.cfm"">Dennis Ritchie</a> began developing a simple and elegant operating system called UNIX, described as ""one of whatever Multics was many of."" UNIX was influenced by both CTSS and Multics.&nbsp; Ritchie later wrote,<br>
""In most ways UNIX is a very conservative system. Only a handful of its ideas are genuinely new. In fact, a good case can be made that it is, in essence, a modern implementation of MIT’s CTSS system. This claim is intended as a compliment to both UNIX and CTSS. Today, more than fifteen years after CTSS was born, few of the interactive systems we know of are superior to it in ease of use; many are inferior in basic design."" [<a href=""/bib/corbato_1009471.cfm#link_8"">8</a>]</p>
<p>Corbató and his late wife Isabel had two daughters, Carolyn and Nancy.&nbsp; He also has two stepsons, David and Jason Gish, with his current wife Emily.</p>
<p style=""text-align: right;""><span class=""callout"">Author: T. Van Vleck</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/corbato_1009471.cfm""><img src=""/images/lg_aw/1009471.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Fernando Corbato""></a>
<br><br>
<h6 class=""label""><a href=""/photo/corbato_1009471.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>July 1, 1926 in Oakland, California</p>
<h6>DEATH:</h6>
<p>July 19, 2019 in Newburyport, Massachusetts&nbsp;</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Attended High school and UCLA in Los Angeles; BS in Physics from the California Institute of Technology, 1950; Ph.D. in physics from the Massachusetts Institute of Technology, 1956</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Joined the US Navy in WWII as an electronics technician; MIT Computation Center research staff under Prof. Philip M. Morse; led the development of Compatible Time-Sharing System (CTSS) from 1961-1965; Deputy Director, MIT Computation Center; led the development of Multics at MIT Project MAC (Laboratory for Computer Science) from 1964-1974; Associate department head, MIT EECS department (1974-1978 and 1983-1993); retired from MIT (1996).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>ACM National Lecturer (1964);&nbsp;<span style=""line-height: 20.8px;"">IEEE W.W. McDowell Award for his work in the development of time-sharing systems (1966);&nbsp;</span><span style=""line-height: 1.3;"">member of the Computer Science and Engineering Board of the National Academy of Science (1970-1973); Fellow of the Institute of Electrical and Electronics Engineers (1975); Fellow of the American Academy of Arts and Sciences (1975); elected to the National Academy of Engineering (1976); American Federation of Information Processing Societies Harry Goode Memorial Award (1980);&nbsp;</span><span style=""line-height: 20.8px;"">Fellow of the American Association for the Advancement of Science (1982);</span><span style=""line-height: 20.8px;"">&nbsp;</span><span style=""line-height: 1.3;"">charter recipient of the IEEE Computer Society Computer Pioneer Award (1982); ACM Alan M. Turing Award (1990); ACM Fellow (1994); NEC Corporation Foundation Computers &amp; Communication (C&amp;C) Prize (1998); Fellow of the Computer History Museum (2012).</span></p>","","https://dl.acm.org/author_page.cfm?id=81100474167","Fernando J (""Corby"") Corbato","<li class=""bibliography""><a href=""/bib/corbato_1009471.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283947&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/corbato_1009471.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/corbato_1009471.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/corbato_1009471.cfm""><span></span>Video Interview</a></li>"
"1573179675-700","https://amturing.acm.org/award_winners/pnueli_4725172.cfm","For seminal work introducing temporal logic into computing science and for outstanding contributions to program and system verification.","<p>Amir Pnueli (pronounced: p’new-EL-ee) was born on April 22, 1941, in Nahalal, Israel. His parents, Henya and Prof. Shmuel Yeshayahu (“Shay”) Pnueli, immigrated to Israel, which was then Palestine, in 1936. They settled in Nahala, a cooperative agricultural community, where Shay Pnueli was the principal of the local school and Henya Pnueli was a teacher. In 1945, when Shay was appointed to teach in a teachers' college in the kibbutz Giva'at Hashlosha, the family relocated to Hulon. In the 1960s Prof. Shay Pnueli became one of the founders of Tel-Aviv University, and chaired the Hebrew literature department there until his death in 1965. Henya Pnueli continued teaching in Gordon school at Hulon until she died at the age of 75 in 1996.</p>
<p>Amir attended Kugel high school in Hulon, and then Tichon Hadash in Tel Aviv. He was active in the Ha'Tnuah H'meuchedet, an Israeli youth movement affiliated with the labour party whose focus was on collaboration between academics and labour. Although Amir showed immense promise for both the humanities (which he attributed to his father) and mathematics (which he attributed to his mother), he decided to focus on mathematics, because, as he later claimed, he did not wish to compete with his accomplished father. His brother, Prof. David Pnueli (1933-2001), was an accomplished physicist who taught fluid mechanics at the Technion, the Israel Institute of Technology.</p>
<p>Amir studied mathematics at the Technion during 1958-1962, graduating summa cum laude. Towards the end of his undergraduate studies he was introduced to the <a href=""https://en.wikipedia.org/wiki/WEIZAC"" target=""_blank"">WEIZAC</a>, the first computer in Israel, by Prof. Philip Rabinowitz and, according to his family, “that's where his love affair with computers started.”</p>
<p>He continued directly to PhD studies—highly unusual in Israel at the time where an MSc was generally required for PhD studies—in the Weizmann Institute of Science in Israel. Under the supervision of <a href=""https://en.wikipedia.org/wiki/Chaim_L._Pekeris"" target=""_blank"">Chaim Pekeris</a>, he wrote a dissertation on <em>Calculation of Tides in the Ocean</em>, also called <em>Tides in Simple Basins</em><a href=""#_ftn1"" name=""_ftnref1"" title="""">[1]</a> in 1967. In addition to being a world-renowned geophysicist, Prof. Pekeris was one of the initiators of the WEIZAC project, and Amir used it for many of the calculations in his PhD dissertation.</p>
<p>During 1967 and 1968 Amir was a postdoctoral fellow at Stanford University and at IBM research center in Yorktown Heights, New York. At Stanford he started a fruitful collaboration with <a href=""https://en.wikipedia.org/wiki/Zohar_Manna"" target=""_blank"">Zohar Manna</a> on the area now known as <em>formal methods</em>. His initial work was on formalizing recursive functions and functional programs. The collaboration with Prof. Manna resulted in numerous research papers and three books [<a href=""/bib/pnueli_4725172.cfm#bib_7"">7</a>].</p>
<p>From 1969 to 1973 Amir was a senior researcher in the department of applied mathematics at the Weizmann Institute, where he helped <a href=""https://en.wikipedia.org/wiki/Shimon_Even"" target=""_blank"">Shimon Even</a> found the graduate program.</p>
<p>During that time he founded, with Ido and Hagi Lochover, the company Mini Systems Ltd, which provided software for the computer-aided design systems manufacturer Scitex. The company was sold to Scitex in 1984.</p>
<p>Amir left the Weizmann Institute to found, and then chair, the department of computer science at Tel Aviv University, where he stayed until 1980. It was during that period that Amir got deeply involved in logics and deductive methods. During a sabbatical at the University of Pennsylvania he was introduced to the work of the philosopher <a href=""https://en.wikipedia.org/wiki/Arthur_Prior"" target=""_blank"">Arthur Prior</a>, who had developed “tense logic” to evaluate statements whose truthfulness changes over time. Amir was the first to realize the potential implications of applying Prior's work to computer programs. Amir’s 1977 seminal paper “The Temporal Logic of Programs” [<a href=""/bib/pnueli_4725172.cfm#bib_1"">1</a>] revolutionized the way computer programs are analyzed. At the time, practical program verification was widely considered to be hopeless. The main methodologies considered all possible pairs of program states. Amir's paper introduced the notion of reasoning about programs as execution paths, which breathed new life into the field of program verification.</p>
<p>To quote Amir from his talk after receiving the Israel Prize:</p>
<p style=""margin-left:.5in;"">“In mathematics, logic is static. It deals with connections among entities that exist in the same time frame. When one designs a dynamic computer system that has to react to ever changing conditions, … one cannot design the system based on a static view. It is necessary to characterize and describe dynamic behaviors that connect entities, events, and reactions at different time points. Temporal Logic deals therefore with a dynamic view of the world that evolves over time."" (Translated from the original Hebrew)</p>
<p>Amir and several of his colleagues returned to the Weizmann Institute in 1980, and together they built the Department of Computer Science into one of the leading departments in the world. He was appointed to the Estrin Chair in November 2000. In 2000 Amir was awarded the Israel Prize in field of Computer Science, “for his breakthrough contribution in the verification of parallel and reactive systems by the introduction of the specification language of Temporal Logic, and the development of methods and algorithms for verifying the correctness of reactive programs and systems.” In 2001, Amir was elected to the Israeli Academy of Science.</p>
<p align=""right""><span class=""callout"">Author: Lenore Zuck</span></p>
<div><br clear=""all"">
<hr align=""left"" size=""1"" width=""33%"">
<div id=""ftn1"">
<p><a href=""#_ftnref1"" name=""_ftn1"" title="""">[1]</a> Amir himself has translated the title of his dissertation in two different ways in his curriculum vita.</p>
</div>
</div>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/pnueli_4725172.cfm""><img src=""/images/lg_aw/4725172.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Amir Pnueli""></a>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>April 22, 1941, Nahalal, Israel.</p>
<h6><span class=""label"">DEATH: </span></h6>
<p>November 2, 2009 (aged 68) New York, USA.</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>B.Sc. (with distinction) Mathematics (Technion, 1962); Ph.D (with distinction), Applied Mathematics (Weizmann Institute of Science, 1967), and three honorary degrees.</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Instructor, Computer Science Department, Stanford University (1967); Summer Visitor, Watson Research Center (I.B.M) Yorktown Heights, N.Y (1968); Research Fellow, Weizmann Institute of Science (1969-1970); Visitor, AI Project, Stanford University (Summer 1970); Senior Research Fellow, Department of Applied Mathematics Weizmann Institute of Science (1970-1973); Associate Professor (and Chairman), Computer Science Division Tel Aviv University (1973-1979); Visiting Associate Professor, The Moore School of Engineering University of Pennsylvania (1976-1978); Professor, Department of Applied Mathematics, Weizmann Institute of Science (1980-2009); Visiting Professor (on leave from Weizmann) Harvard University (1982-1983); A City of Grenoble (visiting) Municipal Chair at the Verimag Laboratory, Universite Joseph Fourier, Grenoble (1994-1997); Professor, Department of Computer Science, Courant Institute, New York University (1999-2009).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>ACM Turing award (1996); honorary doctorate from the University of Uppsala, Sweden (1997); honorary doctorate from Universite Joseph Fourier, Grenoble, France (1998); elected foreign associate of the (American) National Academy of Engineering (1999); Israel Prize, category of exact sciences (2000); honorary doctorate from Carl von Ossietzky Universitaet Oldenburg, Germany (2000); elected to the Israeli Academy of Sciences (2001); elected member of the European Academy of Sciences (EAS) (2004); elected member of Academia Europaea, Informatics Section (2006); Fellow of the ACM (2007). The Weizmann Institute of Science presents a memorial lecture series in his honor.</p>","","https://dl.acm.org/author_page.cfm?id=81100648459","Amir Pnueli","<li class=""bibliography""><a href=""/bib/pnueli_4725172.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=259407&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/pnueli_4725172.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179278-674","https://amturing.acm.org/award_winners/adleman_7308544.cfm","Together with Ronald Rivest and Adi Shamir, for their ingenious contribution to making public-key cryptography useful in practice.","<p style=""margin-left:.5in;"">
<em>Professor Len Adleman had just finished a personal anecdote about the development of the RSA public-key cryptosystem and DNA computing. The roomful of University of Southern California Computer Science students took advantage of the casual atmosphere to pick the brain of one of the department’s most revered faculty members.</em></p>
<p style=""margin-left: 0.5in;"">
<em>A student asked, ""Given your past work, what is your current research project and objective?""</em></p>
<p style=""margin-left:.5in;"">
<em>Len paused, ""I am working on a new approach to Complex Analysis called Strata. I want to add a brick—even a small brick—to the wall of Mathematics.""</em></p>
<p style=""margin-left:.5in;"">
<em>""Isn't RSA already a brick in that wall?"" the student continued.</em></p>
<p style=""margin-left:.5in;"">
<em>Len reflected, ""It is not close enough to the foundation where the bricks of Gauss, Riemann, and Euler lay.""</em></p>
<p>
<strong>Leonard Max Adleman was born December 31, 1945 in San Francisco, California, to a bank teller and an appliance salesman.</strong> Admitted to the University of California in Berkeley with the intention of becoming a chemist, he finally graduated with a BS in Mathematics in 1968. After a brief stint as a computer programmer, he returned to UC Berkeley. His concurrent interests in Mathematics and Computer Science ultimately led him to his Ph.D. thesis in 1976, <em>Number-Theoretic Aspects of Computational Complexity</em>, under the inspiring guidance of <a href=""/award_winners/blum_4659082.cfm"">Manuel Blum</a>, the recipient of the 1995 Turing Award.</p>
<p>
Len quickly secured positions as an Assistant, then Associate, Professor of Mathematics at the Massachusetts Institute of Technology. His collaboration with fellow Turing Awardees <a href=""/award_winners/rivest_1403005.cfm"">Ron Rivest</a> and <a href=""/award_winners/shamir_0028491.cfm"">Adi Shamir</a> led to the development of the RSA public-key cryptosystem and the 1978 publication of their seminal paper, ""A Method for Obtaining Digital Signatures and Public-Key Cryptosystems” [<a href=""/bib/adleman_7308544.cfm#bib_1"">1</a>].&nbsp; RSA, an acronym for Rivest, Shamir and Adleman, uses <a href=""https://en.wikipedia.org/wiki/Computational_number_theory"" target=""_blank"">algorithmic number theory</a> to provide an efficient realization of a public-key cryptosystem, a concept first envisioned theoretically by <a href=""https://en.wikipedia.org/wiki/Whitfield_Diffie"" target=""_blank"">Whitfield Diffie</a>, <a href=""https://en.wikipedia.org/wiki/Martin_Hellman"" target=""_blank"">Martin Hellman</a> and <a href=""https://en.wikipedia.org/wiki/Ralph_Merkle"" target=""_blank"">Ralph Merkle</a>. RSA is now the most widely used encryption method, with applications throughout the Internet to secure on-line transactions. It has also inspired breakthrough work in both theoretical computer science and mathematics.</p>
<p>
Algorithmic number theory, and in particular the problem of testing for prime numbers, has been a longtime research focus of Adleman. With <a href=""https://en.wikipedia.org/wiki/Carl_Pomerance"" target=""_blank"">Carl Pomerance</a> and <a href=""https://en.wikipedia.org/wiki/Adleman%E2%80%93Pomerance%E2%80%93Rumely_primality_test"" target=""_blank"">Robert Rumely</a>, he developed an ""almost"" polynomial time deterministic primality testing algorithm. The paper [<a href=""/bib/adleman_7308544.cfm#bib_2"">2</a>] describing this Adleman-Pomerance-Rumely primality test appears to be the first paper published in the prestigious journal <em>Annals of Mathematics</em> on a topic in theoretical computer science.</p>
<p>
Drawn to beautiful Southern California, Len joined the faculty at the University of Southern California (USC) in 1980, where he is now the Henry Salvatori Professor of Computer Science and Professor of Molecular Biology. In 1987, he and his USC colleague <a href=""http://64.238.147.56/author_page.cfm?id=81339505923&amp;coll=DL&amp;dl=GUIDE"" target=""_blank"">Ming-Deh Huang</a> described the first “Las Vegas” randomized algorithm for primality testing in a landmark paper titled ""Recognizing Primes in Random Polynomial Time."" [<a href=""/bib/adleman_7308544.cfm#bib_3"">3</a>] This result was the last major theoretical breakthrough in a long line of work before the current high point in primality testing, ""<a href=""https://en.wikipedia.org/wiki/AKS_primality_test"" target=""_blank"">PRIMES is in P</a>"" by <a href=""https://en.wikipedia.org/wiki/Manindra_Agrawal"" target=""_blank"">Manindra Agrawal</a>, <a href=""https://en.wikipedia.org/wiki/Neeraj_Kayal"" target=""_blank"">Neeraj Kayal</a>, and <a href=""https://en.wikipedia.org/wiki/Nitin_Saxena"" target=""_blank"">Nitin Saxena</a> in 2002.</p>
<p>
Len also worked on <a href=""https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem"" target=""_blank"">Fermat's Last Theorem</a>, and in 1986, with colleagues <a href=""https://en.wikipedia.org/wiki/Roger_Heath-Brown"" target=""_blank"">Roger Heath-Brown</a> and <a href=""https://de.wikipedia.org/wiki/%C3%89tienne_Fouvry"" target=""_blank"">Etienne Fouvry</a>, proved that the first case of the theorem holds for infinitely many primes [<a href=""/bib/adleman_7308544.cfm#bib_5"">5</a>] . This was a respectable result at the time, but became a footnote following <a href=""https://en.wikipedia.org/wiki/Andrew_Wiles"" target=""_blank"">Andrew Wiles</a> famous proof of Fermat’s Last Theorem in 1995.</p>
<p>
Len is also associated with the creation of an early computer virus, demonstrated in 1983 by his student Fred Cohen, who credited Len for coining the term ""computer virus"" to describe the self-replicating programs. In the 1980's his research took an interdisciplinary turn, from computer viruses to biological ones. With David Wofsy of University of California at San Francisco, he developed a theory of CD4-cell depletion in Acquired Immune Deficiency Syndrome (AIDS) as a homeostatic mechanism failure, and published several papers on the topic.</p>
<p>
His professional interest in biology increased, and was further inspired by James Watson's book <em>The Molecular Biology of the Gene</em>. In a moment of clarity, he noticed the resemblance between the way the protein polymerase produces complementary strands of DNA, and the mechanism of the Turing machine. Len saw the biochemical processes of the cell as computation. Like a Turing machine that runs along a tape processing symbolic information, polymerase runs along a strand of DNA processing chemical information.</p>
<p>
Great scientific breakthroughs sometimes arise from the realization that two seemingly unrelated fields are, in fact, related. In this regard, Len proved to be more than just a brilliant theoretician. By encoding a small instance of the NP-complete <a href=""https://en.wikipedia.org/wiki/Hamiltonian_path"" target=""_blank"">Hamiltonian Path</a> problem in strands of DNA and then experimentally computing its solution, Len created what is probably the first computational device at a molecular scale. For this work [<a href=""/bib/adleman_7308544.cfm#bib_4"">4</a>] Len has been widely credited as the ""Father of DNA Computing.""</p>
<p>
More recently, Len has returned to what he sees as the most beautiful of endeavors: mathematics. Along with students, he investigated analogies between chemistry and mathematics, and developed event systems as a mathematical version of the <a href=""https://en.wikipedia.org/wiki/Law_of_mass_action"" target=""_blank"">law of mass action</a><del cite=""mailto:Len"" datetime=""2012-05-14T10:20"">.</del> in chemistry. This led to further study in complex analysis, where he and his students are currently developing a theory of ""Strata"" to describe multi-valued analytic functions.</p>
<p>
Len Adleman is a unique and talented interdisciplinary scholar. His accomplishments in multiple fields have been driven by remarkable insight, curiosity, and persistence. Len is an inspiring teacher from whom both authors of this essay had the good fortune to take multiple courses.</p>
<p>
He is also an intriguing person when away from the academy. Unable to resist the allure of Hollywood, he served as a mathematical and cryptography consultant for the film <em>Sneakers.</em> He enjoys discussing <em>Memes</em>, the theory of information evolution developed by Richard Dawkins. He converses regularly about history, art, music and culture, and is a mesmerizing storyteller. Perhaps in preparation for lifting his brick into the wall of mathematics, he has whipped himself into physical shape as an amateur boxer who has been in the ring with the likes of ten-time world champion James Toney.</p>
<p align=""right"">
<span class=""callout"">Authors: Joseph Bebel and Shang-Hua Teng</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/adleman_7308544.cfm""><img src=""/images/lg_aw/7308544.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Leonard M. Adleman""></a>
<br><br>
<h6 class=""label""><a href=""/photo/adleman_7308544.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH:</span></h6>
<p>December 31, 1945 in San Francisco, California</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>BA, Mathematics (University of California, Berkley, 1968); PhD, Computer Science (University of California, Berkley, 1976).</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>Massachusetts Institute of Technology, Department of Mathematics (1979-1980 Associate Professor, 1977-1979 Assistant Professor); University of Southern California (1980 Associate Professor, 1983 Professor, 1985 Henry Salvatori Professor).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>ACM Paris Kanellakis Theory and Practice Award (1996); IEEE Kobayashi Award for Computers and Communications (2000-joint with Rivest and Shamir); Distinguished Professor title University of Southern California (2000); ACM Turing Award (2002); Fellow of the American Academy of Arts and Sciences (2006); member of the National Academy of Engineering (1996) and the National Academy of Sciences.<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100129131","Leonard (Len) Max Adleman","<li class=""bibliography""><a href=""/bib/adleman_7308544.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/adleman_7308544.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/adleman_7308544.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/adleman_7308544.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/adleman_7308544.cfm""><span></span>Video Interview</a></li>"
"1573178934-650","https://amturing.acm.org/award_winners/dahl_6917600.cfm","With Kristen Nygaard, for ideas fundamental to the emergence of object oriented programming, through their design of the programming languages Simula I and Simula 67.","<p>
<strong>Object-oriented programming is a dominant programming paradigm of this age.</strong> Fundamental to the emergence of this paradigm were core concepts such as objects, classes, and inheritance with virtual quantities, all clearly established in Ole Johan Dahl and Kristen Nygaard's discrete event simulation language Simula I and general programming language Simula 67. The objects integrate data, procedural, and cooperating action sequence aspects into one very general and powerful unifying entity.</p>
<p>
By embodying these core concepts in a language designed both for system description and programming, Dahl and Nygaard provided both a logical and a notational basis for the ideas. Software could be built in layers of abstraction, each one relying on the description and conceptual platform implemented by the previous layers. By defining Simula 67 to be an extension of an international standard language, Algol-60, this medium of expression was accessible and available to the entire research community. Simula shaped and sped the emergence of object-oriented programming and the management discipline that accompanies it by many years.</p>
<p>
Ole-Johan Dahl was born on 12th October 1931 in Mandal, a small town on the south coast of Norway. Although now a resort, the residents of Mandal had historically looked to the sea for their livelihoods, and Dahl was descended on both sides from long lines of sea captains and sailors. It soon became evident that Dahl was not to continue that tradition, because his early interests were reading, mathematics, and playing the piano. His mother was a housewife, and neither his sister nor his brother was academically inclined.</p>
<p>
When he was seven, his family moved to Drammen, south of Oslo. When he was thirteen, his elder cousin was shot dead by the Nazis and the whole family fled to Sweden. Consequently he missed the final year of elementary school and started directly in high school after taking the entrance examination as an external candidate. Because he was able to help his teacher explain mathematics to the other pupils, he was soon nicknamed “the professor.”</p>
<p>
Dahl heard classical music for the first time when he was 3 years old and was captivated by it. His passion for music stayed with him throughout his life, but it seems that he decided quite early to make his career elsewhere. He chose an academic career focused on mathematics, because music was a personal and private affair used to enrich his life and those of his friends, rather than a field to be used as a profession.</p>
<p>
Dahl studied numerical mathematics at the University of Oslo. During his time at the University he also worked part-time at the Norwegian Defense Research Establishment (NDRE), to which he was assigned in 1952 for his compulsory military service. He continued to work there full-time after he graduated. It was at NDRE that he was first introduced to computers. He was also fortunate to fall under the influence of Jan Garwick (often called the “father of informatics” in Norway), who was able to stimulate and nurture Dahl's talents. In 1954 Dahl became Garwick's assistant.</p>
<p>
By 1957 the NDRE had obtained an early <a href=""https://en.wikipedia.org/wiki/Ferranti_Mercury"" target=""_blank"">Ferranti Mercury computer</a>. Dahl designed and implemented what was then considered to be a high-level language for the Mercury, called MAC (Mercury Automatic Coding). Dahl's university degree, while officially in the area of numerical analysis, was actually about computer science: the title of his thesis was “Multiple Index Countings on the Ferranti Mercury Computer.”</p>
<p>
At NDRE Dahl also encountered <a href=""/award_winners/nygaard_5916220.cfm"">Kristen Nygaard</a>, and the partnership between these two men was to change the face of computing. Nygaard had been working on calculations related to the diameter of the uranium rods for Norway's first nuclear reactor. In 1949 NDRE started using Monte Carlo simulation, with the calculations performed by hand, instead of attempting to solve the equations exactly. “In that [simulation] model the physical paths and histories of a large number of neutrons were generated and a statistical analysis of their properties used to estimate the proper choice of rod diameter” [Nyggard 1986]. Later Nygaard applied the same approach to other problems, changing his focus to operational research, and in 1956 earning a Master of Science degree with a thesis on probability theory.</p>
<p>
In 1960 Nygaard moved to the Norwegian Computing Center (NCC), a semi-governmental research institute that had been established in 1958. His brief was to expand the NCC's research capabilities in computer science and operational research. He wrote “Many of the civilian tasks turned out to present the same kind of methodological problems [as his earlier military work]: the necessity of using simulation, the need of concepts and a language for system description, lack of tools for generating simulation programs'' [<a href=""/bib/dahl_6917600.cfm#link_7"">7</a>].&nbsp; In 1961 he started designing a simulation language as a way of attacking those problems.</p>
<p>
In January 1962 Nygaard wrote a letter describing his progress, addressed to Charles Salzmann, a French specialist in operational research. Nygaard wrote:</p>
<p style=""margin-left:28.0pt;"">
<em>The status of the Simulation Language (Monte Carlo Compiler) is that I have rather clear ideas on how to describe queueing systems, and have developed concepts which I feel allow a reasonably easy description of large classes of situations. I believe that these results have some interest even isolated from the compiler, since the presently used ways of describing such systems are not very satisfactory. ... </em></p>
<p style=""margin-left:28.0pt;"">
<em>The work on the compiler could not start before the language was fairly well developed, but this stage seems now to have been reached. The expert programmer who is interested in this part of the job will meet me tomorrow. He has been rather optimistic during our previous meetings.</em> [<a href=""/bib/dahl_6917600.cfm#link_7"">7</a>].</p>
<p>
The “expert programmer” was Ole-Johan Dahl. Working with Nygaard, Dahl produced the initial ideas for <a href=""/info/dahl_6917600.cfm"">object-oriented programming</a>, which is now the dominant style of programming for commercial and industrial applications. Dahl joined the NCC in 1963 and stayed there until 1968, when he was invited to become a full professor at the University of Oslo</p>
<p>
The languages that Dahl and Nygaard developed together were, first (1962-1964) a simulation language called <a href=""https://en.wikipedia.org/wiki/Simula"" target=""_blank"">SIMULA</a>, now usually referred to as SIMULA I, and subsequently a general-purpose language called <a href=""https://en.wikipedia.org/wiki/Simula"" target=""_blank"">SIMULA 67</a>.</p>
<p>
SIMULA I was intended to be used both for describing complex systems and for programming simulations of their behavior. Nygaard wrote:</p>
<p style=""margin-left:28.0pt;"">
<em>SIMULA [I] should give its users a set of concepts in terms of which they could comprehend the system considered, and a language for precise and complete description of its properties. It should thus be a tool both for the person writing the description and for people with whom he wanted to communicate about the system.</em></p>
<p style=""margin-left:28.0pt;"">
<em>At the same time this system description should, with the necessary input/output and data analysis information added, be compilable into a computer simulation program, providing quantitative information about the system’s behaviour. </em>[<a href=""/bib/dahl_6917600.cfm#link_8"">8</a>].</p>
<p>
Although designed as a simulation language, almost from the beginning SIMULA I was used not only for simulation but also for general-purpose programming. It introduced its users to the idea of organizing their programs as a system of interacting, executing components, and this idea proved useful for a wide range of applications. These interacting components became the “objects” of SIMULA 67.</p>
<p>
SIMULA 67 was designed from the beginning as a general-purpose language, but Dahl and Nygaard invented a mechanism (<a href=""/info/dahl_6917600.cfm#add_1"">class prefixing</a>) that made the simulation-specific features of SIMULA I available in SIMULA 67 as a special kind of library. Prefixing could be used in two rather different ways, which have given rise to two of the most important ideas in modern programming languages: <a href=""https://en.wikipedia.org/wiki/Inheritance_%28object-oriented_programming%29"" target=""_blank"">inheritance</a>, which makes it easy to reuse code in unanticipated ways, and <a href=""https://en.wikipedia.org/wiki/Modular_programming"" target=""_blank"">modules</a>, which are used for extending the vocabulary of a programming language.</p>
<p>
These ideas — <a href=""/info/dahl_6917600.cfm#add_2"">objects</a>, <a href=""/info/dahl_6917600.cfm#add_3"">inheritance</a>, and <a href=""/info/dahl_6917600.cfm#add_4"">modularity</a> — are among the major contributions of Dahl and Nygaard to the discipline of programming. SIMULA also contributed the <a href=""http://onlinelibrary.wiley.com/doi/10.1002/spe.4380120205/pdf"" target=""_blank"">process</a> concept, which enabled programmers to express activities going on concurrently. With wonderful economy, all of these ideas were realized as variant uses of a single linguistic mechanism, the <a href=""/info/dahl_6917600.cfm#add_5"">class</a>.</p>
<p>
Part of the success of Dahl and Nygaard in creating the ideas of object-orientation is clearly due to their extraordinary talents. But part is also due to their very differing backgrounds, which made every language feature proposed by one the subject of criticism by the other. One story has it that in the spring of 1967 a new employee told the switchboard operator in a shocked voice: “Two men are fighting violently in front of the blackboard in the upstairs corridor. What shall we do?” The operator came out of the office, listened for a few seconds and then said: “Relax. It's only Dahl and Nygaard discussing SIMULA.”</p>
<p>
After Dahl moved to the University of Oslo he deliberately stopped work on further development of SIMULA and took on the responsibility, almost single-handed, of building up computer science in Norway as an academic discipline. For the first 10 years he was the only professor of computer science at the University. He taught during the day, wrote textbooks at night, and supervised up to 20 graduate students at a time. He worked on programming methodology, and produced, with <a href=""/award_winners/hoare_4622167.cfm"">Tony Hoare</a>, a chapter called “Hierarchical Program Structures” that became part of the celebrated book “Structured Programming” [<a href=""/bib/dahl_6917600.cfm#link_2"">2</a>].</p>
<p>
Dahl's later work was influenced by Hoare's system for reasoning mathematically about programs. He started using and teaching these reasoning techniques, and felt that they would improve even informally-produced programs. In 1992 Dahl published <em>Verifiable Programming</em>, which includes many of his own research results [<a href=""/bib/dahl_6917600.cfm#link_9"">9</a>]. In the 1990s he returned to object-orientated programming through design of the ABEL language and his research on reasoning about object-oriented systems.</p>
<p>
Dahl died in 2002 after a long battle with Lymphatic Cancer. He and his wife Tove had two children, Fredrik and Ingrid.</p>
<p>
<strong>Ole-Johan Dahl on the Web</strong></p>
<p>
Many articles on Dahl are available on the world-wide web. Some of them have been used as sources in preparing this article.</p>
<p>
The <a href=""http://www.olejohandahl.info/old/"" target=""_blank"">homepage</a> for Ole-Johan Dahl.</p>
<p>
Bertrand Meyer, “<a href=""http://www.jot.fm/issues/issue_2002_09/eulogy.pdf"">In memory of Ole-Johan Dahl and Kristen </a><a href=""http://www.jot.fm/issues/issue_2002_09/eulogy.pdf"">Nygaard</a>”, and <a href=""http://www.jot.fm/issues/issue_2002_09/eulogy.pdf"" target=""_blank"">Nygaard's own eulogy for Dahl</a>.</p>
<p>
Virtual Exhibition. <a href=""http://cs-exhibitions.uni-klu.ac.at/index.php?id=2"" target=""_blank"">In People Behind Informatics</a>.</p>
<p>
ACM <a href=""https://www.acm.org/press-room/news-releases-2002/turing-award-2001/"" target=""_blank"">press release</a> about the 2001 Turing award</p>
<p style=""text-align: right;"">
<span class=""callout"">Author: Andrew P. Black</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/dahl_6917600.cfm""><img src=""/images/lg_aw/6917600.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Ole-Johan Dahl ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/dahl_6917600.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>12th October 1931 in Mandal, Norway</p>
<h6><span class=""label"">DEATH: </span></h6>
<p>29th June 2002 from Lymphatic Cancer at Asker, Norway</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>MS in Numerical Mathematics, University of Oslo (1957).</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>Military Service at Institute of Defense Research, Oslo, under Jan Garwick (1952–1963); Joined Nygaard at Norwegian Computing Center (1963); Professor at the University of Oslo (1968).</p>
<h6><span class=""label"">HONORS AND AWARDS:</span></h6>
<p>Rosing Prize, Norwegian Data Association (1999); Commander of the Order of Saint Olav, awarded by the King of Norway (2000); ACM Turing Award (2001); IEEE von Neumann Medal (2002);&nbsp;Association Internationale pour les Technologies Objets (AiTO) annually awards two prizes named in honor of Dahl and Nygaard.</p>","","https://dl.acm.org/author_page.cfm?id=81452606808","Ole-Johan Dahl","<li class=""bibliography""><a href=""/bib/dahl_6917600.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""key-words""><a href=""/keywords/dahl_6917600.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/dahl_6917600.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179120-663","https://amturing.acm.org/award_winners/milner_1569367.cfm","For three distinct and complete achievements:","<p style=""margin-left:10.0pt;""><strong>Working in challenging areas of computer science for twenty years, Robin Milner has established an international reputation for three distinct achievements, each of which has had a marked, important, and widespread effect on both the theory and practice of computer science.</strong> In addition, he formulated and strongly advanced <em>full abstraction</em>, the study of the relationship between denotational semantics (which formalizes the meaning of programs using abstract mathematical descriptions of behavior called <em>denotations</em>), and <em>operational</em> semantics (which models program execution).</p>
<p style=""margin-left:10.0pt;"">A key ingredient in all of his work has been his ability to combine deep insight into mathematical foundations of the subject with an equally deep understanding&nbsp; of practical and aesthetic issues, thus allowing the feedback of theory into practice in an exciting way. Further, his style of scholarship, rigor, and attention to detail sets a high example for all to follow.</p>
<p style=""margin-left:10.0pt;"">Milner was born into a military family residing on the South coast of England. He was awarded a scholarship to Eton College in 1947, and subsequently served in the Royal Engineers, attaining the rank of Second Lieutenant. He then enrolled at King's College, Cambridge, graduating in 1957. At Cambridge, Milner studied Mathematics (B.Sc.), then Moral Sciences (Philosophy) (Part II).</p>
<p style=""margin-left:10.0pt;"">His first encounter with computing, as an undergraduate in 1956, was uninspiring. He took a 10-day course in programming on <a href=""/award_winners/wilkes_1001395.cfm"">Maurice Wilkes</a>’ EDSAC (<a href=""https://en.wikipedia.org/wiki/Electronic_Delay_Storage_Automatic_Calculator"" target=""_blank"">Electronic Delay Storage Automatic Calculator</a>). His reaction was characteristically decisive: “Programming was not a very beautiful thing. I resolved I would never go near a computer in my life.” He found the activity of “writing one instruction after the other<em>”</em> to be “inelegant” and “arbitrary”.</p>
<p style=""margin-left:10.0pt;"">Robin was, however, interested in tools as prosthetic devices that serve to extend our reach. He was intensely practical, as evidenced by the wooden geometric shapes on the light-strings on his lamps, and the comprehensive collection of chisels, gimlets and awls arranged neatly in the utility room (a spotlessly clean workshop that also served as the laundry) in his Cambridge home. Robin later became fascinated with computers as tools. He realized, very early, that that they were not just mere calculating devices, but could become ‘tools of the mind’.</p>
<p style=""margin-left:10.0pt;"">Milner never studied for a PhD. After one year as a school teacher, and two years of national service in Suez, his next engagement with computers came at the computer manufacturer Ferranti, where he “<em>made sure that all the new machines they were selling were actually working</em>,” and also wrote parts of a compiler. In 1963 he moved to academia with a passionate belief that a deeper understanding of programming could form the basis for a more elegant practice.</p>
<p style=""margin-left:10.0pt;"">At City University, London, he wondered how question-answering in databases might relate to relational algebra, but “didn’t get very far with that.” Then, at Swansea University in Wales, he began to publish on “program schemes,” an early flowchart-like operational model of computer programs.</p>
<p style=""margin-left:10.0pt;"">Milner went to Oxford to hear lectures from <a href=""https://en.wikipedia.org/wiki/Christopher_Strachey"" target=""_blank"">Christopher Strachey</a> and <a href=""/award_winners/scott_1193622.cfm"">Dana Scott</a>, who had been thinking about computable functionals, an abstract mathematical model for the meaning (“<em>denotational</em><em> semantics</em>”) of a program, and about a logic of computable functionals (LCF) for reasoning about “<em>denotation</em>” objects. Milner said that getting interested in program verification and semantics was the real start of his research career. At Swansea he wrote his first automatic theorem prover, but was still “shattered with the difficulty of doing anything interesting”.</p>
<p style=""margin-left:10.0pt;"">He was then invited to Stanford, where he spent two years (1971-1973) as a research associate with <a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy’s</a> Artificial Intelligence Project. Milner was more interested in how human intelligence might be “amplified” than in what an artificial intelligence might achieve independently. Milner thought that machines might help humans apply the Scott-Strachey approach to denotational semantics to practical examples. The calculations required were laborious and error-prone, but many were mechanical and amenable to computerization.</p>
<p style=""margin-left:10.0pt;"">He started work, together with Whitfield Diffie, Richard Weyhrauch, and Malcolm Newey, on an implementation of Scott’s LCF which he described as</p>
<p style=""margin-left:.5in;""><em>…designed to allow the user interactively to generate formal proofs about computable functions and functionals over a variety of domains, including those of interest to the computer scientist - for example, integers, lists and computer programs and their semantics.</em></p>
<p style=""margin-left:10.0pt;"">This LCF <em>proof-assistant</em> included a sub-goaling facility that allowed the user to divide the search for a proof into searches for smaller subproofs, with the machine keeping track of how to put the pieces of the proof back together. It also included a powerful simplification mechanism that allowed the machine to shoulder the burden of performing the more routine calculations.</p>
<p style=""margin-left:10.0pt;"">The beauty of this architecture is that improvements in mechanization can easily be incorporated to build a more powerful assistant without invalidating existing proofs. Furthermore, patterns of proof can be coded as <em>tactics </em>that can be used in many proofs<em>. </em>Proof search is a combination of subgoaling and simplification steps. A tactic (an example might be <em>induction on n</em>) automates some combination of steps. Tactics allow common patterns to be reused; they are not simply a list of one instruction after the other. These ideas have themselves been reused, and built upon, to create proof assistants for a variety of logics.</p>
<p style=""margin-left:10.0pt;"">Milner was still dissatisfied with the difficulty of using the then-existing programming languages – he was programming in LISP – and with the possibility of error. How could one be sure that the system could not introduce an inconsistency, and hence be useless because it could prove anything? When he moved to Edinburgh, in 1974, he embarked on ambitious program to develop and implement a practical programming language that drew on LISP, but was also much influenced by Peter Landin’s <a href=""https://en.wikipedia.org/wiki/ISWIM"" target=""_blank"">ISWIM</a> (If you See What I Mean), an ‘abstract’ language that was never really implemented but which has clear mathematical foundations and operational semantics based on Landin’s ‘<a href=""https://en.wikipedia.org/wiki/ISWIM"" target=""_blank"">SECD</a>’ abstract machine.</p>
<p style=""margin-left:10.0pt;"">Milner’s new language was designed as a metalanguage (hence its name, <em>ML</em>) for the implementation of a new proof-assistant called <em>Edinburgh LCF</em>. It was a robust and efficient practical tool well-adapted for this task, and was defined with the mathematical clarity exemplified by Landin’s work. A further discussion of ML can be found <a href=""/info/milner_1569367.cfm"">here</a>.</p>
<p style=""margin-left:10.0pt;"">ML was way ahead of its time. It is built on clean and well-articulated mathematical ideas, teased apart so that they can be studied independently and relatively easily remixed and reused. ML has influenced many practical languages, including Java, Scala, and Microsoft’s F<sup>#</sup>. Indeed, no serious language designer should ignore this example of good design.</p>
<p style=""margin-left:10.0pt;"">Meanwhile, Milner continued to worry about the foundational issues of semantics, in particular the link between the properties of the mathematical denotation of a program, and its operational behavior. If two programs can be observed to behave differently, they must have different denotations, otherwise the semantics would be unsound. Ideally, the converse should hold: any two observationally indistinguishable programs should have the same denotation.</p>
<p style=""margin-left:10.0pt;"">However, <a href=""https://en.wikipedia.org/wiki/Gordon_Plotkin"" target=""_blank"">Gordon Plotkin</a> discovered a key limitation of sequential programming, which meant that the natural Scott-Strachey model for an ISWIM-like language could have two programs that are observationally indistinguishable, but have different denotations. Milner constructed the first <em>fully abstract</em> model in which two observationally equivalent programs must have the same denotation.</p>
<p style=""margin-left:10.0pt;"">Milner worried about modeling non-deterministic programs and concurrent processes, which Scott-Strachey semantics didn’t address conveniently. The advent of timesharing and networking made this a practical as well as a theoretical issue. A simple functional model of sequential computation does not account for the possibility of interference between two programs that share the same memory, nor does it account for the interactions in a client-server architecture, or the interactions between a system and its users.</p>
<p style=""margin-left:10.0pt;"">It is possible to create clever denotational models that account for such features, but Milner wanted a “semantic theory in which <em>interaction</em> or <em>communication</em> is the central idea.” His first concern was about “the choice of interpretation and of mathematical framework, a small well-knit collection of ideas, which justifies the manipulations of the calculus.” After a long period of reflection and experimentation, he settled on Robert M. Keller’s deceptively simple model of labeled transition systems with synchronized communication, to which he applied the notion of observational equivalence. This formed the basis for his1980 book <em>Calculus of Communicating Systems</em> (CCS) [<a href=""/bib/milner_1569367.cfm#link_2"">2</a>], which sets out a ‘general theory of concurrency’.</p>
<p style=""margin-left:10.0pt;"">But he was not satisfied. He observed that “there is still latitude for choice in the definition of observational equivalence,” and conjectured that CCS might be extended by allowing “labels to be passed as values in communication.”</p>
<p style=""margin-left:10.0pt;"">Shortly after the publication of CCS, Milner learnt of David Park’s notion of “bisimulation”, a subtle but compelling refinement of the notion of observational equivalence. Milner completely reworked his theory in a new book, <em>Communication and Concurrency</em> [<a href=""/bib/milner_1569367.cfm#link_3"">3</a>] in 1989. CCS is little-changed, but the whole theory, in particular the treatment of observational equivalence, is more elegant and less arbitrary.</p>
<p style=""margin-left:10.0pt;"">These three strands of his early works – proof, concurrency, and ML – are highlighted in Milner’s Turing Award citation. The award trophy, representing computing’s highest honor, sat inconspicuously in his kitchen between a vase and a bowl of fruit.</p>
<p style=""margin-left:10.0pt;"">An account that stopped at this point would not do justice to his legacy. In addition to making his own technical contributions to the field, Milner eloquently articulated a vision of Computer Science as an intellectual endeavor. His inaugural address for the Laboratory for Foundations of Computer Science at the University of Edinburgh asked, “Is Computing an Experimental Science?”, and argued that theories of computation should be tested against their impact on practice.</p>
<p style=""margin-left:10.0pt;"">In accepting his honorary doctorate from the University of Bologna in 1997, he reflected on the power of programming</p>
<p style=""margin-left:28.35pt;""><em>Every tool designed by man is a prosthetic device, and for every prosthetic device there is a means of control. Many tools are physical, and their control is manual. In contrast, computers are the most complex tools ever invented, and they are tools of the mind; the means of control is hardly muscular - it is primarily linguistic. Quite simply, the versatility of computers is exactly equal to the versatility of the languages by which we prescribe their behaviour, and this appears to be unbounded.</em></p>
<p style=""margin-left:10.0pt;"">and then articulated his vision of informatics</p>
<p style=""margin-left:28.35pt;""><em>Computing is not only about a computer's internal action; it is about the way a computer behaves as part of a larger system. The terms in which the computer behaviour is prescribed must harmonize with the way we describe information flow in such systems. Thus computing expands into informatics, the science of information and communication […] it can provide an exact view of the world which is not the province of any previously existing science.</em></p>
<p style=""margin-left:10.0pt;"">His technical work continued with this broader perspective. Working with Joachim Parrow and David Walker, he developed the π-calculus in 1992, in which labels are passed as values to create a calculus of communicating systems that can naturally express processes with changing structure. For Milner’s definitive account, see <em>Communicating and Mobile Systems: The π-calculus</em> [<a href=""/bib/milner_1569367.cfm#link_5"">5</a>].</p>
<p style=""margin-left:10.0pt;"">He was increasingly concerned about ensuring the impact of theory on practice.&nbsp; For example, he contributed as an invited expert to the Web Services Choreography Description Language (WS-CDL); aspects of this business-process description language are inspired by the π-calculus.</p>
<p style=""margin-left:10.0pt;"">The calculus also led to further formal calculi for describing and reasoning about other systems, including the <em>spi calculus</em> for cryptographic protocols and the <em>stochastic π-calculus</em> for cellular signaling pathways. Such widely differing applications reinforced Milner’s vision of informatics as a science of information and interaction that can be applied in many domains. “Informatics,” he said, “is a synonym for computer science. The ‘info’ words have an advantage: they express the insight that informatic behavior is wider than what computers do, or what computing is.”</p>
<p style=""margin-left:10.0pt;"">Milner retired in 2001, but he did not stop. The π-calculus had been applied to new examples of interaction that he had not anticipated, but he now saw even more. For example, a</p>
<p style=""margin-left:.5in;"">“daunting range of concepts [is] needed to understand ubiquitous computing: <em>data provenance, privacy, authorization, trust (among agents), self-awareness, self-management, reconfiguration, failure recovery, goals, beliefs</em>, ..., and the list can be extended much further.”</p>
<p style=""margin-left:10.0pt;"">He embarked enthusiastically on the development of a further model, “a topographical model which aims to provide a theoretical foundation for mobile interactive systems”.</p>
<p style=""margin-left:10.0pt;"">This ambitious enterprise led to the “bigraphical"" model reported in his final book, <em>The Space and Motion of Communicating Agents</em> [<a href=""/bib/milner_1569367.cfm#link_7"">7</a>] in 2009. In it Milner seeks to account for agents with locality <em>and</em> connectivity that may be reconfigured through motion (placing) and interaction (linking). His goal is to model “not only interactive behavior among artificial agents, but also interaction with and among natural agents. Ultimately our informatic modelling should merge with, and enrich, natural science.”</p>
<p style=""margin-left:10.0pt;"">Bigraphs model the concurrent and interactive behaviors of mobile communicating agents. Applications include computational systems biology, where the agents include genes and proteins, and the internet, where the agents include people, computers, and programs. Like Milner's earlier work, this is destined to have far-reaching and profound significance, both within computer science and beyond.</p>
<p style=""margin-left:10.0pt;"">He was interviewed as ‘Geek of the Week’ just a few weeks before his death, and said, of his work on bigraphs,</p>
<p style=""margin-left:28.35pt;""><em>I would dearly love to follow this kind of work through to the front line of design and experience, but I don't think I'll be around for long enough. I'm making up for it by listening and talking to those who will be!</em></p>
<p style=""margin-left:10.0pt;"">Robin’s listening and talking stimulated various groups to work on bigraphical programming languages, and on the application of bigraphs to model natural systems. He returned part time to a chair at Edinburgh, still full of plans and projects for the future, when he died of a heart attack on 20th March 2010, just a few days after the funeral of his beloved wife Lucy.</p>
<p align=""right""><span class=""callout"">Author: Michael Fourman</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/milner_1569367.cfm""><img src=""/images/lg_aw/1569367.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""A J Milner ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/milner_1569367.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>13 January 1934 Yealmpton, near Plymouth, England;</p>
<h6><span class=""label"">DEATH: </span></h6>
<p>20 March 2010 in Cambridge, England</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>Eton; Kings College Cambridge 1954-’57 B.Sc. (mathematics) 1956; Part II (moral sciences) 1957.</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Second Lieutenant, Royal Engineers (1952-1954); Mathematics teacher, Marylebone Grammar School (1958-1959); Programmer, Ferranti, (1960-1962); Lecturer, The City University, (1963-1967); Researcher, Swansea University (1968-1970), Stanford University (1971-1972); Lecturer/Personal Chair (1984) Edinburgh (1973-1995); Chair, Cambridge University Computer Laboratory (1995-2001); Chair, Department of Computer Science, University of Edinburgh (2009–2010).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>British Computer Society Technical Award (1987); Distinguished Fellow of the British Computer Society (1988), Founding Member Academia Europaea (1988); Fellow of the Royal Society (1988); Turing Award (1991); Fellow of the Royal Society of Edinburgh (1993); Fellow of the ACM (1994); Royal Medalist of the Royal Society of Edinburgh (2004); Foreign Associate of the National Academy of Engineering (2008).</p>","","https://dl.acm.org/author_page.cfm?id=81332515695","Arthur John Robin Gorell (""Robin"") Milner","<li class=""bibliography""><a href=""/bib/milner_1569367.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283948&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/milner_1569367.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/milner_1569367.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179421-683","https://amturing.acm.org/award_winners/hellman_4055781.cfm","For inventing and promulgating both asymmetric public-key cryptography, including its application to digital signatures, and a practical cryptographic key-exchange method.","<h4 style=""color: rgb(102, 102, 102); font-family: Georgia, serif; font-size: 21.12px; background-color: rgb(255, 255, 255);"">&nbsp;</h4>
<p>Public-key cryptography pioneer and Stanford University Electrical Engineering Professor Emeritus&nbsp; Martin Edward Hellman was born in 1945 and grew up in the Bronx borough of New York City. His father was a high school physics teacher, whose influence and collection of books helped to inspire Hellman’s early interest in science and mathematics. In his leisure time as a youth, Hellman would play stickball on the streets of his largely immigrant neighborhood, or later, practice as a ham radio operator, getting his ham license his senior year of high school—a hobby that furthered his passion for electronics and electrical engineering. [4, 5]</p>
<p>In 1962 Hellman graduated from Bronx High School of Science and earned a Bachelor’s degree in Electrical Engineering from New York University in 1966. After graduating, Hellman enrolled in graduate school at Stanford University’s Electrical Engineering Department, where he earned his Master’s degree in 1967 and his Ph.D. in 1969. He married during his first year of graduate school. His dissertation, written under advisor Thomas Cover, was entitled, “Learning with Finite Memory.” &nbsp;</p>
<p>Hellman’s interest in cryptography developed in the background during his early career. In late 1968, having completed his dissertation, he began work at IBM’s Thomas J. Watson Research Center in Yorktown Heights, New York, where IBM had recently launched a cryptographic research program. Headed by German-born cryptographer Horst Feistel (formerly with the Air Force Cambridge Laboratory, Lincoln Laboratory, and MITRE) it included Alan G. Konheim (a subsequent leader of this research team) and other notable cryptographic scientists. This group’s research would lead to development of the Lucifer encryption system. &nbsp;Hellman, in the same department but working in other areas, ate lunch with Feistel who taught him about classical cryptographic systems and that “problems that sounded unsolvable could actually be solved very quickly.” [4]</p>
<p>Hellman learned more about cryptography during a&nbsp;talk by David Kahn, author of the <em>The Codebreakers: The Story of Secret Writing</em> (1967), at the 1969 IEEE International Symposium on Information Theory. Kahn’s book, a soon to be classic history of cryptography spanning ancient Egypt into the 1960s, had a significant impact on Hellman. [4, 7]</p>
<p>In September 1969 Hellman left IBM to teach in Electrical Engineering at MIT, joining a research group headed by noted information theorist Peter Elias. Elias passed along to Hellman a paper by Claude Shannon published in 1949 in the <em>Bell Systems Technical Journal</em>, alerting him to the fact that cryptography was “a branch of information theory.” [4] The paper was based on earlier, formerly classified, work Shannon did during the war, which Hellman&nbsp;points to as evidence of the influence of cryptography on Shannon's seminal 1948 paper on information theory.</p>
<p>Intrigued by California and Stanford in particular, Hellman left MIT in 1971 to join the faculty in Electrical Engineering at Stanford. &nbsp;At Stanford, Hellman moved gradually into the cryptography field, writing a technical report in 1973.&nbsp;IBM’s investment in cryptography had encouraged Hellman to believe that the area must be of commercial importance. In Hellman’s mind this had helped counter discouraging responses from academics about cryptography. Hellman later reflected that he was “working in a vacuum with discouragement from all my colleagues.” [4] One critique was that academics could not hope to “discover anything new” as the most advanced work was kept secret. To this, Hellman would reply it does not matter what is known <em>there </em>(the classified world), “it is not available for commercial use... the person who gets credit is the first to publish, not the first to discover and keep things secret.”&nbsp; Another argument was anything new that is “good” will be classified. &nbsp;He was&nbsp;further discouraged when he gave a talk at IBM Thomas J. Watson Research Center which included Horst Feistel, Alan Konheim and others. They told Hellman that IBM’s management was discouraging further research on cryptography (whose problems it thought had largely had been&nbsp;solved by the IBM group’s development of Lucifer and the soon-to-be released DES). Instead IBM was trying to refocus energy on the challenge of secure&nbsp;operating systems.</p>
<p>In 1974, at the suggestion of Alan Konheim, <a href=""https://amturing.acm.org/award_winners/diffie_8371646.cfm"">Whitfield Diffie</a>, who had a deep interest in cryptography and had just met with Konheim and others at IBM’s research lab, visited Hellman. Diffie had graduated from MIT in 1965 (B.S. in Mathematics), and then had worked at MITRE and been a resident guest at the MIT Artificial Intelligence (AI) Laboratory headed by Marvin Minsky.&nbsp; He left MITRE and the MIT AI Lab in 1969 to move to California and join the Stanford University AI Lab run by former MIT faculty member John McCarthy. Leaving this laboratory to spend more time studying cryptography, Diffie spent time travelling and meeting others interested in cryptography, including IBM’s research group. &nbsp;A planned half-hour early afternoon meeting in the fall of 1974 at Stanford between Diffie and Hellman evolved to an afternoon-long discussion followed by dinner and further exchange well into the evening.&nbsp; In Hellman’s words, “…it was a mild epiphany, finding an intellectual soul mate.” [4]</p>
<p>Diffie and Hellman’s intellectual quest evolved into an effort to solve cryptography’s key distribution problem without the use of a cumbersome, inefficient, and less secure key distribution center or couriers. These two problems were so novel that previously they had never been posed in the open literature. Prior to the invention of public key cryptography, if two parties wanted secure&nbsp;communication, they were dependent on using a third party, typically a courier, to distribute or exchange the cryptographic key.&nbsp;&nbsp;Key distribution created vulnerabilities, such as the trustworthiness of the messenger or potential interception, delays, and expenses. Many secure transactions required a courier with a locked hand-cuffed briefcase containing the key. [7] &nbsp;Diffie&nbsp;and&nbsp;Hellman also sought to develop digital signatures to authenticate that messages have not been faked or tampered with.</p>
<p>Shortly after Hellman and Diffie’s initial 1974 meeting they began working together—Diffie secured a Stanford University job in support of his cryptography research. &nbsp;In 1975 Hellman encouraged Diffie&nbsp;to enroll as a Stanford doctoral student under his direction, which Diffie did for the fall semester. &nbsp;About eighteen months after their initial meeting, Diffie and Hellman became aware of similar, independent work being done by Ralph Merkle, then a Masters student in Computer Science at the University of California-Berkeley. With no one at Berkeley appreciating Merkle's work, Hellman encouraged him to enroll as graduate student in Electrical Engineering at Stanford University, which he did in 1976. Working under Hellman’s direction, Merkle completed his Ph.D in&nbsp;1979. But, as Hellman recalls in his&nbsp;<a href=""https://www.youtube.com/watch?v=CqoFlL1Y6OY"">ACM Turing Award lecture</a>:&nbsp;""Whit can be nobody’s student,” to which Diffie replies: ""I’m not very good at it. I admire people who are good students. I think it’s a tremendously valuable skill.&nbsp;I just have never proved very good at it.” In consequence, Diffie never finished his degree at Stanford, but received the recognition he deserved in 1992 when the Swiss Federal Institute of Technology (ETH) awarded him an honorary doctorate. [4]</p>
<p>In 1974, Merkle independently developed the concept of public-key&nbsp;distribution, including an impractical proof of concept known as Merkle Puzzles. Soon afterward, and independently, Diffie and Hellman proposed a more elegant approach known as a public-key cryptosystem, which could provide both public-key distribution and digital signatures. They also developed a practical method of public-key&nbsp;distribution, now known as Diffie-Hellman Key Exchange. This led Hellman subsequently to argue that the name of that algorithm should include Merkle’s name.” [4, 7]</p>
<p>The seminal paper for public-key, “New Directions in Cryptography,” was written by Diffie and Hellman, presented at an IEEE Information Theory Symposium in June 1976, and published as an invited paper in the November 1976 issue of <em>IEEE Transactions on Information Theory</em>.&nbsp; The paper boldly began, “We stand today on the brink of a revolution in cryptography.”&nbsp; Their inventive insights did in fact lead to a revolution in cryptography.&nbsp;&nbsp;Diffie and Hellman’s article cites Merkle’s submitted “puzzles” paper, which was subsequently published in <em>Communication of the ACM</em> (April 1978).[1]</p>
<p>Public-key cryptography&nbsp;relies on ""one-way"" functions, so called because they are much easier to compute in one direction than to reverse. This makes it possible for a sender to encode a message using the recipient's openly distributed ""public key,"" without needing (or being able to reverse engineer) the ""private key"" used by recipient to decode the message. This can facilitate secure communication between individuals who have not met (without a key distribution center) and authenticate the message sender (digital signatures). [1] *</p>
<p>Diffie and Hellman described the concept of a public-key cryptosystem in their 1976 paper, but did not have a practical implementation. That was done later by MIT scientists/mathematicians <a href=""https://amturing.acm.org/award_winners/rivest_1403005.cfm"">Ronald Rivest</a>, <a href=""https://amturing.acm.org/award_winners/shamir_0028491.cfm"">Adi Shamir</a>, and <a href=""https://amturing.acm.org/award_winners/adleman_7308544.cfm"">Leonard Adleman</a> &nbsp;with their RSA algorithm (first released in 1977), which formed the basis for the company they founded in 1982, &nbsp;RSA Data Security. It relies on the factoring of large prime numbers as the basis of its one-way function. &nbsp;RSA Data Security spun-off its certifications or digital signatures enterprise as Verisign, Inc., in 1995.&nbsp; These three scientists jointly received the 2002 ACM Turing Award for the RSA algorithm and its impact on cryptography in practice. [8]</p>
<p>Great Britain’s signals intelligence center&nbsp;GCHQ later revealed that&nbsp;Australian-born James Ellis conceptualized secret communication over an&nbsp;unsecure&nbsp;channel, or public-key cryptography, between 1969 and 1970. Independently&nbsp;of Diffie-Hellman, and at around the same time, Malcolm J. Williamson then invented what is called Diffie-Hellman Key Exchange in the open literature.&nbsp;GCHQ also reported that mathematician Clifford Cocks developed the algorithm now known as RSA in 1973.&nbsp;Hellman&nbsp;feels that&nbsp;the evidence that&nbsp;GCHQ&nbsp;solved the key distribution problem is highly persuasive. But he also notes that this work did not address digital signatures, and that the first to publish a public paper is conventionally credited, because only openly disclosed discoveries can impact what is used in commercial and other open communities. [4]</p>
<p>Martin Hellman played a leading role in the late 1970s' and early 1980s' first “crypto war,” where he strongly&nbsp;advocated for&nbsp;cryptographic researchers' right to openly publish their work and the public’s right to strong encryption that can&nbsp;resist decryption efforts by domestic or foreign intelligence agencies including the National Security Agency (NSA).&nbsp;This included his early and continuing critique of the Data Encryption Standard (DES). NSA had&nbsp;worked with NBS (now NIST) and IBM to specify the DES, a modified version of Lucifer. While the general design of DES is stronger than that of Lucifer, its 56-bit key size was significantly weaker than Lucifer’s 128 bits. This is now known to have been a compromise that was thought to be strong enough for commercial use, but was widely believed to be vulnerable to decryption efforts by NSA through brute&nbsp;force with its massive computer&nbsp;processing resources. The National Bureau of Standards officially adopted DES&nbsp;on November 23, 1977. [2, 5]</p>
<p>Hellman published more than 70 technical papers and was granted 12 patents (including the April 1980 public-key cryptography patent with Diffie and Merkle). &nbsp;At Stanford, he played a role in helping students from&nbsp;diverse backgrounds to reach their potential within the university&nbsp;— work that was recognized by several awards from minority student organizations. Among many other honors, he was elected to the National Academy of&nbsp;Engineering (2002). In 1996 Hellman&nbsp;became Professor Emeritus.</p>
<p align=""right""><em>Jeffrey R. Yost</em></p>
<p>*Diffie-Hellman&nbsp;Public-Key</p>
<p>As they explain in their landmark paper:</p>
<p style=""margin-left: 36pt;"">In a public-key&nbsp;cryptosystem&nbsp;enciphering and deciphering are governed by distinct keys, E and D, such that computing D from E is computationally infeasible (e.g. requiring 10<sup>100</sup>&nbsp;instructions).&nbsp; The enciphering key E can be disclosed [in a directory] without compromising the deciphering key D. This enables any user of the system to send a message to any other user enciphered in such a way that only the intended recipient is able to decipher it….The problem of authentication is perhaps an even more serious barrier to the universal adoption of telecommunications for business transactions than the problems of key distribution…[it]…is at the heart of any system involving contracts and billing. Current electronic authentication systems cannot meet the need for a purely digital,&nbsp;unforgeable, message dependent signature. [1]</p>
<p>By convention, cryptography characters “Alice” and “Bob” (seeking secure communication) are used to explain public-key. Alice and Bob agree on large integers&nbsp;<em>n</em>&nbsp;and&nbsp;<em>g</em>&nbsp;with 1&lt;&nbsp;<em>g</em>&lt;&nbsp;<em>n</em>.&nbsp;&nbsp; The selections impact the security of the system.&nbsp; “The modulus n should be a prime; more importantly (n-1)/2 should also be a prime…and&nbsp;<em>g</em>&nbsp;should be a primitive root mod&nbsp;<em>n</em>…[and]...<em>n</em>&nbsp;should be…at least 512 bits long.” [6] The&nbsp;Diffie-Hellman&nbsp;protocol can be stated in basic form in 5 steps. [6]</p>
<p style=""margin-left: 36pt;"">(1)&nbsp;&nbsp;&nbsp; Alice&nbsp;chooses&nbsp;<em>x</em>&nbsp;(a random large integer) and computes&nbsp;<em>X</em>=<em>g<sup>x</sup></em>&nbsp;mod&nbsp;<em>n</em></p>
<p style=""margin-left: 36pt;"">(2)&nbsp;&nbsp;&nbsp; Bob&nbsp;chooses&nbsp;<em>y</em>&nbsp;(a random large integer) and computes&nbsp;<em>Y</em>=<em>g</em><sup>y</sup><sup>&nbsp;</sup>mod&nbsp;<em>n</em></p>
<p style=""margin-left: 36pt;"">(3)&nbsp;&nbsp;&nbsp; Alice sends&nbsp;<em>X&nbsp;</em>to Bob, while Bob sends&nbsp;<em>Y</em>&nbsp;to Alice (they keep x and y secret from each other)</p>
<p style=""margin-left: 36pt;"">(4)&nbsp;&nbsp;&nbsp; Alice computes&nbsp;<em>k</em>&nbsp;=&nbsp;<em>Y<sup>x</sup></em>&nbsp;mod&nbsp;<em>n</em></p>
<p style=""margin-left: 36pt;"">(5)&nbsp;&nbsp;&nbsp; Bob computes&nbsp;<em>k</em>’ =&nbsp;<em>X<sup>y</sup></em>&nbsp;mod&nbsp;<em>n&nbsp;</em></p>","<div class=""featured-photo"">
<a href=""/award_winners/hellman_4055781.cfm""><img src=""/images/lg_aw/4055781.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Martin Hellman ""></a>
</div>
<h6><span class=""label"">BIRTH:</span></h6>
<p>October 1945 in New York, New York, USA</p>
<h6>&nbsp;</h6>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>B.E. (Electrical Engineering, New York University, 1966); M.S. (Electrical Engineering, Stanford University, 1967); Ph.D. (Electrical Engineering, Stanford University, 1969).</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>IBM Thomas J. Watson Research Center (1968-1969); Massachusetts Institute of Technology (Assistant Professor, Electrical Engineering, 1969-1971); Stanford University (Professor, Electrical Engineering, 1971-1996, Professor Emeritus, 1996-present).&nbsp;</p>
<h6><span class=""label"">HONORS AND AWARDS:</span></h6>
<p>Fellow, IEEE (1980); IEEE Centennial Medal (1984); Stanford University Society of Black Scientists and Engineers’ Outstanding Professor Award (1994); Electronic Frontier Foundation Pioneer Award (1994); NIST/NSA National Computer Systems Security Award, with W. Diffie (1996); ACM Kanellakis Award, with W. Diffie (1997); Franklin Institute’s Levy Medal, with W. Diffie (1997); IEEE Information Theory Golden Jubilee Award for Technological Innovation, with W. Diffie (1998); IEEE Kobayashi Award, with W. Diffie and R. Merkle (1999); Marconi International Fellow Award (2000); Member, National Academy of Engineering (2002), IEEE Hamming Medal, with W. Diffie and R. Merkle (2010); National Inventors Hall of Fame (2011); RSA Lifetime Achievement Award (2012); National Cyber Security Hall of Fame (2012); Silicon Valley Hall of Fame (2013); ACM Turing Award, with W. Diffie (2015).</p>","","https://dl.acm.org/author_page.cfm?id=81100428712","Martin Hellman","<li class=""bibliography""><a href=""/bib/hellman_4055781.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/hellman_4055781.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/hellman_4055781.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/hellman_4055781.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179583-693","https://amturing.acm.org/award_winners/cocke_2083115.cfm","For significant contributions in the design and theory of compilers,
the architecture of large systems and the development of reduced instruction set computers (RISC);
for discovering and systematizing many fundamental transformations now used in optimizing compilers
including reduction of operator strength, elimination of common subexpressions, register allocation,
constant propagation, and dead code elimination.","<p>John Cocke was born in Charlotte, North Carolina, in 1925. His father Norman was the president of <a href=""https://en.wikipedia.org/wiki/Duke_Energy"" target=""_blank"">Duke Power Company</a> (later Duke Energy) and a member of the Board of Trustees of Duke University. That connection made it natural for John to attend college there. He earned his BSE (Bachelor of Science in Engineering) in Mechanical Engineering in 1946 and his PhD in mathematics in 1956. After graduation he joined IBM and worked at the T.J. Watson Research Center until his retirement 37 years later.</p>
<p>John Cocke made fundamental contributions to the architecture of high performance computers and to the design of optimizing compilers. Starting in 1975 he led the <a href=""https://en.wikipedia.org/wiki/IBM_801"" target=""_blank"">801 Minicomputer project</a> in IBM, which began as a pure research project but later led to a commercial product. The 801 design philosophy was based on a tight coupling between—and simultaneous development of—the hardware and the compiler. The resulting architecture was unlike the then common practice of having many complex instructions built into the hardware. For example, the earlier IBM 7030 “Stretch” computer had 735 different instructions, including instructions for variable-length operands. Such a rich set of hardware operations proved useful for special programming situations, but the designers of high level language compilers found it very difficult to use them in creating compiled code.</p>
<p>Cocke decided instead to design the 801 with a small set of basic instructions optimized for use by compilers, which resulted in a simpler but faster machine. This had a broad impact on computer architecture research, and was later called the <a href=""https://en.wikipedia.org/wiki/Reduced_instruction_set_computing"" target=""_blank"">Reduced Instruction Set Computer (RISC)</a> approach. Cocke and a small group of collaborators recognized that an appropriately defined set of machine instructions, with programs produced by a compiler carefully designed to exploit that instruction set, could result in a very high performance pipelined processor with lower cost and fewer circuits than computer using complex instructions. Their PL.8 compiler, which supported both Pascal and PL.8 (a variant of PL/I for systems programming), contained the first full implementation of Cocke and others' earlier optimization techniques, as well as new ones needed for RISC like a register allocator that avoided memory access delays and an instruction scheduler that hid <a href=""https://en.wikipedia.org/wiki/Delay_slot"" target=""_blank"">branching delays</a>.</p>
<p>Critical to the success of RISC was an optimizing compiler able to use the reduced instruction set efficiently. A pioneer in the development of the theoretical foundation for such compilers, Cocke co-developed <em>interval analysis</em> with <a href=""/award_winners/allen_1012327.cfm"">Frances Allen</a>, a program analysis technique based on a control flow graph reduction [<a href=""/bib/cocke_2083115.cfm#bib_3"">3</a>]. Cocke co-invented many of the optimizing transformations underlying today's compilers, including efficient range checking for arrays, global common subexpression elimination, code motion, operator strength reduction, constant propagation, dead code elimination, and instruction scheduling. Unlike the work of others, Cocke’s compiler optimization techniques were closely linked to related designs for the computer architecture.</p>
<p>The RISC idea was influenced by Cocke’s prior work on computer architecture and optimizing compiler technology. The <a href=""https://en.wikipedia.org/wiki/IBM_7030_Stretch"" target=""_blank"">Stretch project</a> he worked on had the ambitious goal of being 100 times faster than existing computing systems (such as the<a href=""https://en.wikipedia.org/wiki/IBM_704"" target=""_blank""> </a><a href=""https://en.wikipedia.org/wiki/IBM_704"" target=""_blank"">IBM 704</a>) while providing flexibility in addressing, floating-point arithmetic, and nonnumeric operations. The challenge of improving performance by two orders of magnitude led to several hardware and software features co-designed by Cocke, including instruction pipelining, error correcting codes (ECC), instruction scheduling, and register allocation. Most of these ideas are still used in modern computer architectures more than 50 years after their first use in Stretch. The Stretch project served as a prototype for the IBM System/360, and for the IBM Engineering Verification Engine, a special purpose parallel processor used to simulate IBM computer logic circuits.</p>
<p>The Stretch project focused primarily on performance without regard to cost. But Cocke observed that much of the richness of the Stretch architecture could not be exploited by the compiler. He saw that it is easier to write compilers for a simpler instruction set, and that had a major impact on the approach he took for the 801 project.</p>
<p>Another IBM Research project that had a major influence on Cocke’s work was the <a href=""https://en.wikipedia.org/wiki/ACS-1"" target=""_blank"">Advanced Computer System</a> (ACS) in the 1960’s, whose goal was to build the fastest possible computer for scientific applications. It was capable of executing multiple instructions in a single processor cycle, which is very challenging for a single instruction stream machine. In ACS seven operations could be initiated in a single cycle—one in the branch unit, three in the fixed point unit, and three in the floating point unit—a technique later called <em>superscalar processing</em>. To provide operands for all these instructions, ACS provided fast access to memory by allowing two memory accesses per cycle and having a <em>store queue</em> that was a forerunner of the <em>store buffer</em> used in modern processors. It also included a <em>branch history table</em> that allowed instruction prefetching based on dynamic execution patterns, a concept now referred to as <em>branch prediction</em>.</p>
<p>As a precursor to the RISC design methodology, an experimental ACS optimizing compiler was written long before the hardware design was finalized, which enabled computer architects to prioritize candidate hardware features based on the relative ease or difficulty of using them in the compiler. That compiler was also the breeding ground for many of the code optimization methods co-developed by Cocke. Although ACS never became an IBM product, it was an important &nbsp;research project whose breakthrough ideas greatly influenced later work by Cocke and others.</p>
<p>Cocke’s pioneering research on computer architecture and optimizing compilers had a lasting impact on computing systems. The 801 Minicomputer is considered to be the first RISC machine. That technology directly fed into IBM’s commercial RISC-based <a href=""https://en.wikipedia.org/wiki/RS/6000"" target=""_blank"">RS/6000</a> series workstations, and later into the <a href=""https://en.wikipedia.org/wiki/PowerPC"" target=""_blank"">PowerPC</a> processor architecture co-developed by Apple, IBM, and Motorola that is used in a wide range of computer systems including IBM’s <a href=""https://en.wikipedia.org/wiki/Blue_Gene"" target=""_blank"">Blue Gene/L</a> supercomputers. RISC processors were even used in the IBM 3090 and 390 series of mainframe machines to provide special functions. Many computer companies have incorporated RISC technology into their product lines. Hewlett-Packard based its product line on variants and extensions of the RISC principles, and has also licensed its designs to Hitachi and Samsung. Many computer companies around the world have sought licenses from IBM to use the RISC technologies developed by Cocke. Sun Microsystems, MIPS, Motorola, and Intel also developed RISC-based microprocessors.</p>
<p>John was renowned for the breadth of his intellect, for his energy, for his insights—and for his unconventional working methods. He often wandered the halls of IBM seeking out colleagues to chat with. He was a chain smoker, so an effective method of locating him was to follow the trail of his cigarette butts in the ash trays. Cocke was considered eccentric, particularly within the very proper atmosphere encouraged at IBM. Like many bachelor scientists, he would sometimes forget to cash his pay checks, and might wear the same clothes many days in a row. He eventually married Anne Holloway in 1989, when he was close to retirement. In a 1998 Forbes magazine interview he said, “I guess I was relatively absent-minded… but, you know, there are people more interested in science than in normal ways of life.”</p>
<p>Despite his unconventional demeanor, IBM recognized his many accomplishments by making him an IBM Fellow in 1972. This gave him the flexibility, for the next twenty years until he retired, to work on any aspect of hardware or software that interested him without being narrowly focused on immediate commercial applicability.</p>
<p>Cocke’s technical interests were broad, which led to contributions in areas of computer science beyond computer architecture and optimizing compilers, such as magnetic recording, data compaction, error control, coding theory, and acceleration of circuit simulation. Cocke was able to provide ideas that then served as a catalyst for others to generate more ideas. This influence often occurred through informal interactions rather than via scholarly publications. Many large and high-impact research projects within IBM arose from ""discussions with John.""</p>
<p>Cocke’s influence extended to collaborators in universities as well, most notably at New York University and Rice University. He and Jacob T. Schwartz of NYU authored the first comprehensive studies of compiler optimizations [<a href=""/bib/cocke_2083115.cfm#bib_4""><u>4</u></a>] and were contributors to Addison-Wesley's IBM Programming Book Series.</p>
<p>John Cocke was truly a “renaissance man” in computer science research, whose impact on the field stemmed from both his own technical contributions and his influence on others. He held more than 20 patents. A professorship at Duke University is named in his honor. He died in 2002 after a long illness and a series of strokes, and is survived by his wife Anne.</p>
<p>&nbsp;</p>
<p style=""text-align: right; ""><span class=""callout"">Authors: Michael G. Burke and Vivek Sarkar</span></p>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/cocke_2083115.cfm""><img src=""/images/lg_aw/2083115.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""John Cocke ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/cocke_2083115.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label""><strong>BIRTH:</strong></h6>
<p>Charlotte, North Carolina, May 30, 1925</p>
<h6 class=""label""><strong>DEATH: </strong></h6>
<p>Valhalla, New York, July 16, 2002&nbsp;</p>
<h6 class=""label""><strong>EDUCATION:</strong></h6>
<p>BSE (Bachelor of Science in Engineering, Duke University, mechanical engineering, 1946); Ph.D.&nbsp;(Duke University, Mathematics, 1953).</p>
<h6 class=""label""><strong>EXPERIENCE:</strong></h6>
<p>IBM T. J. Watson Research Laboratory, 1954-1992&nbsp;</p>
<h6 class=""label""><strong>HONORS AND AWARDS:&nbsp;</strong></h6>
<p>IBM Fellow (1972), Member, National Academy of Engineering (1979); Eckert-Mauchly Award (a joint award between the ACM and the IEEE Computer Society, 1985); Fellow, American Academy of Arts and Sciences (1986); ACM Turing Award (1987); Honorary Doctor of Science, Duke University (1988); IEEE Computer Society Pioneer Award (1989); IBM John E. Bertram Award for Sustained Excellence (1990); National Medal of Technology (1991); Member, National Academy of Sciences (1993); NEC Foundation Computers &amp; Communications Prize(1994); National Medal of Science (1994); The Franklin Institute's Certificate of Merit (1996); Seymour Cray Computer Science and Engineering Award (1999); the Benjamin Franklin Medal (2000); Fellow of the Computer History Museum (2002).</p>","","https://dl.acm.org/author_page.cfm?id=81342491861","John Cocke","<li class=""bibliography""><a href=""/bib/cocke_2083115.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283945&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/cocke_2083115.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179767-707","https://amturing.acm.org/award_winners/thacker_1336106.cfm","","<p>
<strong>Chuck Thacker has worked in industrial research labs for over forty years developing innovative computer systems, networks, and computing hardware.</strong> Although primarily a hardware architect, he has also written software for computer-aided design and user interfaces.</p>
<p>
Chuck was born in Los Angeles, California in 1943.&nbsp; Chuck knew quite early in life that he wanted a career in technology, as his father was an electrical engineer. He graduated from high school at the top of his class and entered the California Institute of Technology in 1960.&nbsp; He decided that the San Francisco Bay area would be a better place to work and live, and transferred to the University of California at Berkeley. There, he met his wife Karen; they were married in 1964.</p>
<p>
Chuck originally planned to be an experimental physicist. He had worked at the California Institute of Technology’s synchrotron laboratory, and was excited to work at the boundary of physics and engineering. He obtained a BS from the University of California, Berkeley in 1967, and then worked as an engineer for a small Berkeley company run by Jack Hawley, a local inventor. In late 1967, he heard about the UC Berkeley <a href=""https://en.wikipedia.org/wiki/Project_Genie"" target=""_blank"">Genie project</a>, and applied for a position. There he met future collaborators <a href=""/award_winners/lampson_1142421.cfm"">Butler Lampson</a>,&nbsp;<a href=""https://en.wikipedia.org/wiki/L_Peter_Deutsch"" target=""_blank"">Peter Deutsch</a>, Wayne Lichtenberger, and Mel Pirtle.</p>
<p>
Although he had used computers before joining Genie, he had not yet been irreversibly infected by computing. His first machine was an <a href=""https://en.wikipedia.org/wiki/IBM_1130"" target=""_blank"">IBM 1130</a> operated by the Berkeley Physics department. It was available to students at night, and had a compile-and-run Fortran system that eliminated the need for the operators that were common in most data centers of the time.</p>
<p>
At Genie, he learned about interactive computing, and was thoroughly smitten. He abandoned physics. The Genie project had built one of the first timesharing systems based on a <a href=""https://en.wikipedia.org/wiki/SDS_930"" target=""_blank"">Scientific Data Systems SDS-930</a> that they modified to support paging and have drum and disk storage subsystems, creating what later became the <a href=""https://en.wikipedia.org/wiki/SDS_940"" target=""_blank"">SDS-940</a>. Chuck arrived when the system was essentially complete, but he did implement some incremental expansions. His primary task was to begin planning the successor system, which was a much more ambitious project. Although the Berkeley EECS department has claimed him as one of their more successful alumni, and even provided him with <a href=""https://en.wikipedia.org/wiki/Harry_Huskey"" target=""_blank"">Harry Huskey</a> as a graduate advisor, this is an urban legend—he was a non-academic employee.</p>
<p>
Although it was fairly common at the time for research groups to build and operate their own computers, funding for such large projects was difficult to obtain, and a significant fraction was claimed by the university for “overhead.” As a result, some of the Genie scientists and engineers decided to start a company that was independent of the university. The Berkeley Computer Corporation (BCC), with Pirtle as its president, was founded in 1969.</p>
<p>
The BCC 500 timesharing system was one of the best examples of the “<a href=""https://en.wikipedia.org/wiki/Second-system_effect"" target=""_blank"">second system effect</a>” in computing. Having succeeded brilliantly with the SDS 940, the group now set a much more ambitious goal: to construct a system capable of supporting up to 500 remote users. Chuck once asked Mel Pirtle what people would do with it. Pirtle replied presciently, “They’ll probably play games.”</p>
<p>
The BCC 500 used several independent microprogrammed processors. Two were used to run user programs, one was for job scheduling, one for managing drum and disk I/O, and one for handling remote terminals. The system’s console computer was an IBM 360/30, which was chosen because of its highly reliable tape drives, line printer, and card equipment. Although the BCC 500 used magnetic core for main memory, it also had a small “fast memory”, which today we would call a cache. The computer occupied two floors of a refurbished fur storage vault in the industrial section of Berkeley. The location was deliberately kept secret, since this was a time of student demonstrations, many directed at computing installations.</p>
<p>
At BCC, Chuck was responsible for the design of the microprogrammed processor. He also designed the equipment for supporting remote users by concentrating their traffic over the fastest available data connection available at the time, 2400 bits per second.</p>
<p>
Two years and four million dollars later, the BCC 500 system worked successfully. But the 1970 recession made the needed second round of venture funding impossible to get. The company had to close its doors. The one system they had built was sold to the University of Hawaii, where it served as the departmental computing facility for several years.</p>
<p>
Fortunately for the BCC group, Xerox had just decided to start the multi-disciplinary <a href=""https://en.wikipedia.org/wiki/Palo_Alto_Research_Center"" target=""_blank"">Palo Alto Research Center</a> (PARC). It included a General Science Laboratory (GSL), primarily employing physicists and materials scientists; a System Sciences Lab (SSL), primarily charted to explore systems based on computers; and a Computer Science Lab (CSL), chartered to explore computers themselves. To head CSL, Xerox hired <a href=""https://en.wikipedia.org/wiki/Robert_Taylor_%28computer_scientist%29"" target=""_blank"">Bob Taylor</a>, who had directed the Information Processing Techniques Office (IPTO) at the Department of Defense’s Advanced Research Projects Agency (ARPA, now DARPA). Taylor, who knew most of the people involved in academic computing projects at the time, chose a core group from BCC as his first employees: Chuck, Butler Lampson, Peter Deutsch, Jim Mitchell, and Charles Simonyi.</p>
<p>
The first task for the new laboratory was to acquire the necessary computing capability to support its research. At the time, a <a href=""https://en.wikipedia.org/wiki/DEC_PDP-10"" target=""_blank"">DEC PDP-10</a> running the BBN Tenex operating system was the de facto standard in most academic groups. But having just purchased Scientific Data Systems, Xerox management was reluctant to purchase a competitor’s machine, so the group decided to build its own PDP-10. The Multiple Access Computer System (MAXC—an acronym carefully selected as a pun on the name of SDS President Max Palevsky) was designed much more conservatively than the BCC 500. The most innovative part was the memory system, which used the new Intel 1103 dynamic RAM chip. Chuck designed the memory system and much of the I/O for the machine. Butler Lampson designed the microprocessor. Eventually, three copies of MAXC were built, and it served as the lab’s primary computer for several years. It was one of the first ARPAnet hosts, using an interface to the network’s Interface Message Processor designed by <a href=""https://en.wikipedia.org/wiki/Bob_Metcalfe"" target=""_blank"">Bob Metcalfe</a> and <a href=""https://en.wikipedia.org/wiki/David_Boggs"" target=""_blank"">Dave Boggs</a>. MAXC was more reliable than a DEC PDP-10, primarily because the DEC designers built an asynchronous machine without fully understanding the implications of metastable states in digital synchronizers.</p>
<p>
With MAXC complete and the laboratory growing, the group looked for new challenges. Although non-technical, Taylor explained to the group that timesharing was only a step on the road to true personal computing, and that computers could serve as communication, as well as computing devices. A Xerox theme was to produce the “paperless office”, and they tried to see how this might be accomplished. SSL, the sister lab to CSL, had experimented with an office system that included high resolution raster displays based on complex character generation technology, but CSL used a different approach: the bitmapped display.</p>
<p>
Chuck understood the validity of Moore’s law, and realized that the semiconductor memory used in MAXC would quickly get larger, faster, and cheaper. It would therefore be possible to build a new type of display where each pixel on the screen was represented by a single bit of memory. The result was the 1973 <a href=""https://en.wikipedia.org/wiki/Xerox_Alto"" target=""_blank"">Alto</a>, the first computer system that we would recognize today as a personal computer. The Alto hardware was designed by Chuck and <a href=""https://en.wikipedia.org/wiki/Edward_M._McCreight"" target=""_blank"">Ed McCreight</a>, who designed the disk subsystem for the machine’s 2.5 Mbit disk.</p>
<p>
In order to become pervasive, Alto needed to be inexpensive. To achieve this, Chuck decided to turn the usual assumptions of minicomputer design around. Rather than multiplexing the memory between the CPU and I/O controllers, the machine would multiplex the processor among independent microcoded tasks. This allowed the hardware complexity of I/O controllers to be replaced by task-specific microcode. The highest priority task for the 6 MHz processor was to load a single scan line of display pixels; the lowest was the emulator for the instruction set running user programs. Alto may have been the first simultaneously-multithreaded (SMT) computer.</p>
<p>
The Alto was a hit within Xerox. It provided the hardware on which a number of innovative systems were built. Butler Lampson was responsible for the design of much of the early Alto software, including the operating system and (with Charles Simonyi) the <a href=""https://en.wikipedia.org/wiki/Bravo_%28software%29"" target=""_blank"">Bravo</a> text editor, which eventually became Microsoft Word. Programs that exploited the Alto’s bitmap display included graphic editors specialized for splines, and Chuck’s SIL (“simple illustrator”) computer-aided design program, which became the standard tool for schematic drawing. <a href=""/award_winners/kay_3972189.cfm"">Alan Kay’s</a> <a href=""https://en.wikipedia.org/wiki/Smalltalk"" target=""_blank"">Smalltalk</a> system was one of the first Alto applications, and its microcode included <a href=""https://en.wikipedia.org/wiki/Dan_Ingalls"" target=""_blank"">Dan </a><a href=""https://en.wikipedia.org/wiki/Dan_Ingalls"" target=""_blank"">Ingalls</a>’ <a href=""https://en.wikipedia.org/wiki/BitBlt"" target=""_blank"">bitBlt</a> primitive for character rendering that is still used in graphics today.</p>
<p>
Alto was designed so that it was easy to add new I/O devices with a small amount of hardware and microcode. One of the first extensions was the <a href=""https://en.wikipedia.org/wiki/Ethernet"" target=""_blank"">Ethernet</a> local area network invented by Bob Metcalfe, with some help from Chuck on the properties of transmission lines. Ethernet allowed Altos to be connected into a distributed system, and much of the early distributed systems work, including the Grapevine email system developed by Andrew Birrell and others, were first run on Alto. For Alan Kay, Chuck designed an organ with a 3-manual keyboard, pedals, and a microcoded wave-table synthesizer driving a 12 KHz ADC.</p>
<p>
Although the Alto was not a commercial success, it was the motivation for what became a spectacular success for Xerox: laser printing. <a href=""https://en.wikipedia.org/wiki/Gary_Starkweather"" target=""_blank"">Gary Starkweather</a> of GSL, who intimately understood optics and xerography, built the first laser printer, called EARS (Ethernet Alto Research Character Generator and Scanning Laser Output Terminal), based on a character generator designed by Butler Lampson. EARS provided printing for most of PARC until it was replaced by the much more cost effective Dover Printer designed by <a href=""https://en.wikipedia.org/wiki/Bob_Sproull"" target=""_blank"">Bob Sproull</a>, Butler Lampson, and others.</p>
<p>
Chuck went on to design several successors to the Alto. He started a project to build a computer based on emitter-coupled logic (ECL), which was completed as the “Dorado” by a large team consisting of Severo Ornstein, Gene McDaniel, Lampson, McCreight, and others. Meanwhile Chuck had joined Dave Liddle’s System Development group, which needed an inexpensive computer to control a laser printer. Although they had initially planned to use the Dorado, it proved to be too expensive, so they instead used the lower-cost Dolphin designed by Chuck and Brian Rosen. Chuck returned to PARC, where Dolphin had limited application because Dorado was much faster. Its primary virtue was that it had a color display, which was used to build a computer-aided design (CAD) system to support the <a href=""https://en.wikipedia.org/wiki/Mead_%26_Conway_revolution"" target=""_blank"">Mead-Conway LSI</a> design methodology popular at the time.</p>
<p>
By 1980 it was clear that LSI was the best technology to build personal computers, and Chuck started the Dragon project to do that. Dragon was a multiprocessor that employed one of the first <a href=""https://en.wikipedia.org/wiki/Cache_coherence"" target=""_blank"">cache-coherence protocols</a>. Although never completed, it influenced later systems.</p>
<p>
In 1983, Taylor was fired, and much of the core CSL team followed him to the newly-created Digital Equipment Corporation (DEC) <a href=""https://en.wikipedia.org/wiki/DEC_Systems_Research_Center"" target=""_blank"">Systems Research Center</a>. As at PARC, the first project was to provide the computing infrastructure for the laboratory. This took the form of the Firefly, the first multiprocessor workstation with coherent caches. Firefly was designed by Chuck and Larry Stewart, and served, as did the earlier Alto, as the basis for several projects in the area of distributed systems.</p>
<p>
Chuck also led the development of the AN1 and AN2 networks. AN1 was a packet-switching network that employed 10 Mb/s point-to-point links at a time when 1 Mbit/s Ethernet was the fastest commercially available LAN technology. AN2, started in the late ‘80s was an attempt to use 622 Mb/s ATM technology as a LAN. Although it was commercialized as the DEC GigaNet ATM product, it was not very successful, since 100 Mb Ethernet switches were becoming available at much lower cost.</p>
<p>
In the early 1990s Chuck was approached by Bob Supnik, who was leading the effort to develop the <a href=""https://en.wikipedia.org/wiki/DEC_Alpha"" target=""_blank"">DEC Alpha</a> system. Supnik needed a computer to exercise the early Alpha chips, and it was clear that the normal DEC engineering approach could not meet the required schedule. Chuck, Larry Stewart, and Dave Conroy designed and built the Alpha Demonstration Unit, an ECL multiprocessor that was used for Alpha software development until the production systems were ready. The ADU was credited with saving a year in getting Alpha to the market.</p>
<p>
By 1997 DEC was in decline, and Chuck decided on a change. His children were now grown, so he and his wife Karen considered taking a year’s sabbatical in Europe. That plan was cut short when he received a call from <a href=""https://en.wikipedia.org/wiki/Nathan_Myhrvold"" target=""_blank"">Nathan Myhrvold</a>, the CTO of Microsoft. Chuck had been approached by Microsoft earlier, but had declined due to the company’s software-centric approach to computing. Microsoft now wanted someone with industrial research experience to take a two-year assignment helping <a href=""https://en.wikipedia.org/wiki/Roger_Needham"" target=""_blank"">Roger Needham</a> set up a research laboratory in Cambridge. Roger had been a frequent visitor to PARC and SRC, and the opportunity seemed perfect. Chuck joined Microsoft and moved to the UK.</p>
<p>
The Cambridge Lab was Microsoft’s first attempt to establish a non-US lab, and it was quite successful. Chuck’s work mainly involved hiring talent and growing the lab, although he did do some work on electronic books.</p>
<p>
When he returned to the US in 1999 he decided not to rejoin Microsoft Research, but instead to work with the group within the company developing the <a href=""https://en.wikipedia.org/wiki/Microsoft_Tablet_PC"" target=""_blank"">Microsoft Tablet PC</a>. At that time it was difficult to convince hardware partners that tablets were a viable class of computers, so they decided to construct a prototype to demonstrate the capabilities of the new form factor. Chuck designed the prototype hardware, using Silicon Valley subcontractors for much of the engineering. This effort resulted in the launch of the Tablet PC in 2001. Butler Lampson, who had joined Microsoft in 1995, worked on the tablet software.</p>
<p>
Chuck rejoined Microsoft Research (MSR) in 2005, shortly after the founding of Microsoft Research Silicon Valley, to work in computer architecture and networking. In 2005, he led the effort to build the BEE3, a hardware platform for architectural experimentation. Based on <a href=""https://en.wikipedia.org/wiki/Field-programmable_gate_arrays"" target=""_blank"">field-programmable gate arrays</a> (FPGAs), BEE3 was designed in cooperation with the RAMP Consortium (“Research Accelerator for Multiple Processors”) led by Dave Patterson of the University of California in Berkeley, along with participants from MIT, Carnegie Mellon University, University of Texas in Austin, Stanford University, and University of Washington. BEE3 provides a relatively low-cost platform for experimenting with new architectural features. It resulted in the formation of a startup (BeeCube) that manages the distribution and support of the system. BeeCube has also produced a follow-on line of systems using more modern FPGAs.</p>
<p>
In 2010, Chuck designed the Beehive, a simple multiprocessor system runing on a Xilinx development board that is much less expensive than BEE3. Beehive and its software tool chain, developed by Andrew Birrell and Tom Rodeheffer, have been used in a number of universities as the basis for lab courses in computer architecture. Its design is simple enough to be easily understood and modified by students, and can lead to a deeper understanding into how computers actually work.</p>
<p>
Chuck’s latest project is AN3, whose goal is to improve networking in large data centers. Currently, Ethernet and TCP/IP are used in these networks, but TCP/IP is an old standard designed to solve problems that no longer exist in today’s data centers. AN3 takes a clean-slate approach to the problem to provide simple, reliable, and low cost networking.</p>
<br clear=""all"">
<p>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/thacker_1336106.cfm""><img src=""/images/lg_aw/1336106.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Charles P. Thacker""></a>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>February 26, 1943, Pasadena, California, USA</p>
<h6><span class=""label"">DEATH: </span></h6>
<p>June 12, 2017, Palo Alto, California, United States</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>B.S., physics (University of California, Berkeley, 1967).</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>University of California, Berkeley, Genie project (1967-1969); Berkeley Computer Corporation (1969 – 1970); Palo Alto Research Center—PARC (1970-1983); Digital Equipment Corporation, Systems Research Center (1983-1997); Microsoft Research Laboratory, Cambridge, England (1997-1999); Microsoft Tablet computer project (1999-2005); Microsoft Research, Silicon Valley (from 2005), currently Technical Fellow at Microsoft.</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>ACM’s Software Systems Award (1984, with B. Lampson and R. Taylor); Fellow of the ACM (1994); Distinguished Alumnus in Computer Science at University of California, Berkeley (1996);Charles Stark Draper Prize (2004, together with Alan C. Kay, Butler W. Lampson, and Robert W. Taylor); IEEE John von Neumann Medal (2007); Fellow of the Computer History Museum (2007); member of the National Academy of Engineering, and the American Academy of Arts and Sciences.<br>
He holds an honorary doctorate from the Swiss Federal Institute of Technology ETH Zurich.<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81332531482","Charles P. (Chuck) Thacker","<li class=""bibliography""><a href=""/bib/thacker_1336106.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1816006&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""award-video""><a href=""/vp/thacker_1336106.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/thacker_1336106.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/thacker_1336106.cfm""><span></span>Additional<br> Materials</a></li>"
"1573178838-644","https://amturing.acm.org/award_winners/patterson_2316693.cfm","For pioneering a systematic, quantitative approach to the design and evaluation of computer architectures with enduring impact on the microprocessor industry.","<div class=""awards-winners__citation-text"">
<p>Born in Evergreen, Illinois in 1947, David A. Patterson graduated from South High School in Torrance, California, and then enrolled at the University of California, Los Angeles (UCLA).&nbsp; The first person in his family to graduate from college, Patterson received his Bachelor’s(1969) and Master’s (1970) degrees in computer science.&nbsp; Patterson, a wrestler and math major, tried a programming course when his preferred course was cancelled (‘even with punch cards, Fortran, line printers, one-day turn-around—I was hooked”).<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn1"" name=""_ednref1"" title="""">[1]</a></p>
<p>Patterson married high school sweetheart Linda (raised near Berkeley in Albany) and with two young boys, he worked part-time (20-40 hours per week) on airborne computers at Hughes Aircraft for three years while earning a doctoral degree (1976) in computer science at UCLA.&nbsp; The job hooked him on practical engineering results.&nbsp;&nbsp; His thesis advisor was Gerald Estrin (also advisor for Vinton Cerf, Turing Award, 2004).</p>
<p>Patterson was hired into the University of California at Berkeley’s computer science/ electrical engineering department upon graduation. Patterson’s PhD thesis was on writable control store methods for operating systems, so he began his Berkeley career with Carlo Sequin working on the X-TREE project led by Alvin Despain.<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn2"" name=""_ednref2"" title="""">[2]</a>&nbsp; Years later, he called this modular multiprocessor system 'way too ambitious, no resources, great fun.’ <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn3"" name=""_ednref3"" title="""">[3]</a></p>
<p>Patterson took a three-month sabbatical at Digital Equipment Corporation (1979), where Joel Emer and Douglas Clark were starting measurements on a VAX minicomputer. &nbsp;&nbsp;It had a very complex instruction set and hence a very large and complex microprogram.&nbsp;&nbsp; Patterson worked on reducing micro-coding errors, concluding that simplifying instruction sets would “easily yield reduced errors.” <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn4"" name=""_ednref4"" title="""">[4]</a></p>
<p>Back at Berkeley, Patterson and Sequin teamed on a four-course series where they tasked graduate students to investigate these ideas. Patterson coined the acronym RISC (Reduced Instruction Set Computer) to describe a resultant chip, known as RISC-1, with 44,420 transistors.&nbsp; A good companion computer for Berkeley’s work on UNIX operating systems and C programming techniques, it could handle large amounts of memory, and it used pipelining techniques to handle several instructions simultaneously. <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn5"" name=""_ednref5"" title="""">[5]</a>&nbsp;&nbsp;</p>
<p>Instantly popular, the courses led to a Distinguished Teaching Award (1982).&nbsp; Patterson’s acceptance speech acknowledged why he selected Berkeley: “When I graduated from UCLA, I went around interviewing at a lot of places,….&nbsp; They really said, ‘….&nbsp; Teaching is something we don’t care about—the coin of the realm is publication…’. &nbsp;I was disturbed (because) that meant that I would be spending many hours of my life in front of a bunch of students, and if&nbsp; I didn’t do a good job, I’d disappoint a lot of students.&nbsp; If I did do a good job, I’d disappoint the people I worked for.&nbsp; But when I came to Berkeley, it was great.&nbsp; The electrical engineering/ computer science department emphasized that they really did care about teaching, ...” <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn6"" name=""_ednref6"" title="""">[6]</a></p>
<p>From 1982 to 1983, Sequin led the RISC-II chip project; Patterson managed collaboration between UC Berkeley and the ARPA VLSI program. &nbsp;This 40,760 transistor chip, three times faster and half the size of RISC-1, became the highly influential foundation of Sun Microsystems’ SPARC micro-architecture.</p>
<p>Patterson first met John Hennessy at a meeting for DARPA funded research VLSI projects in 1980 or 1981 where each was presenting their ideas. &nbsp;RISC-2 emerged simultaneously with Hennessy’s MIPS (Microprocessor without Interlocked Pipeline Stages) prototype at Stanford in 1983. Arguments between RISC vs. MIPS designs were soon dwarfed by their common thesis against CISC (Complex Instruction Set Computers), used by the entire industrial computer design community.</p>
<p>Years later, Patterson recalled:&nbsp; “There is this remarkable point in time when it was clear that a handful of grad students at Berkeley or Stanford could build a microprocessor that was arguably better than what industry could build—faster, cheaper, more efficient….&nbsp; RISC was very controversial, it was heretical….&nbsp; We had a hard time convincing people of that.” <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn7"" name=""_ednref7"" title="""">[7]</a>&nbsp;&nbsp;</p>
<p>Patterson resolutely resisted the lure of leaving the university to pursue the RISC technology in a company.&nbsp; John Markoff, in the <em>New York<u> Times</u></em>, quoted Patterson about the chance to start a company. ""I made the choice between being happy and being wealthy.""&nbsp;<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn8"" name=""_ednref8"" title="""">[8]</a></p>
<p>Patterson and Hennessy in 1990 codified their shared insights in a very influential book,&nbsp;<em>Computer Architecture: A Quantitative Approach</em>.&nbsp; This book, now in its 6<sup>th</sup> edition, provided a simple, robust, and quantitative framework for evaluating integrated systems. <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn9"" name=""_ednref9"" title="""">[9]</a></p>
<p>Sun adopted the Berkeley architecture, while Silicon Graphics bought Hennessy’s MIPS.&nbsp; Joel Birnbaum, John Cocke’s supervisor at IBM, brought RISC ideas to Hewlett-Packard.&nbsp;&nbsp; A number of key micro-coded RISC ideas were incorporated into the Intel’s personal computer&nbsp; chips, and then mobile products (e.g. <em>iPhone) </em>emphasized efficiency, power usage, and die size.&nbsp; In their joint Turing Award speech at ISCA (2018), Patterson and Hennessy noted that an astounding 99% of the more than 20 billion micro-processors now produced annually are RISC processors, and are found in nearly all smartphones, tablets, and the billions of embedded devices that comprise the Internet of Things (IoT).<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn10"" name=""_ednref10"" title="""">[10]</a></p>
<p>Between 1989 and 1993, Patterson led the Redundant Arrays of Inexpensive Disks (RAID) project with Berkeley colleague Randy Katz, vastly improving speed and reliability of affordable disk systems. Most web servers now use some form of RAID; many compare this work in importance to Patterson’s RISC work.&nbsp; Later, Patterson contributed in implementing complex systems experiments by networking smaller computers together, foretelling “multi-tier architectures” now used by many Internet companies.&nbsp;</p>
<p>Patterson today is a Distinguished Engineer at Google and serves as Vice Chair of the Board of the RISC-V Foundation.&nbsp;&nbsp; An eternal optimist, Patterson notes that tuned hardware/software designs can offer dramatic performance improvements for deep learning applications, which he hopes will usher in a ‘new golden age of computing.’ <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn11"" name=""_ednref11"" title="""">[11]</a></p>
<p>When lecturing, Patterson frequently mentions his family, and his life-long enthusiasm for several activities, including soccer, wrestling, cycling and weight lifting.&nbsp; He reminds listeners that teams are better than individual activity, noting that you cannot be a winner on a losing team, while all members of a winning team are winners by definition. He worked with his high school wrestling partner, Rick Byrne, to win the American Power Lifting California championship, setting a new national record for age and weight bench press, dead lift, squat, and all three combined lifts in 2013 at age 66.<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn12"" name=""_ednref12"" title="""">[12]</a> &nbsp;Patterson rode in the annual two-day <em>Waves to Wine</em> bike ride through the Bay Area from 2003-2012 and was the top multiple sclerosis research fundraiser for the group for seven straight years.<a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn13"" name=""_ednref13"" title="""">[13]</a></p>
<p>Patterson was on the ACM Executive Council for six years, serving as ACM President, 2004-2006.&nbsp; He took a sabbatical year to do that, explaining that for ‘a big job’ you need really to step up to it.&nbsp; He also chaired the Computing Research Association, and served on PITAC for two years (Presidential Information Technology Advisory Committee).&nbsp; His motto throughout has been, “It’s not how many projects you start, it’s how many you finish….&nbsp; So, pick one big thing a year, and finish it.” <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn14"" name=""_ednref14"" title="""">[14]</a></p>
<p>For many professional occasions in recent years, including the 2018 ACM Annual Awards Dinner, Patterson proudly has worn a Scottish kilt to honor his forebears.&nbsp; In his acceptance speech that evening, as well as in multiple other speeches in recent years, he cited his 50<sup>th</sup> marriage anniversary with his childhood sweetheart, Linda, who co-founded the <em>East Bay Improv </em>group in Berkeley many years ago. <a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_edn15"" name=""_ednref15"" title="""">[15]</a></p>
<p>Patterson, made an ACM Fellow in 1994, is also a Fellow of AAAS and IEEE.&nbsp; He has been elected to the National Academies of Engineering, Sciences, and the American Academy of Arts and Sciences.&nbsp; Hennessy and Patterson have won a number of joint awards, including the John von Neumann Medal (IEEE, 2000), the Eckert-Mauchly ACM/IEEE award in 2001; Fellows for the Computer History Museum in 2007, and the ACM Turing Award in 2017.</p>
<address style=""margin-left: 3.5in; text-align: right;"">Author: Charles H. House</address>
<div><br clear=""all"">
<hr align=""left"" size=""1"" width=""33%"">
<div id=""edn1"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref1"" name=""_edn1"" title="""">[1]</a> Patterson, David, “Closing Remarks,”&nbsp; 40 Years of Patterson Symposium, UC Berkeley EE/CS, May 7, 2016; <a href=""https://www.youtube.com/watch?v=8X0tsp-FVGI"">https://www.youtube.com/watch?v=8X0tsp-FVGI</a></p>
</div>
<div id=""edn2"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref2"" name=""_edn2"" title="""">[2]</a> Carlo H. Séquin,&nbsp;<a href=""https://dblp.uni-trier.de/pers/hd/d/Despain:Alvin_M="">Alvin M. Despain</a>, David A. Patterson:&nbsp; Communication In X-TREE, A Modular Multiprocessor System.&nbsp;<a href=""https://dblp.uni-trier.de/db/conf/acm/csc1978-1.html#SequinDP78"">ACM Annual Conference (1)&nbsp;1978</a>:&nbsp;194-203</p>
</div>
<div id=""edn3"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref3"" name=""_edn3"" title="""">[3]</a> Patterson, David A., “Closing Remarks,” <em>op. cit.</em>&nbsp;&nbsp; Also see Patterson, David A., “My Last Lecture: How to be a Bad Professor,”&nbsp; Berkeley EE/CS, May 6, 2016; <a href=""https://www.youtube.com/watch?v=TK6EPvrmcBk"">https://www.youtube.com/watch?v=TK6EPvrmcBk</a></p>
</div>
<div id=""edn4"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref4"" name=""_edn4"" title="""">[4]</a> Patterson, David A., interview with Jim Demmel , EE/CS chair at Berkeley, UC Berkeley&nbsp; ACM Turing Laureate Colloquium October 10, 2018; <a href=""https://eecs.berkeley.edu/turing-colloquium/schedule/patterson"">https://eecs.berkeley.edu/turing-colloquium/schedule/patterson</a></p>
</div>
<div id=""edn5"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref5"" name=""_edn5"" title="""">[5]</a> John Hennessy and David Patterson, “ACM A.M.Turing Award lecture, 45<sup>th</sup> ISCA (International Symposium of Computer Architecture), Los Angeles, June 4, 2018&nbsp;&nbsp; <a href=""https://www.acm.org/hennessy-patterson-turing-lecture"">https://www.acm.org/hennessy-patterson-turing-lecture</a></p>
</div>
<div id=""edn6"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref6"" name=""_edn6"" title="""">[6]</a> Patterson 1982 UC Berkeley Distinguished Teaching Award lecture, published on YouTube later (March 16, 2016); &nbsp;<a href=""https://www.youtube.com/watch?v=asKcJyFbRm0"">https://www.youtube.com/watch?v=asKcJyFbRm0</a> &nbsp;&nbsp;</p>
</div>
<div id=""edn7"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref7"" name=""_edn7"" title="""">[7]</a> Patterson, David A., &nbsp;“A New Golden Age for Computer Architecture: History, Challenges, and Opportunities,” UC Berkeley&nbsp; ACM Turing Laureate Colloquium lecture, October 10, 2018; <a href=""https://eecs.berkeley.edu/turing-colloquium/schedule/patterson"">https://eecs.berkeley.edu/turing-colloquium/schedule/patterson</a></p>
</div>
<div id=""edn8"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref8"" name=""_edn8"" title="""">[8]</a> Markoff, John, “Chip Technology’s Friendly Rivals,” New York Times, June 4, 1991; <a href=""https://www.nytimes.com/1991/06/04/business/chip-technology-s-friendly-rivals.html"">https://www.nytimes.com/1991/06/04/business/chip-technology-s-friendly-rivals.html</a></p>
</div>
<div id=""edn9"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref9"" name=""_edn9"" title="""">[9]</a> J. L. Hennessy and D. A. Patterson,&nbsp;<em><a href=""https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1"">Computer Architecture: A Quantitative Approach</a></em>, 6th ed., Computer Architecture and Design, Morgan Kaufmann Publishers, 2017</p>
</div>
<div id=""edn10"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref10"" name=""_edn10"" title="""">[10]</a> John Hennessy and David Patterson, “ACM A.M.Turing Award lecture, <em>op. cit.</em></p>
</div>
<div id=""edn11"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref11"" name=""_edn11"" title="""">[11]</a> Patterson, David, “A New Golden Age for Computer Architecture,” Artificial Intelligence Conference, September 12, 2018&nbsp;&nbsp; <a href=""https://www.youtube.com/watch?v=c03Z0Ms8pKg"">https://www.youtube.com/watch?v=c03Z0Ms8pKg</a></p>
</div>
<div id=""edn12"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref12"" name=""_edn12"" title="""">[12]</a> Posting, Baban Zarkovich, April 20, 2013, “Professor David Patterson sets the APA RAW California State Record,”&nbsp; <a href=""https://amplab.cs.berkeley.edu/news/professor-david-patterson-sets-the-apa-raw-california-state-record/"">https://amplab.cs.berkeley.edu/news/professor-david-patterson-sets-the-apa-raw-california-state-record/</a></p>
</div>
<div id=""edn13"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref13"" name=""_edn13"" title="""">[13]</a> <a href=""http://anti-ms-crew.berkeley.edu/theteam.shtml"" target=""_blank"">""Berkeley's Anti-MS Crew""</a><cite>. anti-ms-crew.berkeley.edu.&nbsp;&nbsp; </cite><a href=""http://anti-ms-crew.berkeley.edu/theteam.shtml"">http://anti-ms-crew.berkeley.edu/theteam.shtml</a></p>
</div>
<div id=""edn14"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref14"" name=""_edn14"" title="""">[14]</a> Patterson, David A., “How to have a bad career in research/academia,” Berkeley, November 2001; <a href=""https://people.eecs.berkeley.edu/~pattrsn/talks/BadCareer.pdf"">https://people.eecs.berkeley.edu/~pattrsn/talks/BadCareer.pdf</a></p>
</div>
<div id=""edn15"">
<p><a href=""file://quadrennial/Data/Mydocuments/Academic/ACM%20Turing%20Award%202015%20onward/Pattreson%20and%20Hennsey%202017/Patterson%20CHH%20v2.docx#_ednref15"" name=""_edn15"" title="""">[15]</a> Patterson, “A New Golden Age” presentation;<em> op.cit.</em></p>
</div>
</div>
</div>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/patterson_2316693.cfm""><img src=""/images/lg_aw/2316693.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""David Patterson ""></a>
</div>
<h6><a href=""/stonebraker_1172121.pdf"" target=""_blank"" title=""PDF format"">BIRTH:</a></h6>
<p>November 16, 1947.</p>
<h6><a href=""/stonebraker_1172121.pdf"" target=""_blank"" title=""PDF format"">EDUCATION:</a></h6>
<p>Bachelor’s degree in Mathematics (University of California at Los Angeles, 1969); M.Sc. in Computer Science (University of California at Los Angeles, 1970); Ph.D. in Computer Science (University of California at Los Angeles, 1976).</p>
<h6>EXPERIENCE</h6>
<p>University of California at Berkeley (1977-2016), arrriving as an Assistant Professor and retiring as holder of the E.H. and M.E. Pardee Chair of Computer Science; Google, Distinguished Engineer (2016-present). Concurrent roles included President of the Association for Computing Machinery (2004-2006).</p>
<h6>HONORS AND AWARDS (SELECTED):</h6>
<p>Fellow, Institute of Electrical and Electronic Engineers (1990); Member, National Academy of Engineering (1993); Fellow, Association for Computing Machinery (1994); ACM SIGMOD “Test of Time” Award, (1999); IEEE Reynold B. Johnson Information Storage Systems Award, (1999); IEEE John von Neumann Medal (2000); Computers and Communications Prize (2004); Member, National Academy of Sciences (2006); Member, American Academy of Arts and Sciences member (2006); Fellow, Computer History Museum (2007); Fellow, American Association for the Advancement of Science (2007); Eckert-Mauchly Award (2008); ACM SICOPS Hall of Fame Award (2011); ACM A.M. Turing Award (2017).</p>","","https://dl.acm.org/author_page.cfm?id=81100565162","David Patterson","<li class=""bibliography""><a href=""/bib/patterson_2316693.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/patterson_2316693.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/patterson_2316693.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179151-665","https://amturing.acm.org/award_winners/rabin_9681074.cfm","Along with Dana S. Scott, for their joint paper ""Finite Automata and Their Decision Problem,"" which introduced the idea of nondeterministic machines, which has proved to be an enormously valuable concept. Their (Scott & Rabin) classic paper has been a continuous source of inspiration for subsequent work in this field.","<p><strong>Michael Rabin was born in 1931 in Breslau, Germany, now Wroclaw</strong><strong>, Poland.</strong> His father, a rabbi, moved the family to Palestine in 1935. Michael was given a very good primary education and attended the best high school in Haifa.</p>
<p>Michael related the following story to explain how he became interested in mathematics at about the age of 10 or 11. In the hallway of his school he encountered a few older students attempting to find a proof for an elementary problem in geometry. To his delight, he was able to solve it, and he enjoyed the experience of starting with just a few known facts about a geometrical figure and deducing others that are not obvious. The idea that with thought alone one can prove geometric statements inspired him to study mathematics.</p>
<p>In high school he was taught mathematics by <a href=""https://en.wikipedia.org/wiki/Elisha_Netanyahu"" target=""_blank"">Elisha Netanyahu</a>, the uncle of the later Israeli Prime Minister. Netanyahu was an important mathematician in his own right, and later became a professor at the Technion in Haifa and Dean of the Faculty of Science. While a high school teacher, Netanyahu organized a weekly seminar to teach topics in advanced mathematics to a select group of students. Rabin participated, and quickly learned much more than would be the norm for a student of his age.</p>
<p>Rabin finished high school at the age of 16. Like most of his classmates, he was then drafted into the army to fight for the independence of the then-new state of Israel. He filled the periods of inactivity by reading mathematics textbooks. One by Professor <a href=""https://en.wikipedia.org/wiki/Abraham_Fraenkel"" target=""_blank"">Abraham Fraenkel</a> in Jerusalem was on set theory, and Rabin wrote to him. Fraenkel, impressed by the depth of the correspondence, met with Rabin and later was instrumental in having him mustered out of the military to attend the University of Jerusalem. He was admitted directly into a Master’s degree program to study algebra, and graduated in 1953. His thesis solved a significant open problem that had been proposed by the German mathematician <a href=""https://en.wikipedia.org/wiki/Emmy_Noether"" target=""_blank"">Emmy Noether</a>. On the strength of the thesis he was admitted to a PhD program at Princeton, where he studied under <a href=""https://en.wikipedia.org/wiki/Alonzo_Church"" target=""_blank"">Alonzo Church</a> and graduated in 1957.</p>
<p>After Rabin finished his PhD he was invited by IBM to attend a summer research workshop for a select group of young scientists. It was there that he and <a href=""/award_winners/scott_1193622.cfm"">Dana Scott</a>&nbsp;collaborated on the famous paper “Finite Automata and Their Decision Problem” [<a href=""/bib/rabin_9681074.cfm#bib_1"">1</a>] that led to their joint Turing Award in 1976.</p>
<p>Automata theory had really begun with the 1943 study of artificial neural networks by <a href=""https://en.wikipedia.org/wiki/Walter_Pitts"" target=""_blank"">Walter Pitts</a> and <a href=""https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch"" target=""_blank"">Warren McCulloch</a>. Others continued this biologically-inspired work. Rabin and Scott moved away from neural networks, and instead used a computational model known as a <a href=""https://en.wikipedia.org/wiki/Finite-state_machine"" target=""_blank"">finite state machine</a>. These theoretical machines, like the Turing machine, move from one state to another depending on the input and the defined transition rules. Finite state machines had been investigated before, but Rabin and Scott considered different kinds. One was a <a href=""https://en.wikipedia.org/wiki/Non-deterministic_Turing_machine"" target=""_blank"">nondeterministic machine</a> that did not just have one possible transition out of each state, but had several. Essentially the machine could, upon accepting an input symbol, replicate itself, and then each machine would proceed with the computation along one of the possible transitions. As noted in the citation for the Turing Award, this concept of a nondeterministic machine has proven to be extremely valuable in the theoretical investigation of many problems, and continues to be an inspiration for new work.</p>
<p>The next summer Rabin was again invited to the IBM research workshop. &nbsp;He met another future Turing Award recipient, <a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy</a>, who explained a puzzle to him about spies and guards. The spies have passwords that allow them to pass from enemy territory to their own. The guards cannot be trusted to keep the passwords secret, so some method had to be found to verify that even if the enemy gains knowledge of the password, the spies can safely return but the enemy infiltrators are kept out. One solution came from the <a href=""https://en.wikipedia.org/wiki/Middle-square_method"" target=""_blank"">middle-square method</a>, which had been proposed by mathematician John von Neumann as a way of generating random numbers. Each spy is given a 100-digit number <em>x,</em> and the guards are given another 100-digit number obtained by taking the middle digits from <em>x</em><sup>2</sup>. When returning, the spy gives the guard <em>x.</em> The guard then computes <em>x</em><sup>2</sup> and compares the middle digits to the number he possesses as a pass code. Even if the guard passes his number to an enemy, it is very difficult for the enemy to determine what initial number resulted in those 100 middle digits of <em>x</em><sup>2</sup>.</p>
<p>Rabin began thinking in general about functions that are difficult to invert —in this case computing the original number <em>x</em> knowing only the middle digits of <em>x</em><sup>2</sup>. His study resulted in the groundbreaking paper [<a href=""/bib/rabin_9681074.cfm#bib_2"">2</a>] “Degree of Difficulty of Computing a Function and a Partial Ordering of Recursive Sets”, &nbsp;which was the starting point for his later advances in the theoretical study of computational complexity particularly in relation to cryptography.</p>
<p>Rabin had returned to the University of Jerusalem, first as a senior lecturer, then as Associate Professor, and then Professor. He kept up his prodigious research output while also becoming chairman of their Institute of Mathematics, chairman of the Department of Computer Science, and Rector (academic head) of the entire University.</p>
<p>In 1975, having finished his term at Rector, he went to MIT as a Visiting Professor and worked on primality testing with Gary Miller. This involves determining if a very large number is prime—whether the number has no divisors other than itself and 1. Miller had earlier developed a primality test that was based on the unproven Riemann hypothesis. That bothered Rabin, because if the Riemann hypothesis was eventually shown to be false it would bring into question any methods based on it. Michael had earlier worked on <a href=""https://en.wikipedia.org/wiki/Probabilistic_automaton"" target=""_blank"">probabilistic automata</a>, theoretical machines for which a random number is used to determine which transition to take from each state. While not deterministic in the sense that it always provides a provable result, if run a number of times the chance of it being incorrect can be made vanishingly small. Rabin used this concept to develop a primality testing algorithm [<a href=""/bib/rabin_9681074.cfm#bib_3"">3</a>] called the <a href=""https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test"" target=""_blank"">Miller-Rabin test</a>. It was later shown to be deterministic: it is guaranteed to work if a certain number of tests were done.</p>
<p>Adding randomness to algorithms was to be a theme for much of Rabin’s later work on many different problems. One of his favorites, the <a href=""https://en.wikipedia.org/wiki/Lagrange%27s_four-square_theorem"" target=""_blank"">four-square problem</a> that had first been discussed by <a href=""https://en.wikipedia.org/wiki/Joseph_Louis_Lagrange"" target=""_blank"">Joseph-Louis Lagrange</a> in 1770, was how to express an integer as the sum of four squares: for any integer <em>y</em> find four not necessarily unique, integers <em>a</em>, <em>b</em>, <em>c</em>, <em>d</em> such that <em>y</em> = <em>a</em><sup>2</sup> + <em>b</em><sup>2</sup> + <em>c</em><sup>2</sup> + <em>d</em><sup>2</sup>. Lagrange had shown that it is always possible, but nobody knew an efficient algorithm to find <em>a</em>, <em>b</em>, <em>c</em> and <em>d</em>. In speaking to a class of students at MIT in 1977 Rabin proposed a randomized algorithm, which he and Jeff Shallit later published as part of a larger study of randomized algorithms [<a href=""/bib/rabin_9681074.cfm#bib_4"">4</a>].&nbsp;</p>
<p>Rabin’s later work concerns cryptographic problems for preventing piracy on the internet. Recently he has been examining how to ensure the privacy and secrecy of online auctions. In auctions like those conducted by Google for advertising slots, the participants want their identity and bidding strategy to remain anonymous, but want to be assured that the results of the auction are fair. Rabin has worked as a consultant to create a <a href=""https://en.wikipedia.org/wiki/Zero-knowledge_proof"" target=""_blank"">zero-knowledge proof</a> that gives such assurances.</p>","<div class=""featured-photo"">
<a href=""/award_winners/rabin_9681074.cfm""><img src=""/images/lg_aw/9681074.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Michael O. Rabin ""></a>
</div>

<h6 class=""label""><strong>BIRTH:</strong></h6>
<p>September 1, 1931 Breslau, Germany (now Wroclaw<strong>, </strong>Poland)</p>
<h6 class=""label""><strong>EDUCATION</strong><strong>:</strong></h6>
<p>M.Sc., (Mathematics, Hebrew University, 1953); Ph.D., Mathematics, Princeton University, 1957.</p>
<h6 class=""label""><strong>EXPERIENCE:</strong></h6>
<p>Professor, Harvard (Gordon McKay Professor of Computer Science,1981-1983; Thomas J. Watson Sr. Professor of Computer Science 1983); Professor, Hebrew University of Jerusalem (Albert Einstein Chair,1980-1999; Pro-Rector, 1976-1980); Rector (Academic Head) 1972-1975; Chairman, Computer Science Department 1970-1971; Chairman, Institute of Mathematics 1964-1966; Senior Lecturer, Associate Professor and Professor 1958-1965); Institute for Advanced Study, Princeton (1958 Member; H. B. Fine Instructor1956-1958). He has also held many different visiting appointments at major universities in Europe and the United States.</p>
<h6 class=""label""><strong>HONORS AND AWARDS:</strong></h6>
<p>C. Weizmann Prize for Exact Sciences (1960); Best Teacher Award, Courant Institute of Mathematics (1970); Rothschild Prize in Mathematics (1974); Member American Academy of Arts and Sciences (1975); ACM Turing Award (1976); Harvey Prize in Science and Technology (1980); Israel Academy of Sciences and Humanities (1982); Foreign Associate US National Academy of Science (1984); Foreign Member American Philosophical Society (1988); President, Division for Logic, Methodology, and Philosophy of Science, IUHPS (1990-2003); Israel Prize in Exact Sciences/Computer Science (1995); Associé&nbsp;<em>Étranger</em>, French Academy of Sciences (1995); Honorary Doctorate, University of Bordeaux I (1996); Honorary Doctorate, Haifa University (1996); Honorary Doctorate, New York University (1998); Honorary Doctorate, Israel Open University Honorary Fellow (1999); IEEE Charles Babbage Award in Computer Science (2000); Honorary Doctorate, Ben-Gurion University (2000); EMET Prize in Exact Sciences/Computer Science (2004); ACM Kanellakis Theory and Pratice Award (2004); ASL Godel Award Lecture (2004); Member European Academy of Science (2007); Foreign Member Royal Society (2007); Honorary Doctorate, Wroclaw University (2007); IACR Fellow (2009); Dijkstra Prize (2015); Tel Aviv University Dan David Prize (“Future” category) jointly with Leonard Kleinrock and Gordon E. Moore, for Computers and Telecommunications.</p>","","https://dl.acm.org/author_page.cfm?id=81100510853","Michael O. Rabin","<li class=""bibliography""><a href=""/bib/rabin_9681074.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283931&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/rabin_9681074.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/interviews/rabin_9681074.cfm""><span></span>Video Interview</a></li>
<li class=""additional""><a href=""//dl.acm.org/citation.cfm?id=1529269"" target=""_blank""><span></span>Oral History Interview</a></li>"
"1573179550-691","https://amturing.acm.org/award_winners/kahn_4598637.cfm","With Vinton Cerf, for pioneering work on internetworking, including the design and implementation of the Internet's basic communications protocols, TCP/IP, and for inspired leadership in networking.","<p><strong><a href=""/award_winners/cerf_1083211.cfm"">Vinton G. Cerf</a> and Robert E. Kahn led the design and implementation of the Transmission Control Protocol and Internet Protocol (TCP/IP) that are the basis for the current internet.</strong> They formulated fundamental design principles of networking, specified TCP/IP to meet these requirements, prototyped TCP/IP, and coordinated several early TCP/IP implementations. Since then, they have continued to provide leadership in the networking research community and in the emerging industries of the internet and telecommunications.</p>
<p><em><strong>Background</strong></em></p>
<p>Robert Kahn was born&nbsp;23 December&nbsp;1938, in Brooklyn, New York. He earned his B.E.E. in electrical engineering at the City College of New York in 1960 and went on to earn his M.A. (1962) and Ph.D. (1964) in electrical engineering from Princeton. He took a job at AT&amp;T Bell Labs in 1964 and then joined the electrical engineering department at MIT as an assistant professor in 1966. Wishing to do more applied research, in 1967 Kahn arranged a leave of absence from MIT in to work at the firm of Bolt, Beranek and Newman (now known as BBN), where he began developing his own ideas for computer networking.</p>
<p>In 1968, Lawrence Roberts of the US Department of Defense Advanced Research Projects Agency issued a request for proposals for an experimental large-scale network. The ARPANET project proposed using emerging techniques such as <a href=""/info/kahn_4598637.cfm"">packet switching&nbsp;</a>and&nbsp;<a href=""/info/kahn_4598637.cfm"">distributed communications</a>,&nbsp;along with an unheard-of diversity of computer hardware and operating systems. The ARPANET’s backbone was to consist of a set of long-distance telephone lines connected by packet switches. BBN’s role was to provide the hardware and software for these switches, called <a href=""https://en.wikipedia.org/wiki/Interface_Message_Processor"" target=""_blank"">Interface Message Processors</a> or IMPs. Kahn played a key role in system design for the proposal that ultimately won BBN the contract, and he subsequently joined the ARPANET development team led by Frank Heart.</p>
<p>In 1972, Kahn took the lead in organizing the first public demonstration of the ARPANET at the October International Computer Communication Conference in Washington, D.C. Prior to that demonstration the ARPANET had been an interesting but underused experimental system but, with Kahn’s encouragement, people at the various sites began to bring new applications online, making the network more attractive to users. This effort brought the ARPANET to maturity and dramatically introduced the network to the larger computer science world.</p>
<p>Vint Cerf was born&nbsp;23 June 1943 in New Haven, CT. He suffered from a hearing impairment from an early age, and he later attributed some of his interest in computer networking to its promise as an alternative communications channel for the hearing impaired. Cerf received his B.S. in Mathematics from Stanford University in 1965, then worked for IBM for two years, where he contributed to Quicktran, a FORTRAN based time-sharing system. This whetted his interest in computer science, and he left IBM to study at the University of California, Los Angeles, where he earned his M.S. (1970) and Ph.D. (1972) in Computer Science.</p>
<p>As a graduate student at UCLA, Cerf became involved in the ARPANET through Leonard Kleinrock, an expert in queuing theory who had a contract to do performance analysis for the new network. As part of this role, UCLA was given one of the first four ARPANET nodes. Cerf became deeply involved in the ongoing discussion and development of the ARPANET host computer software (NCP) through the Network Working Group, whose informal, decentralized mode of operation would become the model for Internet protocol development and open software.</p>
<p><em><strong>Joint work on the Internet</strong></em><br>
<br>
Cerf and Kahn first met when Kahn came to UCLA in 1969 to help test the nascent ARPANET. The two formed an effective working relationship to generate test data and predict and diagnose problems in the network.</p>
<p>In late 1972, Kahn joined the <a href=""https://en.wikipedia.org/wiki/Information_Processing_Techniques_Office"" target=""_blank"">Information Processing Techniques Office </a>(part of the US Defense Advanced Research Projects Agency – usually known as “IPTO”) as a program manager and initiated projects in network security, digital speech transmission, and packet radio. In 1973, building on a previous ARPA-funded project called <a href=""https://en.wikipedia.org/wiki/ALOHAnet"" target=""_blank"">Alohanet</a>, Kahn initiated a ground-based packet-radio project, called <a href=""https://en.wikipedia.org/wiki/Packet_radio"" target=""_blank"">PRNET</a>, which started experimental operation in 1975. Kahn also began experimenting with using the Intelsat I satellite to link the Arpanet to sites in Britain and Norway (where ARPA conducted seismic monitoring to detect Soviet underground nuclear tests). In 1975 this effort grew into the Atlantic Packet Satellite Network (SATNET), an experimental transatlantic network operated in conjunction with the British Post Office and Norwegian Telecommunications Authority.</p>
<p>By 1973 Kahn was already thinking about connecting ARPA’s packet radio and satellite networks to the ARPANET, but he faced formidable challenges, since the three networks were technologically incompatible. ARPANET used point-to-point transmission, while the radio networks used broadcast; ARPANET tried to guarantee reliable transmission of packets, while PRNET did not; and SATNET had longer transmission delays because of the great distance the packets had to travel. Successfully connecting such diverse networks would require a new approach.</p>
<p>In the spring of 1973, Kahn approached Cerf with the idea of developing a system for interconnecting networks—what would eventually be called an “internet.” Kahn felt that his own knowledge of the problem of connecting dissimilar networks, combined with Cerf’s expertise in writing host software, would create a strong partnership. In addition, Kahn and Cerf demonstrated farsighted leadership by inviting networking experts from around the world to weigh in on the Internet design at a seminar in June 1973. This move not only led to more robust protocols, but also laid the groundwork for the global spread of the Internet.<br>
Cerf and Kahn outlined the resulting Internet architecture in a seminal 1974 paper, <em>A Protocol for Packet Network Intercommunication </em>[<a href=""/bib/kahn_4598637.cfm#link_2"">2</a>].</p>
<p>There were two key elements. First was a host protocol called the Transmission Control Protocol (TCP), which was intended to provide reliable, ordered, flow-controlled transmission of packets over the interlinked networks. Second was a set of gateways or routers that would sit between networks, passing traffic between them and handling inter-network addressing and routing. There was also a hierarchical address system, whereby packets were first sent by the gateways to a network address and then directed internally to a host address within that network. The Internet architecture was designed to make minimal demands on participating networks, to provide a seamless user experience, and to scale up gracefully, key features that would facilitate the Internet’s rapid expansion in the 1990s.</p>
<p>Cerf initially worked on developing the Internet protocols as an ARPA contractor at UCLA, then moved to IPTO as program manager for networking in 1976, staying at the agency until 1982. In 1978 he collaborated with Jon Postel and Danny Cohen, both at the University of Southern California Information Sciences Institute, to reformulate TCP as a set of two protocols: a host protocol (TCP) and a separate internetwork protocol (IP). IP would be a stripped-down protocol for passing packets within or between networks; it would run on both hosts and gateways, while the more complex TCP would run only on hosts and provide reliable end-to-end service. The new TCP/IP architecture, described in Cerf’s 1980 article “<em>Protocols for Interconnected Packet Networks</em>” [<a href=""/bib/kahn_4598637.cfm#link_3"">3</a>],&nbsp;simplified the operation of internet gateways and helped increase the number and diversity of the networks connected to the Internet.</p>
<p>Cerf and Kahn oversaw the implementation of TCP and the experimental connection of ARPANET, PRNET and SATNET in 1977; this became the first incarnation of the Internet. Kahn became Director of IPTO in 1979, serving until 1985. He helped guide the changeover of ARPANET sites from the original NCP protocol to TCP/IP in 1983. Also in 1983, Kahn initiated ARPA’s Strategic Computing Initiative, a billion-dollar research program that included chip design, parallel computer architectures, and artificial intelligence.</p>
<p><em><strong>Later activities</strong></em><br>
<br>
In 1986 Kahn founded the <a href=""https://en.wikipedia.org/wiki/Corporation_for_National_Research_Initiatives"" target=""_blank"">Corporation for National Research Initiatives</a> (CNRI), where he remains Chairman, CEO, and President. CNRI is a not-for-profit organization that provides technical leadership and funds research and development of emerging information infrastructure components, such as digital object identifiers and “knowbots,” mobile software agents that operate across networked environments.</p>
<p>Cerf left ARPA in 1982 to become Vice president of Digital Information Services at <a href=""https://en.wikipedia.org/wiki/MCI_Communications"" target=""_blank"">MCI</a>, where he created MCI Mail. Cerf arranged for MCI Mail to become the first commercial email service to use the Internet in 1989. Cerf later returned to MCI as Senior Vice President from 1994-2005.</p>
<p>In 1986 Cerf joined Kahn as Vice President of CNRI. In 1991, recognizing the need for a neutral forum for Internet standards development, Cerf and Kahn founded the <a href=""https://en.wikipedia.org/wiki/Internet_Society"" target=""_blank"">Internet Society </a>(ISOC), an international nonprofit organization. ISOC provided an institutional home for the Internet Engineering Task Force, which sets technical standards for the Internet, and eventually expanded into policy and educational activities. Cerf served as President of ISOC from 1992 until 1995. He also served as chairman of the board of the Internet Corporation for Assigned Names and Numbers (ICANN), which coordinates the&nbsp;<a href=""/info/kahn_4598637.cfm"">domain name</a>&nbsp;system among other functions, from 2000-2007. In 2005 Cerf was hired by Google as Vice president and Chief Internet Evangelist.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Janet Abbate</span><br>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/kahn_4598637.cfm""><img src=""/images/lg_aw/4598637.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Robert E Kahn""></a>
<br><br>
<h6 class=""label""><a href=""/photo/kahn_4598637.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>23 December 1938, Brooklyn, New York.</p>
<h6 class=""label"">EDUCATION:</h6>
<p>B.E.E. (City College of New York, 1960); M.A. (Electrical Engineering, Princeton, 1962); Ph.D. (Electrical Engineering, Princeton, 1964).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Technical Staff, AT&amp;T Bell labs (1964-66); Bolt, Beranek and Newman (BBN) (on leave from MIT) (1966-72); Program Manager / Director, Information Processing Techniques Office, Defense Advanced Research Projects Agency (DARPA) (1972-1985); Chairman, CEO and President of the Corporation for National Research Initiatives (CNRI) (from 1986).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>SIGCOMM Award, 1993;&nbsp;Marconi Award, 1994; US National Medal of Technology, 1997; IEEE Alexander Graham Bell Medal, 1997; National Academy of Engineering Charles Stark Draper Prize, 2001; ACM Turing Award, 2004; Inductee, National Inventors Hall of Fame, 2006; US Presidential Medal of Freedom, 2005; Fellow of the Computer History Museum, 2006; Japan Prize, 2008; Harold Pender Award, 2010; Queen Elizabeth Prize for Engineering, 2013; AFIPS Harry Goode Memorial Award; President's Award from ACM; ASIS Special Award; Secretary of Defense Civilian Service Award (twice). Kahn has also been named a Fellow of the following organizations: National Academy of Engineering; American Academy of Arts &amp; Sciences; New York Academy of Sciences, IEEE, ACM, AAAI, and an Honorary Fellow of the University College London. He holds honorary doctorates from several institutions.<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100452904","Robert (“Bob”) Elliot Kahn","<li class=""bibliography""><a href=""/bib/kahn_4598637.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/kahn_4598637.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/kahn_4598637.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/kahn_4598637.cfm""><span></span>Additional<br> Materials</a></li>"
"1573178983-653","https://amturing.acm.org/award_winners/perlis_0132439.cfm","For his influence in the area of advanced programming techniques and compiler construction.","<p><strong>Alan J&nbsp;Perlis was born on April 1, 1922 into a Jewish family in Pittsburgh, Pennsylvania. </strong>He attended primary schools in his hometown, starting with the Colfax Public School in his Squirrel Hill neighborhood. In 1933 Perlis enrolled in the public but prestigious Taylor Allderdice High School. Six years later he entered Carnegie Institute of Technology and graduated with honors in Chemistry on December 20 1942.</p>
<p>Two days after graduation, Perlis started his war-time experience: on the morning of December 22 he signed up for the Aviation Cadet Meteorology Program of the US Army Air Force. After nine months of training he became a 2nd lieutenant in the meteorology services. He was assigned to the Army Air Force Intelligence School in Harrisburg, Pennsylvania, a training center for photo interpretation and combat intelligence. Perlis was later sent to the operational headquarters of the 9th US Army Air Force in the United Kingdom, where he served for eighteen months as intelligence officer and weather officer with a reconnaissance squadron.</p>
<p>In September 1945, he returned to civilian life and enrolled in the California Institute of Technology for graduate study in chemistry. He quickly realized that this was not his passion and switched to the study of mathematics at the Massachusetts Institute of Technology. At MIT Perlis carried out research on numerical analysis methods, under the supervision of Philip Franklin, which led to the completion of a master thesis in 1949 (The solution of linear integral equations by iterative methods) and a PhD dissertation in 1950 (On integral equations, their solution by iteration and analytic continuation).</p>
<p>More relevant for Perlis’ future career was Philip Franklin’s involvement with Project Whirlwind, MIT’s digital computer project. By 1946 <a href=""https://en.wikipedia.org/wiki/Whirlwind_%28computer%29"" target=""_blank"">Project Whirlwind </a>had evolved to become the blueprint for the construction of an ambitious digital computer capable of providing rapid solutions to all types of simulations and control problems, such as tracking the motion of aircraft. Franklin was head of the Mathematics Group, which was responsible for the preparation of software programs for the planned machine.</p>
<p>Project Whirlwind was the introduction to modern computing for many young MIT graduates. Graduate students with military experience, like Perlis, were invited to collaborate on the project on part-time basis. During the summers of 1948 and 1949 Perlis assisted Franklin’s team with coding basic programs for the still-unfinished computer, and in surveying known numerical methods, looking for those that would be a good fit with the characteristics of the Whirlwind when it became operational.</p>
<p>After receiving his PhD in mathematics, Perlis spent 1951 in the <a href=""https://en.wikipedia.org/wiki/Ballistic_Research_Laboratory"" target=""_blank"">Ballistic Research Laboratory </a>(BRL) at Aberdeen Proving Ground, Maryland, where he worked on the computation of ballistics tables. Like Project Whirlwind, Aberdeen Proving Ground was a cradle of new research on computer design, programming, and numerical methods. It was one of the first multi-machine computing installations, housing three different computers: <a href=""https://en.wikipedia.org/wiki/ENIAC"" target=""_blank"">ENIAC</a> (transferred to BRL in 1947), <a href=""https://en.wikipedia.org/wiki/EDVAC"" target=""_blank"">EDVAC</a> (transferred in 1949), and <a href=""https://en.wikipedia.org/wiki/ORDVAC"" target=""_blank"">ORDVAC</a> (transferred in the spring of 1951).</p>
<p>In January 1952 Perlis came back to the finished and operational Project Whirlwind, now renamed the MIT Digital Computer Laboratory. This time Perlis was assigned to Robert Wieser’s team in charge of Project Cape Cod, a prototype system to automate the detection of enemy aircraft. Operational in 1953, the Cape Cod system established the basis for the first full-scale automatic air defense system, <a href=""https://en.wikipedia.org/wiki/Semi_Automatic_Ground_Environment"" target=""_blank"">SAGE</a> (Semi-Automatic Ground Environment).</p>
<p>Perlis did not stay long at MIT. In September he joined the Statistical Laboratory at Purdue University, which was expanding its activities under the leadership of Carl F. Kossac. In 1952 Purdue's Statistical Lab was about to receive an IBM card-programmed electronic calculator (<a href=""https://en.wikipedia.org/wiki/IBM_CPC"" target=""_blank"">CPC</a>), an assembly of basic accounting equipment sophisticated enough to require the services of someone skilled in modern computing technology. On the recommendation of Alex Orden (one of Perlis’s colleague at MIT), Kossac recruited Perlis both as an assistant professor in the Mathematics Department and as the director of the computational division of the Statistical Laboratory.</p>
<p>At Purdue Alan Perlis began the research that would lead him to eminence. Shortly after his arrival, Perlis persuaded Kossac and the university administrators to purchase the best medium size computer available at that time, the <a href=""https://en.wikipedia.org/wiki/Burroughs_Corporation"" target=""_blank"">Datatron 205</a>. In early 1955, Perlis’ team began the design of a “mathematical language compiler” for it, later named the IT language (for Internal Translator).</p>
<p>IT was a “compiler”—a set of programs that automatically translate programs written in a notation similar to that of mathematics into machine code. In this respect IT was not very different from IBM’s Fortran language, which was also under development at that time. IT was interesting in that its logical structure was sufficiently independent of the Datatron 205 to allow for implementations on other computers with only few modifications. This relatively machine-independent character of IT was developed earlier than most higher level languages.</p>
<p>In 1956 Perlis left Purdue to accept a position as associate professor of mathematics and director of the computation centre at the Carnegie Institute of Technology (now Carnegie Mellon University). There he completed the first version of IT on an <a href=""https://en.wikipedia.org/wiki/IBM_650"" target=""_blank"">IBM 650 computer</a>. Soon IT became adopted by many university computing installations as an alternative to Fortran, which, at first, was only available for much larger and more expensive computers. In this way, IT contributed much to stimulate research on programming techniques. Perlis’ IT seemed to hint at a way towards a universal language or at least towards a small set of mutually translatable languages. Perlis himself was to play an important role in this process.</p>
<p>In the spring of 1957 representatives of the <a href=""https://en.wikipedia.org/wiki/Users%27_group"" target=""_blank"">user groups </a>SHARE (IBM), USE (UNIVAC) and DUO (Datatron) requested that ACM evaluate the prospects of a “universal programming language”—a language capable of transcending the characteristics of particular machines and allowing the creation of machine-independent programs for many kinds of applications.</p>
<p>This led to contacts between the ACM and a German-Swiss group of programming specialists led by <a href=""https://en.wikipedia.org/wiki/Heinz_Rutishauser"" target=""_blank"">Heinz Rutishauser</a> and <a href=""https://en.wikipedia.org/wiki/Friedrich_L._Bauer"" target=""_blank"">Friederich L. Bauer</a>. In May 1958, under the leadership of Perlis, a small group of ACM representatives met in Zurich with the German-Swiss group. This meeting resulted in the definition of the International Algorithmic Language (IAL) and its publication as report [<a href=""/bib/perlis_0132439.cfm#link_4"">4</a>] in the<em> Communications of the ACM</em>, of which Perlis had become the first editor-in-chief. In the next year, IAL was renamed ALGOL (for Algorithmic Language) and was widely discussed in forums like <em>Communications of the ACM</em>, which resulted in a new definition of the language in early 1960 called ALGOL-60.</p>
<p>Arguably one of the most influential programming languages in history, ALGOL-60 had a very complex and controversial history in its early years. Nevertheless, ALGOL-60 brought about a qualitative leap forward in the understanding and acceptance of programming as a legitimate object of study, not just a practical necessity for getting a computer to work. In this regard, Perlis played an important role in turning ALGOL into a model for programming research, for example by working with his colleagues at Carnegie on several extensions of the language, and by publicly arguing [<a href=""/bib/perlis_0132439.cfm#link_2"">2</a>] for the centrality of programming languages and algorithms as a defining concept of computer science.</p>
<p>Equally important during those years was Perlis’ leadership in helping to mold the nascent field of Computer Science into an academic discipline. Particularly relevant was the establishment of the ACM Curriculum Committee on Computer Science during Perlis’ term as President of the ACM (1962-1964). The work of this committee led to the publication of the first recommendation for a Computer Science undergraduate program. In addition, at Carnegie-Mellon, Perlis was able to secure contracts from the Department of Defense Advanced Research Projects Agency (ARPA), which proved to be a great help in launching their Department of Computer Science in 1965.</p>
<p>In 1971 Perlis moved to Yale University as the Eugene Higgins Professor of Computer Science in their new Computer Science Department. Except a&nbsp;brief interlude in the 1977-1978 academic year, when he was appointed Gordon and Betty Moore Professor of Computer Science at the California Institute of Technology, Perlis remained at Yale until his death on February 7, 1990.</p>
<p style=""text-align: right;""><span class=""callout"">Author: David Nofre</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/perlis_0132439.cfm""><img src=""/images/lg_aw/0132439.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""A. J. Perlis ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/perlis_0132439.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>April 1 1922, Pittsburgh, Pennsylvania, US</p>
<h6 class=""label"">DEATH:</h6>
<p>February 7 1990, New Haven, Connecticut,&nbsp;US</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Taylor Allderdice High School (1933-1939), Pittsburgh, Pennsylvania; B.S. (1942, Carnegie Institute of Technology, Chemistry); M.Sc. (1949, Massachusetts Institute of Technology, Mathematics), PhD (Massachusetts Institute of Technology, 1950, Mathematics). Honorary degrees: Davis and Elkins College, Purdue University, Waterloo University, and Sacred Heart University.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>MIT (1945-1950); Ballistic Research Laboratory, Aberdeen Proving Ground (1951); MIT (1952); Purdue University (1952-1956); Carnegie Institute of Technology (1956-1971); first editor of the Communications of the ACM (1958-1962); California Institute of Technology (1977-1978); Yale University (1971-1990).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>President of the ACM (1962-1964); A. M. Turing Award of the ACM (1966); Fellow, American Academy of Arts and Sciences (1974); Member, National Academy of Engineering (1977); AFIPS Education Award (1984); Computer Pioneer Award (1986); Member, National Research Council (1979-1989), where he served on the Assembly (1979-1981), the Computational Mechanics Committee of the Assembly of Engineering (1981-1985), the Commission on Engineering and Technical Systems (1982-1989), the Commission of Physical Sciences, Mathematics, and Resources (1988-1989).</p>","","https://dl.acm.org/author_page.cfm?id=81100086771","Alan J Perlis","<li class=""bibliography""><a href=""/bib/perlis_0132439.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283921&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/perlis_0132439.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179503-688","https://amturing.acm.org/award_winners/allen_1012327.cfm","For pioneering contributions to the theory and practice of optimizing compiler techniques that laid the foundation for modern optimizing compilers and automatic parallel execution.
press release","<p><strong>Frances Elizabeth (""Fran"") Allen was born August 4, 1932.</strong> She was the oldest of six children and grew up on a farm in Peru, New York, near Lake Champlain. Her father was a farmer, and her mother an elementary-school teacher.</p>
<p>In high school she was greatly inspired by her math teacher, and set out to become a math teacher herself. She attended the New York State College for Teachers (now the State University of New York at Albany) for four years and earned a BA in mathematics with a minor in physics as well as education credits. She then taught math -- everything from elementary algebra to advanced trigonometry -- for two years at the same high school she had attended.</p>
<p>She realized that she would need a master's degree to be fully certified as a teacher. After taking some summer courses at Columbia University, she enrolled at the University of Michigan at Ann Arbor and earned an MA in mathematics. She also took courses in computing there—some of the first ever offered—and learned how to program an <a href=""https://en.wikipedia.org/wiki/IBM_650"" target=""_blank"">IBM 650</a> from <a href=""https://en.wikipedia.org/wiki/Bernard_Galler"" target=""_blank"">Bernard Galler</a>, who was a co-developer of the <a href=""https://en.wikipedia.org/wiki/MAD_%28programming_language%29"" target=""_blank"">MAD programming language</a> and later President of the ACM and an ACM Fellow. IBM held job interviews on the Michigan campus and offered her a job in research. She accepted with the idea that she would earn enough money to pay off her student debts and then return to teaching. Instead, she stayed at IBM for the next 45 years.</p>
<p>She joined IBM on July 15, 1957, exactly two months after the FORTRAN programming language had been released. Her first assignment was to teach research scientists within IBM how to use this language and indirectly encourage IBM customers to use it. She did what teachers often must: she learned the subject matter just a few days ahead of her students. As part of this process, she read the source code for the FORTRAN compiler that had been developed by <a href=""/award_winners/backus_0703524.cfm"">John Backus</a>&nbsp;(later a Turing Award winner) and his team. In her words, ""It set my interest in compiling, and it also set the way I thought about compilers, because it was organized in a way that has a direct heritage to modern compilers.""</p>
<p>She spent most of the rest of her career developing cutting-edge programming language compilers for IBM Research. At first she worked on the IBM 704’s Monitored Automatic Debugging operating system (developed by <a href=""https://en.wikipedia.org/wiki/Roy_Nutt"" target=""_blank"">Roy Nutt</a>, who had also implemented the FORMAT statement for the FORTRAN compiler), but her first major project was for the <a href=""https://en.wikipedia.org/wiki/IBM_7950_Harvest"" target=""_blank"">Stretch/Harvest</a> computer. Stretch was one of the first supercomputers, and Harvest was a coprocessor for Stretch that had been designed for the US National Security Agency (NSA) to do codebreaking of secret messages. Allen and her team designed a single compiler framework to handle three very different programming languages: FORTRAN, Autocoder (a business language similar to COBOL), and the new language Alpha (designed for rapidly detecting patterns in arbitrary text represented in any alphabet). The three language compilers shared a common optimizing back end that could produce code for both the Stretch supercomputer and its Harvest coprocessor. This was an extraordinarily ambitious effort for the time, and they pulled it off. Allen served as the liaison between IBM and NSA, coordinating the design of the Alpha language and its acceptance tests. She spent a year in 1962 at NSA overseeing the installation and testing of the system, which was used for 14 years before being retired in 1976.</p>
<p>At that point she was offered the opportunity to coordinate programming language work for the IBM System/360, but that would have required a lot of traveling. After all the traveling she had done while working on Harvest, she decided to decline that offer and instead return to IBM Research at the new T. J. Watson Research building in Yorktown, New York. She joined <a href=""https://en.wikipedia.org/wiki/ACS-1"" target=""_blank"">Project Y</a>, which allowed her to collaborate once again with <a href=""/award_winners/cocke_2083115.cfm"">John Cocke</a>,&nbsp;another future Turing Award winner who had worked on the hardware for Stretch. Project Y, later called the Advanced Computing Systems project (ACS), included another set of cutting-edge advances in computer system design. The hardware might be described in today's terms as the first ""<a href=""https://en.wikipedia.org/wiki/Superscalar"" target=""_blank"">superscalar</a>"" processor, where the central processing unit does not execute instructions one at a time as in previous designs, but works on several instructions at once, perhaps even performing instructions ""in the wrong order"" to get the work done more quickly. The compiler techniques for this project represented equally novel technological improvements, including new ""flow analysis"" techniques that allowed the compiler to automatically optimize programs for greatly improved performance.</p>
<p>A key advance was to represent programs within the compiler not as a sequence of statements as in the original source code, but as a mathematical <a href=""https://en.wikipedia.org/wiki/Graph_theory"" target=""_blank"">graph</a> that could be analyzed to discover hidden properties of the code -- such as that a computed value could be re-used in another region of the code, or that it would definitely not be needed in yet another region of code. The techniques involved two clever tricks: labeling edges of the graph with mathematical sets, and then representing those sets with a very compact data structure that required only a single bit (0 or 1) of storage for each member of the set. That allowed the sets to be processed rapidly. In addition, a mathematical technique for decomposing the graphs into ""intervals"" allowed the sets attached to the edges to be processed in an order that usually resulted in the greatest efficiency. These techniques developed for the ACS compilers allowed both the compilers themselves and the programs they processed to execute much faster than in previous systems.</p>
<p>Allen was then assigned to work on IBM’s doomed ""<a href=""https://en.wikipedia.org/wiki/IBM_Future_Systems_project"" target=""_blank"">Future Systems</a>"" (FS) project, which aimed to revolutionize the way IBM built computer systems. She thought the machine architecture was technically flawed in ways that would limit performance. She wrote a letter to IBM management saying so, and recalls that it ""was kind of put on the shelf for a while."" After four years, the project was killed.</p>
<p>She took a sabbatical from IBM to teach graduate courses on compilers at the Courant Institute for Mathematical Sciences at New York University at the invitation of <a href=""https://en.wikipedia.org/wiki/Jacob_T._Schwartz"" target=""_blank"">Jacob ""Jack"" Schwartz</a>, creator of the <a href=""https://en.wikipedia.org/wiki/SETL"" target=""_blank"">SETL</a> programming language and the NYU <a href=""https://en.wikipedia.org/wiki/Ultracomputer"" target=""_blank"">Ultracomputer</a> at the Courant Institute. Schwartz had previously visited IBM to collaborate with her and John Cocke on ACS. Later, she and Schwartz were married.</p>
<p>Her next major project for IBM was the Experimental Compiler Systems project (ECS). This system, like the earlier compiler framework for Stretch/Harvest, was designed to support multiple programming languages. But the primary focus was on a new language called PL/I, which presented much more difficult problems for an optimizing compiler. ECS featured aggressive interprocedural analysis, procedure inlining, an extensive collection of optimizing transformations, and a runtime environment that allowed the free mixing of interpreted code and optimized compiled code.</p>
<p>Allen's last big project for IBM was the Parallel Translator (<a href=""https://dl.acm.org/citation.cfm?id=129260"" target=""_blank"">PTRAN</a>), a system for compiling Fortran programs not specially written with parallelism in mind for execution on parallel computer architectures. For this effort she consulted with <a href=""https://en.wikipedia.org/wiki/David_Kuck"" target=""_blank"">David Kuck</a> (later an ACM Fellow) at the University of Illinois, who worked for many years on parallelizing compilers. She eventually hired some of Kuck’s students, including Ron Cytron (later an ACM Fellow). She applied her extensive experience with interprocedural flow analysis to produce new algorithms for extracting parallelism from sequential code. PTRAN introduced the concept of the <a href=""https://en.wikipedia.org/wiki/Dependency_graph"" target=""_blank"">program dependence graph</a>, a representation now used by many parallelizing compilers.</p>
<p>Allen was named an IBM Fellow in 1989, an IEEE Fellow in 1991, and an ACM Fellow in 1994. She retired from IBM in 2002. As an IBM Fellow Emerita, she has continued to advise IBM on a number of projects including the <a href=""https://en.wikipedia.org/wiki/Blue_Gene"" target=""_blank"">Blue Gene</a> supercomputer, and has worked to encourage the involvement of other women in computer-related fields.</p>
<p>Allen has traveled internationally not only to give lectures about compilers, but also for athletic pursuits: she is a veteran of many mountain-climbing expeditions in Austria, China, Tibet, and elsewhere.</p>
<p>Fran Allen's focus has not been on inventing new programming languages or language features and then trying to get people to program using them. Rather, she focused on taking programs as programmers like to write them, and made them run efficiently by doing sophisticated analysis and optimization of the code. She didn’t create paper designs, but a series of working systems that run real programs, not just artificial benchmarks, faster. Today's programming language compilers still rely on techniques that she pioneered.&nbsp;</p>
<p style=""text-align: right; ""><span class=""callout"">Author: Guy Steele</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/allen_1012327.cfm""><img src=""/images/lg_aw/1012327.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Frances Allen""></a>
<br><br>
<h6 class=""label""><a href=""/photo/allen_1012327.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>August 4, 1932, Peru, New York</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>BA (mathematics, New York State College for Teachers, 1954); summer courses at Columbia University; MA (mathematics, University of Michigan, 1957); six honorary Doctor of Science degrees (University of Alberta, 1991; Pace University, 1999; University of Illinois Champaign-Urbana, 2004; University of Michigan, Ann Arbor, 2008; State University of New York at Albany, 2008; McGill University, 2009); honorary Doctor of Engineering degree (University of Notre Dame, 2008)</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>High school math teacher for two years; IBM (International Business Machines) Corporation (teaching FORTRAN to company scientists and engineers, 1957-1959; design and management of compiler for Stretch/Harvest computer, 1959-1963; design and development of experimental optimizing compiler for IBM ACS project, 1963-1968; compilers for Future Systems (FS) project, 1970-1973; visiting professor (partly on sabbatical from IBM) at New York University, 1970-1973; initiator and manager of Experimental Compiling Systems project, 1973-1980; development of product compilers, 1980-1985; initiator and manager of Parallel Programming Technology project, 1984-1994; IBM Fellow and President of the IBM Academy of Technology, 1995-1996; IBM Fellow and Senior Technical Advisor, SAS and BlueGene project, 1996-2002; IBM Fellow Emerita, from 2002)</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>IBM Corporate Award for Algorithms for Optimizing Compilers (1968); Member of the US National Academy of Engineering (1987); IBM Fellow (1989); IEEE Fellow (1991); ACM Fellow (1994); Fellow, American Academy of Arts and Sciences (1994); IEEE Computer Society Charles Babbage Award (1997); Women in Technology International (WITI) Hall of Fame (1997); Fellow, Computer History Museum (2000); Member, American Philosophical Society (2001); Association for Women in Computing Ada Lovelace Award (2002); IEEE Computer Society Computer Pioneer Award (2004); Anita Borg Technical Leadership Award (2004); ACM Turing Award (2006); Member of the US National Academy of Sciences (2010)<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100651807","Frances (""Fran"") Elizabeth Allen","<li class=""bibliography""><a href=""/bib/allen_1012327.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/allen_1012327.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/allen_1012327.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/allen_1012327.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/allen_1012327.cfm""><span></span>Video Interview</a></li>"
"1573178805-642","https://amturing.acm.org/award_winners/hartmanis_1059260.cfm","With Richard E. Stearns, in recognition of their seminal paper which established the foundations for the field of computational complexity theory.","<p>
<strong>Juris Hartmanis was the son of a senior Latvian army officer who was arrested when the Soviet Union occupied Latvia during World War II and subsequently died in prison.</strong> At the end of the War, the remaining members of the family moved to Germany where Juris obtained an undergraduate degree in physics from the University of Marburg in 1949. He then moved to the United States to begin graduate work and eventually received an MA in 1951 and a PhD in 1955, both in mathematics.</p>
<p>
Juris began his career with posts as an Instructor at Cornell University (1955–1957) and later as an Assistant Professor at Ohio State University (1957–1958). He then took a position as a Research Mathematician at General Electric Research Laboratory, which lasted for ten years.</p>
<p>
In the last few years of his time at General Electric, he and <a href=""/award_winners/stearns_1081900.cfm"">Richard Stearns</a> became interested in how much time and memory are needed to perform different computations – a field that they named <em>computational complexity</em>. They defined different classes of calculations according to how much calculation and/or memory space each would require, which is a measure of their difficulty. Their famous 1965 joint paper [<a href=""/bib/hartmanis_1059260.cfm#bib_7"">7</a>] began to interest other computer scientists in looking at complexity the same way.</p>
<p>
In contrast to other approaches to complexity theory, Hartmanis and Sterns based their analysis on measuring the resources that algorithms use when run on specific machines. In order to have some generality, they used a Turing Machine as their basic model. They were pleasantly surprised to discover a number of important mathematical theorems related to complexity analysis.</p>
<p>
The full Turing Award citation for Hartmanis and Stearns notes:</p>
<p style=""margin-left:.5in;"">
In their paper <em>On the Computational Complexity of Algorithms</em> they provided a precise definition of the complexity measure defined by computation time on Turing machines and developed a theory of complexity classes. The paper sparked the imagination of many computer scientists and led to the establishment of complexity theory as a fundamental part of the discipline.</p>
<p>
They introduced a basic concept called a <em>computational complexity class</em>. Informally, a class represents all the computations that can be done using a given amount of resources. For example, an individual class might contain all problems that use <em>N</em><sup>2</sup> steps, where <em>N</em> is the size of the problem. Some of today’s most important theoretical problem areas are the relations between these classes, such as the relation between the class of problems that can be solved in an amount of time that can be expressed as a polynomial function of N, and those that scale more quickly than can be represented by a polynomial. The classic unsolved P=NP problem asks whether there are problems whose answers can checked in polynomial time but will always take longer to solve, no matter how clever we are.</p>
<p>
Hartmanis’ work on the foundations of complexity theory was instrumental in establishing computer science as a formal discipline distinct from mathematics, physics and electrical engineering. In the context of later work by <a href=""/award_winners/rabin_9681074.cfm"">Michael Rabin</a>, <a href=""/award_winners/blum_4659082.cfm"">Manuel Blum</a> and others, Hartmanis and Stearns, in their seminal 1964 (conference) and 1965 (journal) papers [<a href=""/bib/hartmanis_1059260.cfm#bib_5"">5</a>, <a href=""/bib/hartmanis_1059260.cfm#bib_6"">6</a>], provided the basis for the development of complexity theory as it is now studied. In particular, they showed how Turing’s work in defining the limitations of “what is computable” could be extended to a model for “what is efficiently computable”.</p>
<p>
Many seminal results and insights were included in these two influential early papers. The multi-tape Turing machine was shown to be a precise and helpful model for sequential time complexity analysis. They showed the importance of asymptotic behavior, and the use and extension of Yamada’s real time counting functions [<a href=""#_ftn1"" name=""_ftnref1"" title="""">1</a>] to establish the existence of a hierarchy of <a href=""/info/hartmanis_1059260.cfm"">complexity classes</a>.</p>
<p>
The time-bounded complexity results were soon followed by a Lewis, Stearns and Hartmanis paper [<a href=""/bib/hartmanis_1059260.cfm#bib_8"">8</a>]&nbsp; establishing a similar hierarchy for space-bounded computation. In addition to the tighter hierarchy results, this paper defined a precise concept for sublinear space, and showed that problems such as context free language recognition could be solved with O(log2 <em>n</em>) space. The divide-and-conquer method used in this result was the starting point for <a href=""https://en.wikipedia.org/wiki/John_E._Savage"" target=""_blank"">John E. Savage’s</a> deterministic S2 space simulation of any non-deterministic space S ≥ log <em>n</em> computation. This in turn led to simulations of the tradeoff between parallel time and space, reinforcing the importance of sublinear space and the question of whether or not non-deterministic space can be deterministically simulated without any loss of efficiency.</p>
<p>
Hartmanis established complexity theory as the dominant theme in theoretical computer science with his major results concerning the structural nature of NP sets. In particular, he explored the question of whether or not all NP complete sets were isomorphic, i.e., whether all NP complete sets are basically the same set. Hartmanis and his student Leonard C. Berman showed that all natural NP complete sets are isomorphic (under polynomial time reductions), and further showed that complete sets computable in exponential time cannot be sparse.</p>
<p>
This <a href=""https://en.wikipedia.org/wiki/Berman%E2%80%93Hartmanis_conjecture"" target=""_blank"" title=""Berman–Hartmanis conjecture"">Berman–Hartmanis conjecture</a> is important. As Hartmanis himself explained it:</p>
<p style=""margin-left:.5in;"">
We conjectured that they are polynomial time isomorphic, which is a strictly defined term; that in structure they are basically identical—that one is just a permutation of another. And in some sense this conjecture could be used to date what is now known as structural complexity theory. And structural complexity theory refers to relating the structure of the different complexity class to each other. So one does not ask about less specific algorithms, but more interested in what are big, more global, structural relationships, like the question of do all NP complete sets really look the same, or are there different ones. [<a href=""#_ftn2"" name=""_ftnref2"" title="""">2</a>]</p>
<p>
Hartmanis’ seminal research paralleled his leadership as chair of the Cornell computer science department and his active participation in numerous advisory boards.</p>
<p>
Hartmanis believes that computational complexity—the study of the quantitative laws that govern computation—is an essential part of the science needed to guide and exploit computer technology.</p>
<p align=""right"">
<span class=""callout"">Author: Allan Borodin</span></p>
<div>
<br clear=""all"">
<hr align=""left"" size=""1"" width=""33%"">
<div id=""ftn1"">
<p>
[<a href=""#_ftnref1"" name=""_ftn1"" title="""">1</a>] H. Yamada, “Real-time computation and recursive functions not real-time computable,” IRE Transactions, EC-ll (1962), pp. 753-760.</p>
</div>
<div id=""ftn2"">
<p>
[<a href=""#_ftnref2"" name=""_ftn2"" title="""">2</a>] From an IEEE transcript of an oral history interview of Juris Hartmanis conducted in 1991 by Andrew Goldstine, IEEE History Center, New Brunswick, NJ, USA, available from</p>
<p>
<a href=""http://www.ieeeghn.org/wiki/index.php/Oral-History:Juris_Hartmanis"" target=""_blank"">http://www.ieeeghn.org/wiki/index.php/Oral-History:Juris_Hartmanis</a></p>
</div>
</div>
<p>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/hartmanis_1059260.cfm""><img src=""/images/lg_aw/1059260.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Juris Hartmanis""></a>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>July 5, 1928 in Riga, Latvia</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>Cand. Phil., University of Marburg (1949, Physics); M.A., University of Kansas City (1951, Mathematics); Ph.D., California Institute. of Technology (1955, Mathematics)</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Instructor, Cornell University (1955-1957); Assistant Professor, Ohio State University (1957-1958); Research Mathematician, General Electric Research Laboratory (1958-1965); Professor, Computer Science Department, Cornell University (1965-); Chairman, Computer Science Department, Cornell University (1965-1971, again 1977-1982, and 1992-1993); Walter R. Read Professor of Engineering, Cornell University (from 1980)</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>Association for Computing Machinery Turing Award (with R.E. Stearns), 1993; Member: National Academy of Engineering, 1989; Member: Latvian Academy of Sciences, 1990 (foreign member); Member: New York State Academy of Sciences, 1982; Fellow: Association for Computing Machinery, 1994; Fellow: American Academy of Arts &amp; Sciences, 1992; Fellow: American Association for the Advancement of Science, 1981; Humboldt Foundation Senior US Scientist Award, 1993-94; B. Blozano Gold Medal of the Academy of Sciences, Czech Republic, 1995; CRA Distinguished Service Award, 2000; Grand Medal, Latvian Academy of Science, 2001; he also has received two honorary doctorates: University of Dortmund, Germany, 1995, and University of Missouri, Kansas City, 1999.&nbsp;<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100525769","Juris Hartmanis","<li class=""bibliography""><a href=""/bib/hartmanis_1059260.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283949&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/hartmanis_1059260.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/hartmanis_1059260.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/hartmanis_1059260.cfm""><span></span>Video Interview</a></li>"
"1573178902-648","https://amturing.acm.org/award_winners/micali_9954407.cfm","Along with Shafi Goldwasser, for transformative work that laid the complexity-theoretic foundations for the science of cryptography, and in the process pioneered new methods for efficient verification of mathematical proofs in complexity theory.","<p>
Silvio Micali was born in Palermo, Sicily, in 1954. He received his undergraduate education in Rome, graduating with a degree in mathematics from Sapienza University in 1978 as one of the brightest students of Professor Corrado Böhm. He earned his PhD under <a href=""/bib/blum_4659082.cfm"">Manuel Blum</a>&nbsp;at the University of California, Berkeley in 1982. After a postdoctoral position in Toronto (1982-1983), he joined the faculty at MIT in July 1983, where he has been since.</p>
<p>
<span style=""line-height: 1.3;"">Silvio Micali is a visionary whose work has contributed to the mathematical foundations of cryptography and has advanced the theory of computation. His non-conventional thinking has fundamentally changed our understanding of basic notions such as </span><em style=""line-height: 1.3;"">randomness, secrets, proof, knowledge, collusion, </em><span style=""line-height: 1.3;"">and</span><em style=""line-height: 1.3;""> privacy, </em><span style=""line-height: 1.3;"">which have been contemplated and debated for millennia. This foundational work was a key component in the development of the computer security industry, facilitated by his patents and start-up companies. His work has also had great impact on other research areas in computer science and mathematics.</span></p>
<p>
Silvio’s<span style=""line-height: 1.3;""> educational and teaching talents are no less legendary. His lectures are consistently entertaining and illuminating. They use numerous cartoons and remarkable stories of unlikely heroes, villains and impossible tasks, designed to highlight the new ideas and concepts. His strong Sicilian accent is the perfect spice for these treats. </span>Silvio<span style=""line-height: 1.3;""> has mentored and advised many PhD students in that spirit.</span></p>
<p>
<strong>Cryptography</strong></p>
<p>
Micali’s<span style=""line-height: 1.3;""> work with </span><a href=""/bib/goldwasser_8627889.cfm"" style=""line-height: 1.3;"">Goldwasser</a><span style=""line-height: 1.3;""> (his co-winner of the Turing award and long-time collaborator) helped make cryptography a precise science. The mathematical structures they created, including formal notions of privacy, adversaries, </span><a href=""https://en.wikipedia.org/wiki/Pseudorandomness"" style=""line-height: 1.3;"">pseudorandomness</a><span style=""line-height: 1.3;"">, </span><a href=""https://en.wikipedia.org/wiki/Interactive_proof_system"" style=""line-height: 1.3;"">interactive proofs</a><span style=""line-height: 1.3;"">, </span><a href=""https://en.wikipedia.org/wiki/Zero-knowledge_proof"" style=""line-height: 1.3;"">zero-knowledge proof</a><span style=""line-height: 1.3;"">, and numerous other basic notions that are often extremely subtle to formulate, set cryptography on rigorous foundations of the highest standards, and opened up whole new areas of research within computer science.</span></p>
<p>
<span style=""line-height: 1.3;"">Their revolutionary first paper, written when they were graduate students, is “Probabilistic Encryption,” [</span><a href=""/bib/micali_0557920.cfm#bib_1"" style=""line-height: 1.3;"">1</a><span style=""line-height: 1.3;"">], one of the most influential papers in the history of computer science. It set the foundations on which thousands of researchers base their work.</span></p>
<p>
<span style=""line-height: 1.3;"">The first question asked and answered in this paper is “What is a secret?” This very basic question had never been formally addressed, despite centuries of research in cryptography and an ancient natural human interest in the notion. They set very high standards: an adversary should not be able to gain even partial information about a secret. In this paper they define </span><a href=""https://en.wikipedia.org/wiki/Probabilistic_encryption"" style=""line-height: 1.3;"">probabilistic encryption</a><span style=""line-height: 1.3;"">, </span><a href=""https://en.wikipedia.org/wiki/Semantic_security"" style=""line-height: 1.3;"">semantic security</a><span style=""line-height: 1.3;"">, and also </span><a href=""https://en.wikipedia.org/wiki/Computational_indistinguishability"" style=""line-height: 1.3;"">computational indistinguishability</a><span style=""line-height: 1.3;"">, the notion that objects which look the same to efficient algorithms </span><em style=""line-height: 1.3;"">are</em><span style=""line-height: 1.3;""> the same. Using these concepts they are able to make formal sense of </span>Diffie<span style=""line-height: 1.3;""> &amp; </span>Hellman’s<span style=""line-height: 1.3;""> [</span><a href=""/bib/micali_0557920.cfm#bib_10"" style=""line-height: 1.3;"">10</a><span style=""line-height: 1.3;"">] ideas of computational cryptography. They combine all these to give a public-key encryption scheme which is secure by their standard: they </span><em style=""line-height: 1.3;"">prove</em><span style=""line-height: 1.3;""> that any leak will result in an efficient algorithm for factoring integers. Such formal definitions and proofs are missing from previous seminal papers like </span>Diffie<span style=""line-height: 1.3;""> &amp; </span>Hellman<span style=""line-height: 1.3;""> and RSA [</span><a href=""/bib/micali_0557920.cfm#bib_11"" style=""line-height: 1.3;"">11</a><span style=""line-height: 1.3;"">] of </span><a href=""/bib/rivest_1403005.cfm"" style=""line-height: 1.3;"">R. L. Rivest</a><span style=""line-height: 1.3;"">, </span><a href=""/bib/shamir_0028491.cfm"" style=""line-height: 1.3;"">A. Shamir</a><span style=""line-height: 1.3;""> and </span><a href=""/bib/adleman_7308544.cfm"" style=""line-height: 1.3;"">L. Adleman</a><span style=""line-height: 1.3;"">. </span>Micali<span style=""line-height: 1.3;""> and </span>Goldwasser’s<span style=""line-height: 1.3;""> first paper paved the way for them and numerous others to advance the rich and important field of cryptography, which was critical to the development of commercial applications of the Internet.</span></p>
<p>
<strong>Randomness</strong></p>
<p>
<span style=""line-height: 1.3;"">The issues and techniques raised by cryptographic research have led to exciting developments in other areas in the theory of computation, such as the study of “computational randomness”, or </span>“pseudorandomness”<span style=""line-height: 1.3;"">. This field has matured in the past 20 years and produced such fundamental results as the ability to convert hard functions into </span>pseudorandom<span style=""line-height: 1.3;""> objects, and the ability to extract randomness from defective random sources. </span>Micali’s<span style=""line-height: 1.3;""> paper with his advisor </span><a href=""/bib/blum_4659082.cfm"" style=""line-height: 1.3;"">Manuel Blum</a><span style=""line-height: 1.3;"">, [</span><a href=""/bib/micali_0557920.cfm#bib_2"" style=""line-height: 1.3;"">2</a><span style=""line-height: 1.3;"">] constructs a “pseudo-random generator”, which deterministically and efficiently stretches a short random seed into a long sequence of bits, and then proves this sequence is completely unpredictable by any efficient algorithm, assuming the hardness of the </span><a href=""https://en.wikipedia.org/wiki/Discrete_logarithm"" style=""line-height: 1.3;"">discrete logarithm function</a><span style=""line-height: 1.3;"">. This idea is elaborated in his PhD thesis, the title of which, </span><em style=""line-height: 1.3;"">Hardness vs. Randomness</em><span style=""line-height: 1.3;"">, has come to denote a central paradigm used today to allow the removal of randomness from probabilistic algorithms. Another magical construct of </span>Micali<span style=""line-height: 1.3;""> (in a paper with </span>Goldreich<span style=""line-height: 1.3;""> and </span>Goldwasser<span style=""line-height: 1.3;""> [</span><a href=""/bib/micali_0557920.cfm#bib_3"" style=""line-height: 1.3;"">3</a><span style=""line-height: 1.3;"">]) is a “</span><a href=""https://en.wikipedia.org/wiki/Pseudorandom_function_family"" style=""line-height: 1.3;"">pseudo-random function</a><span style=""line-height: 1.3;"">” whose output is indistinguishable from that of a truly random one. These have proven fundamental not only in cryptography, but in other fields. For example (as shown by Les </span>Valient<span style=""line-height: 1.3;""> and Mike Kearns), they yield limitations in computational learning theory, and partly explain our inability to prove computational lower bounds (such as </span><a href=""https://en.wikipedia.org/wiki/P_versus_NP_problem"" style=""line-height: 1.3;"">P ¹ NP</a><span style=""line-height: 1.3;"">).</span></p>
<p>
<strong>Interactive Proofs</strong></p>
<p>
<span style=""line-height: 1.3;"">Yet another field which emerged from cryptographic concerns (mainly from </span>Micali’s<span style=""line-height: 1.3;""> joint paper with </span>Goldwasser<span style=""line-height: 1.3;""> and </span>Rackoff<span style=""line-height: 1.3;""> [</span><a href=""/bib/micali_0557920.cfm#bib_4"" style=""line-height: 1.3;"">4</a><span style=""line-height: 1.3;"">]) is the field of “interactive proofs.” Such proofs generalize the ancient notion of mathematical proof by allowing interaction, randomization and error, thus vastly enriching meaning and utility. In 20 years this concept has given rise to fundamental discoveries in computational complexity, such as </span><a href=""https://en.wikipedia.org/wiki/IP_%28complexity%29"" style=""line-height: 1.3;"">IP=PSPACE</a><span style=""line-height: 1.3;""> and the </span><a href=""https://en.wikipedia.org/wiki/PCP_theorem"" style=""line-height: 1.3;"">PCP Theorem</a><span style=""line-height: 1.3;"">, and has had a fundamental impact on understanding the hardness of approximation. The same paper defines the paradoxical notion of “</span><a href=""https://en.wikipedia.org/wiki/Zero-knowledge_proof"" style=""line-height: 1.3;"">zero-knowledge</a><span style=""line-height: 1.3;"">” proofs that are convincing but reveal nothing beyond their validity. In two follow-up papers (with </span>Goldreich<span style=""line-height: 1.3;""> and </span>Wigderson<span style=""line-height: 1.3;""> [</span><a href=""/bib/micali_0557920.cfm#bib_5"" style=""line-height: 1.3;"">5</a><span style=""line-height: 1.3;"">], [</span><a href=""/bib/micali_0557920.cfm#bib_6"" style=""line-height: 1.3;"">6</a><span style=""line-height: 1.3;"">]) </span>Micali<span style=""line-height: 1.3;""> showed that this notion is universal; assuming </span><a href=""https://en.wikipedia.org/wiki/One-way_function"" style=""line-height: 1.3;"">one-way functions</a><span style=""line-height: 1.3;"">, </span><em style=""line-height: 1.3;"">every</em><span style=""line-height: 1.3;""> theorem has such a “zero-knowledge” proof. This resolves problems such as those involving basic copyright issues. It also enables a remarkably general design tool for cryptographers: a compiler which transforms any cryptographic protocol designed for honest, cooperating parties into one which can tolerate arbitrary malicious behavior.</span></p>
<p>
Silvio<span style=""line-height: 1.3;""> and </span>Shafi’s<span style=""line-height: 1.3;""> originality and foresight are inspirational, and their work holds computational proofs to the most stringent mathematical standards. They have trained a generation of students and colleagues to be equally bold and creative.</span></p>
<p>
<strong>Mechanism Design</strong>.</p>
<p>
<span style=""line-height: 1.3;"">In the past few years </span>Micali<span style=""line-height: 1.3;""> turned his attention to </span><a href=""https://en.wikipedia.org/wiki/Game_theory"" style=""line-height: 1.3;"">Game Theory</a><span style=""line-height: 1.3;"">. In particular, he has been working on developing a more robust form of </span><a href=""https://en.wikipedia.org/wiki/Mechanism_design"" style=""line-height: 1.3;"">mechanism design</a><span style=""line-height: 1.3;"">, in which collusion and privacy are explicitly taken into account. Collusion and privacy have always played a central role in our human endeavors, but until this work they had not been central to game theoretic analysis.</span></p>
<p>
<strong>Practical impact </strong></p>
<p>
<span style=""line-height: 1.3;"">Complementing his academic work, </span>Micali<span style=""line-height: 1.3;""> has many patents on practical implementations of his inventions for encryption, digital signatures, electronic cash, certified transactions, </span><a href=""https://en.wikipedia.org/wiki/Key_escrow"" style=""line-height: 1.3;"">key-escrow</a><span style=""line-height: 1.3;""> and more. It is highly unusual for a theoretician to be awarded over 50 patents. His practical bent goes further, with the creation of two start-up companies: </span>Peppercoin<span style=""line-height: 1.3;""> (for micro-payments), which was acquired by </span>Chockstone<span style=""line-height: 1.3;""> in 2007, and </span>CoreStreet<span style=""line-height: 1.3;""> (for real-time credentials) which was acquired by </span>ActiveIDentity<span style=""line-height: 1.3;""> in 2009.</span></p>
<p>
<strong>Cooking</strong></p>
<p>
<span style=""line-height: 1.3;"">Well, if you never had </span>Silvio<span style=""line-height: 1.3;""> cook one of his mother’s recipes for you, it is very hard to explain what you are missing.</span></p>
<p>
<strong>Summary</strong></p>
<p>
Micali is an intellectual giant of a rare variety. His leadership has steered the academic agenda of our field in the key areas. Time and time again his ideas challenged conventional wisdom with originality, vision and technique, to create what would become tomorrow's better conventional wisdom.</p>
<p style=""text-align: right;"">
<span class=""callout"">Author: Avi Wigderson</span></p>
<p>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/micali_9954407.cfm""><img src=""/images/lg_aw/9954407.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Silvio Micali""></a>
<br><br>
<h6 class=""label""><a href=""/photo/micali_9954407.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<div class=""savvyobjectnormal"" id=""highlight906"">
<h6 class=""label"">
<strong>BIRTH:</strong></h6>
<p>
October 13, 1954, Palermo, Italy</p>
<h6 class=""label"">
<strong style=""color: rgb(230, 126, 41); line-height: normal;"">EDUCATION:</strong></h6>
<p>
<span style=""line-height: 1.3;"">Undergraduate degree in mathematics from </span>Sapienza<span style=""line-height: 1.3;""> University, Rome (1978); PhD in mathematics from the University of California, Berkeley (1982).</span></p>
<h6 class=""label"">
<strong style=""line-height: 1.3;"">EXPERIENCE:</strong><span style=""line-height: 1.3;""> </span></h6>
<p>
<span style=""line-height: 1.3;"">University of Toronto Post-doctoral Fellow (1982-1983); MIT: Assistant Professor (1983-1986), Associate Professor (1986-1988), Tenured Associate Professor (1988-1991), Full Professor (1991- now)</span></p>
<h6 class=""label"">
<strong style=""line-height: 1.3;"">HONORS AND AWARDS:</strong><span style=""line-height: 1.3;""> </span></h6>
<p>
Gödel<span style=""line-height: 1.3;""> Prize in Theoretical Computer Science (1993, with others); Member, American Academy of Arts and Sciences (2003); RSA Prize in Cryptography (2004); </span>Rademacher<span style=""line-height: 1.3;""> Lecturer, University of Pennsylvania (2005); Member, National Academy of Sciences (2007); Fellow of the International Association for </span>Cryptologic<span style=""line-height: 1.3;""> Research (</span><a href=""https://en.wikipedia.org/wiki/International_Association_for_Cryptologic_Research"" style=""line-height: 1.3;"" title=""International Association for Cryptologic Research"">IACR</a><span style=""line-height: 1.3;"">) (2007); Member, National Academy of Engineering (2007); Chair Professor, </span>Tsinghua<span style=""line-height: 1.3;""> University (2007); Distinguished Alumnus Award, Computer Science and Engineering, </span>UC<span style=""line-height: 1.3;""> Berkeley.</span></p>
</div>
<p>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100228636","Silvio Micali","<li class=""bibliography""><a href=""/bib/micali_9954407.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/micali_9954407.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/micali_9954407.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/micali_9954407.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/micali_9954407.cfm""><span></span>Video Interview</a></li>"
"1573179373-680","https://amturing.acm.org/award_winners/iverson_9147499.cfm","For his pioneering effort in programming languages and mathematical
notation resulting in what the computing field now knows as APL, for his contributions to
the implementation of interactive systems, to educational uses of APL, and to programming
language theory and practice.","<p><strong>Kenneth Eugene Iverson was born on December 17, 1920 on a farm near Camrose, Alberta, Canada, a small city about 100 km. southeast of the provincial capital of Edmonton.</strong> His parents, of Norwegian descent, had come to Alberta from North Dakota when they were children. He was educated in rural one-room schools until the end of 9th grade. He then stayed home to work on the farm because, he said, the only purpose of continuing his schooling would be to become a schoolteacher, and that was a profession he decidedly did not want. During the long winter months he studied calculus on his own. Years later he still recalled the joy of discovering how the “beautiful circular functions were finally united in a single family under the exponential.”</p>
<p>In 1942 Ken was drafted into the army, and the following year he transferred to the Royal Canadian Air Force and served as a flight engineer specializing in reconnaissance. During his military service he took eight correspondence courses offered by the Canadian Legion, nearly enough to complete high school. On his discharge from the services in 1946, with encouragement from both his counselors and fellow servicemen, he began his formal university education at Queen’s University in Kingston, Ontario.</p>
<p>Ken earned a B.A. in mathematics and physics from Queen’s in 1950, and an M.A. in mathematics from Harvard University in 1951. He switched to the Department of Engineering and Applied Physics as a result of taking a course from computer pioneer Professor Howard Aiken. His Ph.D. thesis was supervised by Professor Aiken and the Nobel prize-winning economist Wassily Leontief. Iverson extended Leontief’s input-output model to handle capital goods. While doing the research, he realized the need to experiment with matrix methods using a computer, and he wrote a software package of matrix routines for the <a href=""https://en.wikipedia.org/wiki/Harvard_Mark_I"" target=""_blank"">Harvard Mark I</a>.</p>
<p>After completing his doctorate, Iverson joined the Harvard faculty to teach in Aiken’s new automatic data processing program, for one year as an Instructor, and for five years as an Assistant Professor. He became increasingly frustrated with the inadequacy of conventional mathematical notation for expressing algorithms. He began to invent his own notation, based on an extension of the higher dimensional arrays of tensor algebra and the concept of operators previously developed by the English physicist Oliver Heaviside (1850-1925).</p>
<p>In 1960 Ken joined the new Research Center of IBM in Yorktown Heights, New York, on the advice of <a href=""/award_winners/brooks_1002187.cfm"">Frederick Brooks</a>, who had been one of his teaching fellows at Harvard and was now at IBM. The two collaborated on the continuing development of the new notation. In 1962 Ken published the now-classic book <em>A Programming Language </em>[<a href=""/bib/iverson_9147499.cfm#link_1"">1</a>], the title of which gave the&nbsp;name APL to the notation which had up until then been informally called “Iverson’s notation”. In 1963 he and Fred Brooks published <em>Automatic Data Processing </em>[<a href=""/bib/iverson_9147499.cfm#link_3"">3</a>] based on their lecture notes prepared for Aiken’s course at Harvard.</p>
<p>APL began as a notation for describing algorithms, and only later become a programming language. The first implementation of APL was written in 1965 in Fortran for an IBM 7090 computer, and was used in batch mode. In 1966 an interactive APL/360 system running on an IBM/360 Model 50 provided a regular service to users in the Yorktown Heights research center. APL became publicly available in 1968.</p>
<p>Ken continued to work on the development of APL throughout his tenure at IBM. He became an IBM Fellow in 1970. In 1980 he left IBM, and returned to Canada to work for I.P. Sharp Associates, which had established an APL-based time-sharing service widely used in Canada, the United States and Europe.</p>
<p>In 1987 he “retired from paid employment” and turned his full attention to the development of a more modern dialect of APL. APL was successfully used for commercial purposes, but Iverson wanted to develop a new simple executable notation more suitable for teaching, which would be available at low cost. Programs could be printed on standard printers, without the special characters that APL used. They could run on a wide variety of computing platforms, yet the language would have the simplicity and generality of APL. The first implementation of this language, called J, was announced at the APL90 Users’ Conference. (For examples of APL and J programs, see <a href=""/info/iverson_9147499.cfm"">here</a>.)</p>
<p>With collaborators, including his son Eric, Ken continued to work on the development of J, and he published many papers, monographs and books. On Saturday, October 16, 2004 Ken suffered a stroke while working on a J tutorial, and died three days later on October 19, at the age of 83.</p>
<p>Ken Iverson strongly believed in the power of appropriate notation as an aid to thought, especially when that notation can be interpreted by a computer. In the introduction to his Turing lecture he said, “The advantages of executability and universality found in programming languages can be effectively combined, in a single coherent language, with the advantages offered by mathematical notation.” He quoted from Alfred North Whitehead’s <em>An Introduction to Mathematics</em> first published in 1911:</p>
<p style=""margin-left: 40px;""><br>
“By&nbsp;relieving the brain of all unnecessary work, a good notation sets it free to concentrate on more advanced problems, and in effect increases the mental power of the&nbsp;race.”</p>
<p style=""text-align: right;""><span class=""callout"">Author: Keith Smillie</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/iverson_9147499.cfm""><img src=""/images/lg_aw/9147499.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Kenneth E. Iverson ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/iverson_9147499.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>December 17, 1920, Camrose, Alberta, Canada</p>
<h6 class=""label"">DEATH:</h6>
<p>October 19, 2004, Toronto, Ontario, Canada</p>
<h6 class=""label"">EDUCATION:</h6>
<p>B.A., mathematics and physics (Queen’s University, Kingston, 1950); M.A., mathematics (Harvard University, 1951); Ph.D., applied mathematics (Harvard University, 1954).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Instructor (applied mathematics), Harvard University (1954 – 1955); Assistant Professor, (applied mathematics), Harvard, University (1955 – 1960); International Business Machines (1960 – 1980); I. P. Sharp Associates (1980 – 1987).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>IBM Fellow (1971); Harry Goode Memorial Award (1975); ACM Turing Award (1979); IEEE Computer Society Computer Pioneer Award (1991); National Medal of Technology (1991); Honorary D.Sc., York University (1998)</p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100366906","Kenneth E. (""Ken"") Iverson","<li class=""bibliography""><a href=""/bib/iverson_9147499.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283935&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/iverson_9147499.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/iverson_9147499.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179599-694","https://amturing.acm.org/award_winners/hoare_4622167.cfm","For his fundamental contributions to the definition and design
of programming languages.","<p><strong>“Tony” Hoare, as he is universally known, was named Charles Antony Richard Hoare when born on 11 January 1934 in the city of Colombo in what was Ceylon but is now Sri Lanka.</strong> Tony's parents were involved in the business of what was then the British Empire. Tony received secondary education at the Dragon School in Oxford and King's School in Canterbury. His university education was also in Oxford. His degree course, known as “Greats,"" involved the study of Latin and Greek as well as philosophy. He chose to study modern philosophy, which provided a path to understand logic.</p>
<p>At the time of his graduation in 1956 Tony was called up into the Royal Navy. On completion of the obligatory two years of military service, he returned to Oxford to study Statistics and began computer programming in <a href=""https://en.wikipedia.org/wiki/Autocode"" target=""_blank"">Mercury Autocode</a>. He then went as a graduate student to Moscow State University to study <a href=""https://en.wikipedia.org/wiki/Machine_translation"" target=""_blank"">Machine Translation</a> with Andrey Kolmogorov. Tony states that it was during this time that he saw the problem of sorting (for dictionaries) as important, and here first thought of the novel sorting algorithm that was to become known as <em>Quicksort</em>.</p>
<p>Returning to the UK, Tony joined a small British computer company called Elliott Brothers. One of his major projects was to lead the team that produced the <a href=""/info/hoare_4622167.cfm"">ALGOL 60&nbsp;</a>compiler for the Elliott 503 computer. This period of practical industrial projects appears to have provided the inspiration for two major themes that were to occupy Tony's academic career. During a course on the programming language ALGOL 60 (given by <a href=""/award_winners/dijkstra_1053701.cfm"">Edsger Dijkstra</a>, Peter Landin and&nbsp;<a href=""/award_winners/naur_1024454.cfm"">Peter Naur</a>) Tony saw that the concept of recursion was the key to lucid expression of <a href=""/info/hoare_4622167.cfm"">Quicksort</a>. This contribution alone would have established Tony's reputation, but far more significant was the appreciation of the crucial role of programming languages that he took from this experience.</p>
<p>What at first looks like a failure was another inspiration from Tony's time at Elliott. He candidly reports that it was the inability to deliver an operating system for the Mark II Elliott 503 that caused him to devote much of his later research to understanding and taming concurrency in program execution.</p>
<p>A happy outcome of this period of his life was marriage to the fellow team member Jill Pym, who became his lifetime companion.</p>
<p>The Turing citation emphasizes Tony's contribution to the design of programming languages. When one thinks of the enormous cost to society of constructing programs, one cannot escape the view that the design of the medium in which programs are expressed is of major importance. Unfortunately software is all too often of poor quality, and some of the blame must rest on the languages in which the software is written. Many security holes that let in “malware"" such as viruses could be avoided by using better languages. Furthermore, the annual cost of software errors is estimated to run to tens of billions of dollars. Hoare has repeatedly issued warnings about software design, including the clear message that simplicity and elegance are essential if software is to remain within intellectual control. This message is clearly delivered in his Turing Lecture [<a href=""/bib/hoare_4622167.cfm#link_3"">3</a>].</p>
<p>So many of Tony's contributions relate to programming language design that it is difficult to choose a single paper for the reader to peruse. Perhaps “Hints on programming language design” [<a href=""/bib/hoare_4622167.cfm#link_2"">2</a>]&nbsp;is a good place to start. Its 1973 publication date is an indication of how long he has been offering good advice—which sadly has all too rarely been heeded.</p>
<p>The design of programming languages is a fascinating task of balancing a desire for mathematical tractability with the need to recognize the engineering concern of efficient compilation. But there is another issue: if there are to be multiple implementations, (or even an effective sharing of understanding between users and compiler writers) it is essential that the meaning of all programs be well-documented. Taking a term from linguists who studied natural languages, computer scientists refer to this task as recording the “semantics” of a programming language.</p>
<p>When Hoare moved from industry to academia in 1968, his first post was as a Full Professor at the Queen's University, Belfast. For some years, he had been dissatisfied with the accepted way of documenting the semantics of programming languages and had sought a style which permits recording those places where a language designer deliberately leaves some features under-determined. He has described how, on unpacking his papers in Belfast, he became excited by <a href=""/award_winners/floyd_3720707.cfm"">Bob Floyd's</a>&nbsp;1967 paper “Assigning meaning to programs"". Floyd had described a way of adding assertions to flowcharts of programs that made it possible to prove that a program satisfies its specification.</p>
<p>Hoare made two bold additional steps, and his “An axiomatic basis for computer programming” [<a href=""/bib/hoare_4622167.cfm#link_1"">1</a>] is one of the most influential papers on the theory of programming. First he discarded the flowcharts and developed a logical system for reasoning about programs using specifications of statement behavior that have become known as <a href=""https://en.wikipedia.org/wiki/Hoare logic"" target=""_blank"">Hoare triples</a>. Secondly, he argued that his “axiomatic"" system could be viewed as an abstract way of recording the semantics of programming languages.</p>
<p>The first of these steps has the profound effect of opening up a way of developing provable programs rather than treating their verification as a post hoc concern. Hoare published a number of developments of this idea, and the pursuit of “Hoare semantics"" has had a profound effect on the understanding of programming languages and the task of reasoning about programs. There is a fascinating <a href=""/info/hoare_4622167.cfm"">link</a> back to work by Alan Turing in the late 1940s.</p>
<p>Tony Hoare became a professor at Oxford University in 1977 and there returned to his interests in concurrency. Programming languages started by making at first minor and then more significant abstractions from the instruction set of hardware. Each step raises the engineering concern of whether it is possible to translate programs efficiently into machine code. The progress of increasing abstraction has been difficult, but nowhere more so than in the area of concurrency and here Hoare has made further seminal contributions.</p>
<p>It would offer a huge simplification if one could avoid computer programs interacting with one another, but doing so could reduce potential efficiency of some forms of programs by orders of magnitude. More importantly, once programs interact with the physical world they need to react to events that occur at unpredictable points in time. The early ways of synchronizing concurrency in programs was to permit sharing of variables between parts of a program. But unless careful limitations are put on such sharing, it becomes almost impossible to think about all eventualities. Errors in such programs are as elusive as they are damaging. Hoare himself made some of the most important proposals to constrain such interference. He saw, however, that such shared variable concurrency was fatally flawed.</p>
<p>At Oxford, he made another bold intellectual step: in a 1978 paper in the <em>Communications of the ACM</em> (and subsequently in his book <em>Communicating Sequential Processes</em> [<a href=""/bib/hoare_4622167.cfm#link_4"">4</a>]) he proposed a language (<a href=""https://en.wikipedia.org/wiki/Communicating_sequential_processes"" target=""_blank"">CSP</a>) where the interaction between programs&nbsp;was limited to pre-planned communications. CSP has inspired a wealth of further research, including tools which have encouraged its use in industrial applications. Initially CSP was only a software tool, but it also gave rise to the <a href=""https://en.wikipedia.org/wiki/Transputer"" target=""_blank"">Transputer</a> machine architecture.</p>
<p>Tony retired from Oxford University in 1999, but not from active research. He joined Microsoft's Research Laboratory in Cambridge (UK) and actively pursued several research ideas. His “Unifying Theories"" book [<a href=""/bib/hoare_4622167.cfm#link_6"">6</a>]&nbsp;has attracted a similar following among researchers as his earlier contributions. With his proposal to pursue research “Grand Challenges"" [<a href=""/bib/hoare_4622167.cfm#link_7"">7</a>], he jointly leads the international initiative on “Verified Software"" [<a href=""/bib/hoare_4622167.cfm#link_8"">8</a>].</p>
<p>Tony has been an inspiration to many researchers, and his “family tree"" of PhD students can be seen in the book <em>Reflections on the work of C.A.R. Hoare </em>[<a href=""/bib/hoare_4622167.cfm#link_9"">9</a>], written in honor of Sir Tony's 75th birthday. It provides an in-depth discussion the influence of his work on current research.</p>
<p>Apart from the Turing Prize, Tony Hoare was awarded the Kyoto Prize in 2000, the year he was also Knighted by the Queen for services to education and computer science. He has honorary Doctorates from several universities and is a Fellow of both The Royal Society and The Royal Academy of Engineering.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Cliff Jones</span><br>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/hoare_4622167.cfm""><img src=""/images/lg_aw/4622167.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""C. Antony R. Hoare ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/hoare_4622167.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>in Sri Lanka in 1934</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Dragon School in Oxford and King's School in Canterbury. His university undergraduate education was also in Oxford and he did post graduate there as well as Moscow State University.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Royal Navy (1956-57), Elliott Brothers (1960-1968), Queens’s University, Belfast (1968 - 1977); Oxford University (1977); currently Emeritus Professor at Oxford University and a Senior Researcher at Microsoft Research in Cambridge, UK.</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Turing Prize (1980); Harry H. Goode Memorial Award (1981);&nbsp;<span style=""line-height: 20.8px;"">Fellow of The Royal Society (1982);&nbsp;</span><span style=""line-height: 1.3;"">Kyoto Prize (2000); Knighted by the Queen for services to education and computer science (2000); honorary Doctorates from several universities; Fellow of The Royal Academy of Engineering (2005); Computer History Museum Fellow (2006); Programming Languages Achievement Award (2011); IEEE John von Neumann Medal (2011).</span></p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81385591905","C. Antony (""Tony"") R. Hoare","<li class=""bibliography""><a href=""/bib/hoare_4622167.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283936&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/hoare_4622167.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/hoare_4622167.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/hoare_4622167.cfm""><span></span>Video Interview</a></li>"
"1573179437-684","https://amturing.acm.org/award_winners/thompson_4588371.cfm","With Dennis M. Ritchie, for their development of generic operating systems theory and specifically for the implementation of the UNIX operating system.","<p><strong>Kenneth Lane Thompson was born February 4, 1943 in New Orleans, Louisiana.</strong> His father was in the US Navy and the family moved often.</p>
<p>Thompson received a Bachelor of Science in 1965 and a master's degree in 1966 in Electrical Engineering and Computer Science from the University of California, Berkeley.</p>
<p>After graduation, Thompson and <a href=""/award_winners/ritchie_1506389.cfm"">Dennis Ritchie</a> joined the Bell Laboratories Computing Sciences Research Center in Murray Hill NJ. At the time, staff members of this group had considerable latitude in choosing research topics in computing theory, languages, programming and systems. Since 1964, members of the <a href=""/info/thompson_4588371.cfm#add_2"">group</a> had been participating in the design and development of the Multics timesharing system, along with developers from <a href=""/info/thompson_4588371.cfm#add_1"">MIT's Project MAC</a> and General Electric.</p>
<p>In 1969, Bell Labs withdrew from the Multics project. The Computing Sciences Research group members searched for other projects, and in particular for a computing environment with an on-line community that avoided the ""big system mentality.""&nbsp; Unix would provide such an environment.</p>
<p>Thompson wrote the first version of the Unix operating system for a <a href=""/info/thompson_4588371.cfm#add_5"">Digital Equipment Corporation PDP-7</a>&nbsp; in a month, using a cross-assembler that ran on <a href=""/info/thompson_4588371.cfm#add_4"">GECOS</a>. The PDP-7 he used had only 4K of 18-bit words. Dennis Ritchie wrote,</p>
<p style=""margin-left: 40px;""><span class=""callout"">It began in 1969 when Ken Thompson discovered a little-used PDP-7 computer and set out to fashion a computing environment that he liked. His work soon attracted me; I joined in the enterprise, though most of the ideas, and most of the work for that matter, were his.</span></p>
<p>Unix provided users with interactive remote terminal computing and a shared file system. Source code was provided with the system, and the community of users could share ideas and programs directly and informally. Because Unix ran on a relatively inexpensive minicomputer, small research groups could experiment with it without dealing with computation center bureaucracies.</p>
<p>In 1971, the Bell Laboratories Computing Sciences Research group ported Unix to a <a href=""/info/thompson_4588371.cfm#add_5"">Digital Equipment Corporation PDP-11</a> to support text processing for the Bell Laboratories Patents Office. By 1972, there were 10 installations of Unix at AT&amp;T.</p>
<p>Thompson also created an interpretive language, called B, based on BCPL, which he used to re-implement the non-kernel parts of Unix. Ritchie added types to the B language, and later created a compiler for the C language. Thompson and Ritchie rewrote most of Unix in C in 1973, which made further development and porting to other platforms much easier.</p>
<p>The second <em>ACM Symposium on Operating Systems Principles</em> was held in Elmsford, NY in 1973, and Thompson and Ritchie presented a clear and well-written paper [<a href=""/bib/thompson_4588371.cfm#link_3"">3</a>] describing Unix. The Unix system presented in the paper was elegant and simple, providing a useful and extensible multi-user programming environment on an affordable machine. The file system and libraries included with the system made it easy to build and share application programs and to augment the system's functions. By the end of 1973, there were over 20 Unix systems running.</p>
<p>Thompson and Ritchie continued the development of Unix and C at Bell Laboratories, along with other Computing Sciences Research group members. Unix use spread further within AT&amp;T. The Sixth Edition, released in 1975, began the spread of Unix to university, commercial, and government users of the popular DEC PDP-11 computers. AT&amp;T, forbidden by court decree from selling Unix, licensed it for the cost of media. Enthusiastic users had the source code available, and fed improvements to Unix back to the Bell Labs developers. A 1977 retrospective paper by Ritchie [<a href=""/bib/thompson_4588371.cfm#link_4"">4</a>] said that there were more than 300 Unix installations running, on configurations from a single-user DEC LSI-11 to a 48-user PDP-11/70. By 1978, there were over 600 Unix installations, and Unix had begun to be ported to other minicomputers.</p>
<p>In the late 1970s, John Lions of the University of New South Wales circulated a book [<a href=""/bib/thompson_4588371.cfm#link_8"">8</a>] on Unix that included the source code and commentaries on it. This book was used to teach Unix in operating systems courses around the world, and created a generation of computer scientists familiar with Unix internals.</p>
<p>In 1983 Thompson and Ritchie received the ACM A. M. Turing Award. The Turing Award selection committee wrote:</p>
<p style=""margin-left: 40px;""><span class=""callout"">The success of the UNIX system stems from its tasteful selection of a few key ideas and their elegant implementation. The model of the Unix system has led a generation of software designers to new ways of thinking about programming. The genius of the Unix system is its framework, which enables programmers to stand on the work of others.</span></p>
<p>In the mid-1980s, several organizations promoted different technical approaches to Unix on different platforms, with different licensing arrangements. Thompson and Ritchie were honored as the originators of the system but no longer controlled its destiny. They went on to other computing research projects within AT&amp;T.</p>
<p>Thompson worked with other members of the Computing Science Research Center on the Bell Laboratories Plan 9 distributed operating system from the mid-1980s until the late 1990s. This system uses the Unicode character encoding system, which allows text in many languages to be represented. Thompson and Rob Pike developed the UTF-8 multi-byte character encoding scheme, the most widely used encoding on the World Wide Web, in 1992.</p>
<p>Thompson also studied computer chess with other AT&amp;T colleagues. He and Joe Condon built the world champion chess-playing computer <a href=""https://en.wikipedia.org/wiki/Belle_%28chess_machine%29"" target=""_blank"">Belle</a>, the first computer to attain a master rating. (Belle was later donated to the Smithsonian Institution.)</p>
<p>In late 2000, Thompson retired from Bell Labs. He worked at Entrisphere, Inc as a Fellow until moving to Google in 2006 as a Distinguished Engineer, where he worked on Google's programming language Go.</p>
<p>Also see:<br>
<a href=""http://snap.nlc.dcccd.edu/learn/drkelly/hst-hand.htm"" target=""_blank"">http://snap.nlc.dcccd.edu/learn/drkelly/hst-hand.htm</a><br>
Unix timeline</p>
<p style=""text-align: right;""><span class=""callout"">Author: Tom van Vleck</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/thompson_4588371.cfm""><img src=""/images/lg_aw/4588371.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Kenneth Lane Thompson""></a>
<br><br>
<h6 class=""label""><a href=""/photo/thompson_4588371.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>February 4, 1943 in New Orleans.</p>
<h6 class=""label"">EDUCATION:</h6>
<p>EECS Bachelor of Science (1965) and master's degree (1966), Electrical Engineering and Computer Science, University of California, Berkeley.</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Member of Technical Staff, Bell Laboratories, Murray Hill NJ (Multics project 1967-1969, Co-creator of Unix operating system, Co-creator of Belle, winner of the 3rd World Computer Chess Championship 1980 in Linz, co-creator of Plan 9 From Bell Labs operating system); Entrisphere, Fellow; Google, Distinguished Engineer (co-created Google's programming language Go).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>(the following awards were jointly given to both Thompson and Ritchie) ACM Programming Systems and Languages Paper Award (1975); ACM A. M. Turing Award (1983); ACM Software System Award (1983); IEEE Emmanuel R. Piore Award (1983). the IEEE Richard W. Hamming Medal (1990); IEEE Computer Pioneer Award (1994); Computer History Museum Fellow Award (1997); 1998 National Medal of Technology from President Bill Clinton; ACM SIGOPS Hall of Fame Award (2005). Japan Prize for Information and Communications (2011).<br>
The IEEE chose Thompson to receive the first Tsutomu Kanai Award (1999); Thompson was elected to the National Academy of Engineering in 1980.<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100436668","Kenneth Lane Thompson","<li class=""bibliography""><a href=""/bib/thompson_4588371.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283940&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/thompson_4588371.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/thompson_4588371.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179325-677","https://amturing.acm.org/award_winners/diffie_8371646.cfm","For inventing and promulgating both asymmetric public-key cryptography, including its application to digital signatures, and a practical cryptographic key-exchange method.","<h4 style=""color: rgb(102, 102, 102); font-family: Georgia, serif; font-size: 21.12px; background-color: rgb(255, 255, 255);"">&nbsp;</h4>
<p><span style=""line-height: 1.3;"">Public-key cryptography pioneer Bailey Whitfield (“Whit”) </span>Diffie<span style=""line-height: 1.3;""> was born in 1944 in Washington, D.C.&nbsp; His father, Bailey Wally </span>Diffie<span style=""line-height: 1.3;""> was a professor specializing in Iberian history at City College of New York. His mother, Justine Louise Whitfield, was a writer and scholar who passed away while he was in high school. He grew up in a Jewish immigrant neighborhood in the Queens borough of New York City, a liberal environment that helped to shape </span>Diffie’s<span style=""line-height: 1.3;""> longstanding counter-cultural ethos. During his youth, </span>Diffie<span style=""line-height: 1.3;""> read books on cryptography and had a deep interest in mathematics. Despite unremarkable grades, </span>Diffie<span style=""line-height: 1.3;""> strongly impressed those he encountered with his deep intellect and was admitted to MIT where he completed his B.S. in Mathematics in 1965. [6]</span></p>
<p>To avoid being drafted for the Vietnam War, Diffie then accepted a job offer from the MITRE Corporation, one of the (nonprofit) Federally Funded Research and Development Centers (FFRDC) that could shield employees from military service.&nbsp; Diffie worked under mathematician Roland Silver, and co-developed a symbolic mathematical manipulation software system/package that (through the work of Carl Engelman, William Martin, and Joel Moses) evolved to become Macsyma (an influential computer algebra system). &nbsp;&nbsp;Diffie was a “resident guest” researcher at MIT’s Project MAC’s Artificial Intelligence Laboratory, founded by renowned artificial intelligence scientists Marvin Minsky and John McCarthy, spending more time there than at the MITRE complex in Bedford, Massachusetts. [6]</p>
<p>John McCarthy, who had departed from MIT for Stanford in 1962, invited Diffie to join the Stanford Artificial Intelligence Laboratory, SAIL, in 1969. Now too old to be drafted, Diffie left MITRE and the MIT AI Lab for California, where he felt more culturally at home. &nbsp;&nbsp;Diffie often had discussions with McCarthy about computer networking, electronic keys, and electronic authentication—Stanford was one of the four original hubs of the ARPANET in late 1969. These experiences at Stanford and MIT (home to &nbsp;Whirlwind/SAGE, CTSS, and Multics) helped to lay the groundwork for interactive computing, and provided an ideal background for computer networking and security. [1, 6]</p>
<p>Diffie carefully read David Kahn’s <em>The Codebreakers: The Story of Secret Writing</em>, a book that had a profound influence on him and his ever deepening interest in cryptography as well as his evolving ideas on the importance of privacy. On travel back to the Northeast, Diffie reconnected with his friend Mary Fischer in New Jersey. Her marriage was faltering and she soon became Diffie’s partner and later his wife. &nbsp;She was his companion on his frequent travels in 1973 and 1974 to meet with other scientists with a deep interest in cryptography. It was in this span that Diffie became particularly interested in one-way functions.&nbsp; He visited the Thomas J. Watson Laboratory in Yorktown Heights to meet with the cryptography research team that included Horst Feistel, Alan Konheim, Alan Tritter, and others. Konheim suggested Diffie get in touch with Martin Hellman, a professor at Stanford University with similar interests who had visited the IBM research lab and cryptography group recently. [5]</p>
<p>In the fall of 1974 Diffie requested a meeting with Martin Hellman.&nbsp; What was planned for a short early afternoon meeting expanded to a rich discussion over many hours that continued through dinner at Hellman’s house and deep into the evening. Shortly thereafter Diffie began working with Hellman (taking a programming job in the research group) and in the second half of 1975 took Hellman’s suggestion to enroll as a doctoral student at Stanford to work with him.&nbsp; Diffie was enthralled in the intellectual pursuit to conceptualize what became public-key cryptography, but chose not to follow through with all of the necessary bureaucratic hurdles, classes, and requirements to completing a doctoral degree. [5]</p>
<p>In 1975 Hellman and Diffie became aware of a similarly-focused individual, Ralph Merkle.&nbsp; Merkle was a student at the University of California-Berkeley working on a protocol for public-key cryptography, who back in 1974 had formulated what became known as Merkle’s puzzles, a substantial contribution to key distribution of public-key.&nbsp; As Diffie later reflected, however, he and Hellman recognized they had “a far more compact solution to the key distribution problem than Merkle’s puzzles…”&nbsp; Further, Diffie wrote that Merkle’s subsequent “trap-door knapsack system…[did]…not lend itself readily to the production of signatures.” Nonetheless, these early contributions led Diffie to reflect on Merkle as “possibly the most inventive character in the public-key saga,” and Hellman to later argue public-key credit should be to Diffie-Hellman-Merkle. [1, 5] &nbsp;Diffie and Hellman cited Merkle’s work as a submitted paper (to <em>Communications of the ACM</em>—published in 1978) in their path-breaking “New Directions in Cryptography” paper (presented in 1975 and published the next year). In this paper Diffie and Hellman conceptualized and explained a full public-key cryptosystem with message authentication.&nbsp; Their article began, “We stand today on the brink of a revolution in cryptography,”—a revolution their mid-1970s insights were foundational to bringing to fruition in the years and decade’s ahead. [2]</p>
<p>Diffie-Hellman’s public-key is an asymmetric cryptosystem that relies on one-way functions (mathematically far easier to compute in one direction than the in reverse)—the product of very large prime numbers exceedingly difficult to factor—to allow parties to share their public-key but not their mathematically-linked private-key.&nbsp; This can facilitate secret communication between individuals who have not met and it can authenticate the message sender (digital signatures). [2] *</p>
<p>Diffie-Hellman public-key cryptosystems concepts were implemented by MIT scientists/mathematicians <a href=""https://amturing.acm.org/award_winners/rivest_1403005.cfm"">Ronald Rivest</a>, <a href=""https://amturing.acm.org/award_winners/shamir_0028491.cfm"">Adi Shamir</a>, and <a href=""https://amturing.acm.org/award_winners/adleman_7308544.cfm"">Leonard Adleman</a> with their pioneering RSA algorithm (first released in 1977). They jointly received the 2002 ACM Turing Award for the RSA algorithm and its impact on cryptography in practice.&nbsp; The RSA algorithm was the basis for the company Rivest, Shamir, and Adleman founded in 1982, RSA Data Security.&nbsp; In the mid-1980s, after some early struggles with finances and management, James Bidzos became the president and CEO of RSA Data Security, a position he thrived at and held until retiring in 1999.&nbsp; Bidzos also served as the Chair of the Board of Directors of RSA Data Security 1995 spin-off—for certifications or digital signatures—Verisign, Inc.&nbsp; [8]</p>
<p>A form of public-key cryptography had been conceptualized in the 1969 to 1970 timeframe by Great Britain’s intelligence agency GBHQ’s James Ellis.&nbsp; In 1973 GBHQ&nbsp; mathematician Clifford Cocks invented an algorithm for its implementation. And a past National Security Agency Director, without providing any details, “pointed out that two-key cryptography had been discovered at the agency roughly a decade earlier…” than Diffie-Hellman’s 1976 paper. [1] &nbsp;This highlighted the importance of researchers in the open (non-classified) community, as the work of Diffie, Hellman, Merkle, Rivest, Shamir, and Adleman, as well as businessman James Bidzos, greatly enhanced possibilities for secure communications and digital authentication; the work at GBHQ and NSA did not.</p>
<p>Back in 1977 Hellman, Diffie, and Merkle filed a patent for “public-key cryptography,” which was granted (US Patent 4200770) in April 1980.&nbsp; Stanford Ph.D. and UCLA Electrical Engineering Professor Jim Omura obtained a license to use the Diffie-Hellman-Merkle patent (held by Stanford University) for his startup company Cylink that produced a silicon chip implementation of public-key in the early to mid-1980s. By 1984 Cylink was selling this hardware implementation to large corporations and some departments and agencies of the U.S. federal government (competing against RSA Data Security—Stanford had sublicensed the Diffie-Hellman-Merkle patent to MIT).&nbsp; Other than Cylink, most enterprises focused on MIT’s RSA patent granted in September 1983 for implementation of public-key cryptography. [6, 8]</p>
<p>In December 1978 Diffie became the Manager for Secure Systems Research at Northern Telecom in Mountain View California.&nbsp; In his dozen years in this post, he maintained a center for expertise in advanced computer security for Northern Telecom, Bell Canada, and Bell-Northern Research (the R&amp;D joint venture of Northern Telecom and Bell Canada).&nbsp;&nbsp; This included designing the key management architecture for Northern Telecom’s PDSO security for X.25 packet networks.</p>
<p>In 1991 Diffie left Northern Telecom to become the Chief Security Officer for Sun Microsystems, where he was both a vice president and a Sun Microsystems Fellow. During his time at both Northern Telecom and Sun Microsystems, he was a frequent presenter at computer security conferences and published a number of articles and book chapters on cryptography and its contexts. From his co-published (with Martin Hellman) early critiques of the Data Encryption Standard forward, Diffie has been a policy advocate for rights to private communication—with strong and widespread public-key cryptography as a primary tool and goal.&nbsp; In the early 1990s and beyond he testified before various subcommittees of the U.S. House of Representatives and the U.S. Senate on issues of computer security, cryptography, and privacy.</p>
<p>Diffie co-wrote the book <em>Privacy on the Line: The Politics of Wiretapping and Encryption</em> (1998) with Susan D. Landau.&nbsp; This broadly accessible and influential study placed relatively recent issues—from early public-key, the DES key length debate (resulting in the compromised 56-bit key length), the Clipper Chip (NSA cryptographic device facilitating private communications with the exception of the NSA/U.S. government intelligence having the key) to policies and practices in the 1990s—within the longer historic context of cryptographic systems and wire-tapping to explore topics of law enforcement, national security, privacy protections, and public policy. [3]</p>
<p>From 2009 to 2012 Diffie served as a Visiting Scholar and Affiliate at Stanford University, and then became a Consulting Scholar for Stanford’s Center for International Security and Cooperation. Among his many honors and awards he was the recipient of the Golden Jubilee Award for Technological Innovation from the IEEE Information Theory Society in 1998 and co-recipient (with Hellman and Merkle) of the IEEE Richard W. Hamming Medal in 2010.</p>
<p><em>Jeffrey R. Yost</em></p>
<p>&nbsp;</p>
<p>*Diffie-Hellman Public-Key</p>
<p>As they explain in their landmark paper:</p>
<p style=""margin-left:36.0pt;"">In a public-key cryptosystem enciphering and deciphering are governed by distinct keys, E and D, such that computing D from E is computationally infeasible (e.g. requiring 10<sup>100</sup> instructions).&nbsp; The enciphering key E can be disclosed [in a directory] without compromising the deciphering key D. This enables any user of the system to send a message to any other user enciphered in such a way that only the intended recipient is able to decipher it….The problem of authentication is perhaps an even more serious barrier to the universal adoption of telecommunications for business transactions than the problems of key distribution…[it]…is at the heart of any system involving contracts and billing. Current electronic authentication systems cannot meet the need for a purely digital, unforgeable, message dependent signature. [2]</p>
<p>By convention, cryptography characters “Alice” and “Bob” (seeking secure communication) frequently are used to explain public-key. Alice and Bob agree on large integers <em>n</em> and <em>g</em> with 1&lt; <em>g</em>&lt; <em>n</em>.&nbsp;&nbsp; The selections impact the security of the system.&nbsp; “The modulus n should be a prime; more importantly (n-1)/2 should also be a prime…and <em>g</em> should be a primitive root mod <em>n</em>…[and]...<em>n</em> should be…at least 512 bits long.” [7] The Diffie-Hellman protocol can be stated in basic form in 5 steps. [7]</p>
<p style=""margin-left:36.0pt;"">(1)&nbsp;&nbsp;&nbsp; Alice choses <em>x</em> (a random large integer) and computes <em>X</em>=<em>g<sup>x</sup></em> mod <em>n</em></p>
<p style=""margin-left:36.0pt;"">(2)&nbsp;&nbsp;&nbsp; Bob choses <em>y</em> (a random large integer) and computes <em>Y</em>=<em>g</em><sup>y</sup><sup> </sup>mod <em>n</em></p>
<p style=""margin-left:36.0pt;"">(3)&nbsp;&nbsp;&nbsp; Alice sends <em>X </em>to Bob, while Bob sends <em>Y</em> to Alice (they keep x and y secret from each other)</p>
<p style=""margin-left:36.0pt;"">(4)&nbsp;&nbsp;&nbsp; Alice computes <em>k</em> = <em>Y<sup>x</sup></em> mod <em>n</em></p>
<p style=""margin-left:36.0pt;"">(5)&nbsp;&nbsp;&nbsp; Bob computes <em>k</em>’ = <em>X<sup>y</sup></em> mod <em>n&nbsp;</em></p>
<h5 style=""color: rgb(102, 102, 102); font-family: Georgia, serif; background-color: rgb(255, 255, 255);""><a href=""https://www.acm.org/media-center/2016/march/turing-award-2015"" target=""_blank"">Press Release</a><img alt="""" src=""https://amturing.acm.org/images/logos/pdf_logo.gif"" style=""width: 16px; height: 16px;""></h5>
<p style=""color: rgb(102, 102, 102); font-family: Georgia, serif; font-size: 21.12px; background-color: rgb(255, 255, 255);""><span style=""line-height: 18.9px; font-style: italic;"">ACM presented the 2015 A.M. Turing Award at its annual Awards Banquet on June 11, 2016 in San Francisco, CA.</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/diffie_8371646.cfm""><img src=""/images/lg_aw/8371646.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Whitfield Diffie ""></a>
</div>
<h6><span class=""label"">BIRTH:</span></h6>
<p>5 June 1944, Washington, D.C., USA</p>
<h6><span class=""label"">EDUCATION:</span></h6>
<p>B.S. (Mathematics, Massachusetts Institute of Technology, 1965). Honorary Ph.D. (Swiss Federal Institute of Technology, 1992).</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>MITRE Corporation (Research Assistant, 1965-1969); Stanford University, Artificial Intelligence Laboratory (Research Programmer 1969-1973); Stanford University (Graduate Student and Research Assistant, 1975-1978); BNR, Inc./Northern Telecom (Manager, Secure Systems Research, 1978-1991); Sun Microsystems (Chief Security Officer, Distinguished Engineer, Sun Microsystems Fellow, 1991-2009); Stanford University (Visiting Scholar and Affiliate, 2009-2012); Stanford University, Center for International Security and Cooperation (Consulting Scholar).</p>
<h6><span class=""label"">HONORS AND AWARDS:</span></h6>
<p>IEEE Information Theory Society Golden Jubilee Award for Technological Innovation, with M. Hellman (1998); NIST/NSA National Computer Systems Security Award, with M. Hellman (1996); Franklin Institute’s Levy Medal, with M. Hellman (1997); ACM Kannellakis Award, with M. Hellman (1997); IEEE Information Theory Society Golden Jubilee Award, with M. Hellman (1998); IEEE Kobayashi Award, with M. Hellman and R. Merkle (1999); Fellow, International Association for Cryptographic Research (2004); IEEE Richard W. Hamming Medal, with M. Hellman and R. Merkle (2010); ACM Turing Award, with M. Hellman (2015).</p>","","https://dl.acm.org/author_page.cfm?id=81100513962","Whitfield Diffie","<li class=""bibliography""><a href=""/bib/diffie_8371646.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=2949031&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""award-video""><a href=""/vp/diffie_8371646.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/diffie_8371646.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/diffie_8371646.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/diffie_8371646.cfm""><span></span>Video Interview</a></li>"
"1573179088-661","https://amturing.acm.org/award_winners/mccarthy_1118322.cfm","Dr. McCarthy's lecture ""The Present State of Research on Artificial Intelligence"" is a topic that covers the area in which he has achieved considerable recognition for his work.","<p><strong>John McCarthy was born September 4, 1927 in Boston, Massachusetts to immigrant parents.</strong> His father, John Patrick McCarthy, was an Irish Catholic who became a labor organizer and later the Business Manager of the <em>Daily Worker,</em> a national newspaper owned by the Communist Party USA. His mother, Ida Glatt, was a Lithuanian Jewish immigrant who worked for a wire service, then for the <em>Daily Worker</em> and finally as a social worker.</p>
<p>The family moved to New York City, but John was a sickly child and his parents took him to Los Angeles for his health. There he began reading books on mathematics at the nearby California Institute of Technology (Caltech) and when he was admitted there as an undergraduate in 1944 he was given advanced standing. He then was suspended for failing to attend physical education classes and spent some time in the U.S. Army, but still managed to graduate in 1948.</p>
<p>After a year of graduate studies at Caltech he went to Princeton and received a PhD in mathematics in 1951 based on a dissertation that solved a problem in partial differential equations. He taught there until 1953, when he became an assistant professor of mathematics at Stanford until 1955. One of his first major publications was a book (<em>Automata Studies</em> [<a href=""/bib/mccarthy_0239596.cfm#link_11"">11</a>]) he and Claude Shannon co-edited during this period.<br>
Having been raised by Communist parents, he became interested in the Soviet Union and developed friendships with several computer scientists there. He also learned to speak Russian and visited the Soviet Union a number of times and, in doing so, became aware of the human rights violations of that regime. He began taking an active role in support of the human rights of computer professionals and, over time, moved further away from Communist ideas. Vera Watson, his second wife, was the daughter of Russian missionaries living in China. She helped persuade him to move further to the right, and he became a conservative Republican. Unfortunately she died in a climbing accident in Nepal while trying to ascend Annapurna.</p>
<p>His work has emphasized epistemological problems—the problems of what information and what modes of reasoning are required for intelligent behavior. Given that McCarthy was primarily a mathematician and technologist who had little use for puffery, it is ironic that his most widely recognized contribution turned out to be in the field of marketing, specifically in choosing a brand name for the field. Having noticed that the title of the Automata Studies book didn’t stir up much excitement, when he subsequently moved to Dartmouth College he introduced the name artificial intelligence at a 1956 conference there [<a href=""/bib/mccarthy_0239596.cfm#link_3"">3</a>] and saw that it was embraced both by people working in the field and the general public.</p>
<p><br>
Moving on to the Massachusetts Institute of Technology (MIT) in 1958, he and another Turing Award recipient, <a href=""/award_winners/minsky_7440781.cfm"">Marvin Minsky</a>, formed the Artificial Intelligence Project there, where pioneering work took place in a wide range of fields from robotics, the theory of computation and common sense reasoning [<a href=""/bib/mccarthy_0239596.cfm#link_4"">4</a>], to human-computer interfaces. McCarthy also created the <a href=""https://en.wikipedia.org/wiki/Lisp_%28programming_language%29"" target=""_blank"">LISP</a> (LISt Processor) language [<a href=""/bib/mccarthy_0239596.cfm#link_1"">1</a>]. It became an important tool in artificial intelligence research and is still widely used. He also made substantial contributions to the algebraic languages ALGOL 58 and 60.</p>
<p>McCarthy’s students developed the first computer program to convincingly play chess. It ran initially on an IBM 704 computer (later on an IBM 709 and <a href=""/info/mccarthy_0239596.cfm#add_1"">7090</a>) and incorporated McCarthy’s version of an <a href=""https://en.wikipedia.org/wiki/Alpha-beta_pruning"" target=""_blank"">alpha-beta pruning</a> scheme to reduce the number of positions that had to be considered.</p>
<p>In this period McCarthy observed the nearby development of the SAGE air defense system, which had been initiated by a computer group at MIT. It included timesharing support for many concurrent users at large screen displays with point-and-click interfaces. He wanted to use interactive computing in his research, but SAGE was a special purpose system that did not support interactive program development. He then came up with a scheme for creating general purpose timesharing and described it in a memorandum [<a href=""/bib/mccarthy_0239596.cfm#link_5"">5</a>]. His approach inspired a number of groups in the MIT community to build such systems.</p>
<p><br>
The first demonstration system, called CTSS, developed by Prof. <a href=""/award_winners/corbato_1009471.cfm"">Fernando Corbato</a> and his colleagues, began operating in June 1962. McCarthy concurrently developed another timesharing system through his consultancy at Bolt Beranek and Newman (BBN) with J.C.R. Licklider and Edward Fredkin, and it began working a few months later. CTSS led directly to the creation of Project MAC, which revolutionized computing at MIT and inspired the switch to timesharing systems in many places.</p>
<p><br>
General purpose timesharing was an essential precursor to computer networking. The first general purpose computer network, created exclusively as a network of timesharing systems, was called ARPAnet and was conceived by J.C.R. Licklider, then specified by Dr. Lawrence Roberts and a group of academics who wanted to be able to collaborate by sharing resources and ideas. It was funded by the Advance Research Projects Agency (ARPA), constructed by BBN and became operational around 1970. Its successor, the internet, has always depended on timesharing systems at its heart, which came to be called “servers.” All of that likely would have been delayed if McCarthy had not instigated timesharing system development in the early 1960s.</p>
<p>In late1962 McCarthy left MIT to return to Stanford University’s mathematics department as a full professor. He started a new Artificial Intelligence Project there, which was soon funded by the Defense Department’s Advanced Research Projects Agency (ARPA, later called DARPA). He also initiated the development of the first display-based timesharing system, called Thor [<a href=""/bib/mccarthy_0239596.cfm#link_6"">6</a>], which included many of the features found in modern personal computers and subsequently was used by others in the development of computer-aided instruction systems.</p>
<p>McCarthy also continued to develop his chess program, and in 1965 he challenged a group at the Moscow Institute for Theoretical and Experimental Physics (ITEP) to a match, with moves exchanged by telegraph. The competition received substantial media attention. Neither program did very well, although the Russian program won. The cryptic telegraphic exchanges were reportedly noticed by the Russian KGB security authorities, who investigated.</p>
<p>In 1968 McCarthy (and three others) bet Chess Master <a href=""https://en.wikipedia.org/wiki/David_Levy_%28chess_player%29"" target=""_blank"">David Levy</a> that a computer program would beat him at chess within the following 10 years. The bet gained publicity and eventually involved more than $2000 (a considerable sum for Levy, who was a graduate student in Glasgow at the time). McCarthy had to pay up, though a computer did eventually beat the world champion in 1997.</p>
<p>McCarthy always loved to hear about new ideas in almost any field, and generated many of them himself. He acquired many new high tech devices to see what he could do with them. He continued his work on mathematical theory of computation, and on developing programs with common sense including formalization of non-monotonic reasoning whereby people and computers draw conjectural conclusions by assuming that complications are absent from a situation [<a href=""/bib/mccarthy_0239596.cfm#link_4"">4</a>, <a href=""/bib/mccarthy_0239596.cfm#link_9"">9</a>].</p>
<p>In 1965 the Stanford Computer Science Department spun off from Mathematics and became independent. McCarthy’s support from ARPA increased to include a million dollar computer facility, initially using a DEC PDP-6 timesharing system and later a DEC KA-10 computer. He recruited Lester Earnest as executive officer of the Project, and together they encouraged a diverse set of research projects to use the new facility when it became available in mid-1966.</p>
<p>McCarthy’s group made many significant contributions to a number of different computer related fields. The following paragraphs will give an indication of a few of the major projects that were done by this group.</p>
<p>McCarthy’s former student , and later Turing Award recipient, <a href=""/award_winners/reddy_6247682.cfm"">Raj Reddy</a>, who did pioneering work in speech understanding, accepted a Stanford faculty appointment and scaled up that project. A music graduate student named John Chowning put together a computer music project and earned a faculty appointment. That project became a world leader in its field and eventually spun off as the Center for Computer Research in Music and Acoustics (CCRMA).</p>
<p>Following up on McCarthy’s interest in robotics, Lester Earnest initiated a hand-eye project that used information from a video camera to guide a robotic arm in doing assembly tasks, a project that was taken over by Jerome Feldman, a new faculty member. Ultimately, it led to the Robotics Institute at Carnegie Mellon under the direction of Raj Reddy when he moved there. Earnest also put together a robot vehicle with the goal of guiding it by visual information from a video camera and McCarthy took that over. However the performance of that system turned out to be severely limited by the computer processing speeds then available.</p>
<p>Dr. Kenneth Colby brought in his <em>Higher Mental Functions</em> project that developed a conversational model of a paranoid called Parry and also developed a computer interface that helped autistic children.</p>
<p>In 1971 the Stanford AI Project became what might be the first computer facility anywhere in the world to put display terminals on everyone’s desk. Those terminals also provided access to various video cameras in the laboratory and to live television, which encouraged football fans to work on Saturdays and Sundays.</p>
<p>Concurrently, a small group of graduate students led by David Poole and Phil Petit were given support to develop SUDS (Stanford University Drawing System), the first display-based computer aided design system for digital systems, which they used to design a new computer that became the DEC KL-10. SUDS produced artwork for printed circuit cards and instructions for back panel wiring machines to facilitate automatic production. It became the primary design tool of Digital Equipment Corporation and a number of other corporations for many years.</p>
<p>By the early 1970s McCarthy had begun to think about the potential of networks of personal computers in the home and presented a paper on “The Home Information Terminal” [<a href=""/bib/mccarthy_0239596.cfm#link_7"">7</a>]. Given that the diversity of projects had greatly expanded [<a href=""/bib/mccarthy_0239596.cfm#link_8"">8</a>], in 1972 the name of the facility was changed to Stanford Artificial Intelligence Laboratory (SAIL).</p>
<p><a href=""/award_winners/cerf_1083211.cfm"">Vint Cerf’s</a> development at Stanford of the TCP/IP protocols, upon which the internet was based, was supported by the same DARPA contract that supported SAIL. Over time SAIL produced many able PhDs and other graduates and became a hotbed of entrepreneurial activity that produced dozens of corporate spinoffs, both direct and indirect, including activities at Microsoft, Sun Microsystems, DE Shaw &amp; Co., Amazon.com, Cisco Systems and Yamaha’s music synthesizer business. All of that was enabled by the diversity of SAIL projects that shared facilities and ideas.</p>
<p>Over time sixteen ACM Turing Awards have been given to people who had been affiliated with SAIL.</p>
<p>John McCarthy nominally retired at the end of 2000 but remained very active in developing and documenting new ideas. He passed away at age 84 at his Stanford home on 24 October 2011. He is survived by his first wife, Martha Coyote, and their two daughters Susan and Sarah McCarthy and his third wife, Carolyn Talcott, and their son Timothy McCarthy.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Lester Earnest</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/mccarthy_1118322.cfm""><img src=""/images/lg_aw/1118322.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""John McCarthy""></a>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>4 September 1927, Boston, Massachusetts</p>
<h6 class=""label"">DEATH:</h6>
<p>24 October 2011, Stanford, California</p>
<h6 class=""label"">EDUCATION:</h6>
<p>BS mathematics, California Institute of Technology (1948); PhD mathematics, Princeton University (1951).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Private, U.S. Army (1945-1946); Instructor in mathematics, Princeton University (1951-1953); Assistant Professor of mathematics, Stanford University (1953-1955); Assistant Professor of mathematics, Dartmouth College (1955-1958); Assistant Professor of communication science, Massachusetts Institute of Technology (1958-1962); Professor of mathematics, Stanford University (1962-1965); Professor of Computer Science, Stanford University (1965-2011); Director of the Stanford Artificial Intelligence Laboratory (1966-1980).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Member of the National Academy of Engineering (1987) and National Academy of Sciences (1989); A.M. Turing Award of the Association for Computing Machinery (1971); Research Excellence Award of the International Conference on Artificial Intelligence (1985); Kyoto Prize (1988);&nbsp;<span style=""line-height: 20.8px;"">National Medal of Science (1990);&nbsp;</span><span style=""line-height: 1.3;"">Computer History Museum Fellow (1999); Benjamin Franklin Medal in Computer and Cognitive Science (2003). He has also received many other honors and prizes from international associations and universities as well as from the United States government.</span></p>
<p>&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81406600200","John McCarthy","<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283926&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/mccarthy_1118322.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179027-656","https://amturing.acm.org/award_winners/tarjan_1092048.cfm","With John E Hopcroft, for fundamental achievements in the design and analysis of algorithms and data structures.","<p>
<strong>Bob Tarjan was born on April 30, 1948 in Pomona, California.</strong> He received a B.S. in mathematics from Caltech in 1969, and was determined to do a Ph.D. but was undecided between mathematics and computer science. He finally chose computer science as a way to use his mathematical skills to solve problems of more practical interest. He entered Stanford to study artificial intelligence, but, guided by his course advisor <a href=""/award_winners/knuth_1013846.cfm"">Don Knuth</a>, he was soon reading Volume 1 of Knuth’s <em>The Art of Programming</em> and studying the analysis of algorithms.</p>
<p>
At Stanford Tarjan began a collaboration with <a href=""/award_winners/hopcroft_1053917.cfm"">John Hopcroft</a> on developing efficient algorithms for graph problems. At the time there was no commonly used model for measuring efficiency analytically. Hopcroft and Tarjan decided that the model of computation would be a hypothetical computer in which the goal of the algorithm design was to minimize the worst case running time. Constant factors in the running time were ignored, so as to be machine independent. Their example problem was testing the planarity of a graph, that is, whether it can be drawn in a plane so that no edges cross.</p>
<p>
This work led to the first linear time algorithm for planarity, which was the subject of Tarjan's Ph.D. thesis completed under the supervision of <a href=""/award_winners/floyd_3720707.cfm"">Robert W. Floyd</a> in 1972. It emphasized depth-first search as an important algorithmic technique and advocated the use of an adjacency-list representation for sparse graphs, rather than an adjacency matrix. Other applications of the depth-first search method followed shortly, including Tarjan's linear time algorithm for finding strongly connected components. These techniques are now covered in most undergraduate courses in algorithm design. Tarjan and Hopcroft jointly received the Turing award for this and related work in 1986.</p>
<p>
Also now part of the algorithmic canon is Tarjan's work on data structures. Tarjan realized that designing a data structure to minimize the worst case running time for each operation was unnecessarily limiting; what mattered was the total running time of the sequence of operations. Alternatively, from the point of view of the data structure, one could study the algorithm’s <em>amortized</em> running time, that is, its average running time per operation over a long enough sequence of inputs.</p>
<p>
An early and well-known example of this work is Tarjan's analysis of the “union-find” data structure. [<a href=""/bib/tarjan_1092048.cfm#bib_3"">3</a>]&nbsp; The union-find problem is to maintain a collection of disjoint sets so as to efficiently perform two operations: <strong>union</strong>, which joins two sets into a single set, and <strong>find</strong>, which returns the set containing a specified element. Representing each set as a tree, two simple methods were used to give improved performance: union by weight and path compression, but their impact was not completely understood. In 1975, Tarjan was the first to analyze their combined performance exactly, showing an almost constant time per operation over long enough sequences.&nbsp; This nontrivial analysis gives a time which was proportional to <a href=""https://en.wikipedia.org/wiki/Ackermann_function"" target=""_blank"">inverse Ackermann's function</a> of the number of operations and elements. This was later shown to be optimal.</p>
<p>
Tarjan held academic positions at Cornell University and then at the University of California in Berkeley before returning to Stanford University in 1974. At Stanford, Tarjan and his student <a href=""https://en.wikipedia.org/wiki/Daniel_Sleator"" target=""_blank"">Danny Sleator</a> worked on obtaining faster solutions for the maximum flow problem by efficiently maintaining information about residual flow. They also devised the dynamic tree structure, a forest of disjoint trees in an edge-weighted graph, where trees can be split into two or linked together, and, for any path in a tree, the minimum weight edge in the path can be retrieved. Each operation runs in time which is logarithmic in the size of the tree. Underlying this data structure is a <a href=""https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree"" target=""_blank"">balanced ordered binary tree</a>.</p>
<p>
After Tarjan and Sleator moved to AT&amp;T Bell Laboratories in 1980, they discovered a simpler means to maintain balanced binary trees, and created the “self-adjusting” binary search tree known as a <a href=""https://en.wikipedia.org/wiki/Splay_tree"" target=""_blank"">splay tree</a>. At the time, many kinds of balanced binary search trees were known which would enable lookups, inserts, deletes and other operations to be done in a worst case time which is logarithmic in the size of the tree. However maintaining these balanced trees required extra space for balance information, and complicated algorithms. The splay tree is simpler and requires no extra balance information, but has amortized running time (rather than worst case running time) which is logarithmic in the size of the tree. Whether splay trees perform as well as any binary search trees up to a constant factor is a question still unresolved. Known as the <em>dynamic optimality conjecture</em>, this open problem continues to inspire new research.</p>
<p>
Tarjan's book <em>Data Structures and Network Algorithms</em> [<a href=""/bib/tarjan_1092048.cfm#bib_6"">6</a>] beautifully presents his work on disjoint sets, tree data structures, minimum spanning trees, matching, and maximum flow problems. Regarded as a ``model of precision and clarity"", it received the <a href=""https://en.wikipedia.org/wiki/Frederick_W._Lanchester_Prize"" target=""_blank"">Frederick W. Lancester Prize</a> in 1984.</p>
<p>
Again exploiting the fact that data structures with good amortized performance would suffice, Tarjan and <a href=""https://en.wikipedia.org/wiki/Michael_Fredman"" target=""_blank"">Michael Fredman</a> devised the <a href=""https://en.wikipedia.org/wiki/Fibonacci_heap"" target=""_blank"">Fibonacci heap</a>, a priority queue which implements all standard heap operations except deletion in constant amortized time. Appearing in 1985, this data structure provided significant speed-ups to several important combinatorial problems including minimum spanning tree, shortest paths, and the assignment problem.</p>
<p>
In a seminal paper [<a href=""/bib/tarjan_1092048.cfm#bib_7"">7</a>] in 1985, Tarjan and Sleator studied the performance of “online algorithms”, which process inputs as they happen without seeing them all at the same time, and compared them to “offline algorithms” that get to see the whole input stream at once. They introduced the notion of “competitive analysis” to rate online algorithms compared to the optimal offline algorithm, where worst-case data is imagined to be generated by an “adversary”.</p>
<p>
They used two example problems, the list update problem and the paging problem. The list update problem is as follows: given a list of items where the cost of accessing an item is its distance from the front of the list, come up with a strategy of reordering the list so that the total cost for a sequence of access requests is minimized. Their paper shows that if each requested item is moved to the front (at constant cost) then the cost of servicing any online sequence of requests is no more than twice the cost of the optimal offline strategy that knows all future requests. The paging problem is to determine a strategy for moving pages out of cache so as to minimize the number of cache faults.</p>
<p>
Tarjan remained at AT&amp;T until 1989, while also serving as an adjunct professor at New York University from 1981-85. At NYU, Tarjan and his student Neal Sarnak began the first systematic study of persistent data structures—data structures which preserve the previous version of themselves when modified. This initial work resulted in a publication [<a href=""/bib/tarjan_1092048.cfm#bib_11"">11</a>] with Jim Driscoll, Danny Sleator, and later several others with Tarjan's student Haim Kaplan at Princeton.</p>
<p>
In 1985 Tarjan joined the faculty at Princeton University, where he is currently the James S. McDonnell Distinguished University Professor. He remained actively involved in industry at <a href=""https://en.wikipedia.org/wiki/NEC"" target=""_blank"">NEC Research Institute</a>, <a href=""https://en.wikipedia.org/wiki/InterTrust_Technologies"" target=""_blank"">Intertrust</a> and the Compaq/<a href=""https://en.wikipedia.org/wiki/HP_Labs"" target=""_blank"">HP Research Labs</a>.</p>
<p>
Tarjan returned several times to work on maximum flow and other network flow problems, with Andrew Goldberg and others. In 1995, Tarjan, with David Karger and Phil Klein, published the first linear expected time algorithm for the minimum spanning tree problem [<a href=""/bib/tarjan_1092048.cfm#bib_12"">12</a>].</p>
<p>
Tarjan has written over 250 papers with over 190 co-authors, and holds 15 patents. He continues to work in the area of combinatorial algorithms and data structures. In the spirit of Paul Erdös, he searches and inspires others to search for algorithms from “The Book"" that records God’s best and most elegant mathematical proofs and algorithms.</p>
<p align=""right"">
<span class=""callout"">Author: V. King</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/tarjan_1092048.cfm""><img src=""/images/lg_aw/1092048.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Robert E Tarjan""></a>
<br><br>
<h6 class=""label""><a href=""/photo/tarjan_1092048.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>April 30, 1948, Pomona, California</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>B.S., California Institute of Technology (1969, Mathematics); MS Stanford University (1971, Computer Science); Ph.D., Stanford University (1972, Computer Science with minor in Mathematics).</p>
<h6><span class=""label"">EXPERIENCE:</span></h6>
<p>Assistant Professor of Computer Science, Cornell University (1972 – 1973); Miller Research Fellow, University of California, Berkeley, California (1973 – 1975); Stanford University Assistant Professor of Computer Science (1974 – 1977), Associate Professor of Computer Science (1977 – 1980); Member of Technical Staff, AT&amp;T Bell Laboratories, Murray Hill, New Jersey (1980 – 1989); Adjunct Professor of Computer Science, New York University (l98l – 1985); James S. McDonnell Distinguished University Professor of Computer Science, Princeton University (from 1985); Co-Director, National Science Foundation Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) (1989-1994, 2001 -); Fellow, NEC Research Institute, Princeton, New Jersey (1989 – 1997); Visiting Scientist, Massachusetts Institute of Technology (1996); Chief Scientist, InterTrust, and Senior Research Fellow, STAR Labs, InterTrust Technologies Corporation, Sunnyvale, CA (1997 – 2001); Corporate Fellow, Compaq Computer Corporation, Houston, TX (2002), Chief Scientist (2002-2003); Senior Fellow (from 2003), Hewlett Packard Corporation, Palo Alto, CA</p>
<h6><span class=""label"">HONORS AND AWARDS:</span></h6>
<p>Miller Research Fellowship, University of California, Berkeley,<br>
California (1973-1975); Guggenheim Fellowship (1978-1979); Nevanlinna Prize in Information Science (1983); National Academy of Sciences Award for Initiatives in Research (1984); Honorable Mention, Lanchester Prize of the Operations Research Society of America (1984); Fellow, American Academy of Arts and Sciences (1985); AT&amp;T Bell Laboratories, Distinguished Member of Technical Staff (1985); A. M. Turing Award of the Association for Computing Machinery (1986); Member, National Academy of Sciences (1987); Member, National Academy of Engineering (1988); Fellow, American Association for the Advancement of Science (1990); Member, American Philosophical Society (1990); Foundation Fellow, Institute for Combinatorics and its Applications (1991); Honorable Mention, Lanchester Prize of the Operations Research Society of America (1993); Fellow, Association for Computing Machinery (1994); Fellow, New York Academy of Sciences (1994); Paris Kanellakis Award in Theory and Practice, Association for Computing Machinery (1999); Blaise Pascal Medal in Mathematics and Computer Science, European Academy of Sciences (2004); Fellow, Society for Industrial and Applied Mathematics (2009); Edelman Award, INFORMS, member of winning HP team (2009); Caltech Distinguished Alumni Award, California Institute of Technology (2010).<br>
&nbsp;</p>","","https://dl.acm.org/author_page.cfm?id=81100645220","Robert (Bob) Endre Tarjan","<li class=""bibliography""><a href=""/bib/tarjan_1092048.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283944&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/tarjan_1092048.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/tarjan_1092048.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/tarjan_1092048.cfm""><span></span>Video Interview</a></li>"
"1573179716-703","https://amturing.acm.org/award_winners/wilkes_1001395.cfm","Professor Wilkes is best known as the builder and designer of the EDSAC, the first computer with an internally stored program. Built in 1949, the EDSAC used a mercury delay line memory. He is also known as the author, with Wheeler and Gill, of a volume on ""Preparation of Programs for Electronic Digital Computers"" in 1951, in which program libraries were effectively introduced.","<p><strong>Maurice Vincent Wilkes was born 26 June1913 in Dudley, in the county of Staffordshire in the English Midlands.</strong> His father was a financial officer for the estate of the Earl of Dudley which had extensive mining interests. His mother was a housewife. He was educated at King Edward VI Grammar School, Stourbridge. In his teens he built crystal sets, read&nbsp;<em>Wireless World</em>, and eventually gained a radio amateurs license -- a background which proved useful when it came to building electronic computers two decades later. He entered St Johns College, Cambridge University, in 1931, where he read mathematics.</p>
<p>In October 1935 he became a research student at the Cavendish Laboratory, Cambridge University, working on the propagation of long radio waves. The following spring, he attended a lecture by Douglas Hartree, a computing expert and professor of mathematical physics at Manchester University. Hartree described the “differential analyzer” invented by Vannevar Bush at MIT. This was an analog computing machine for the integration of differential equations. Hartree had built a model differential analyzer from Meccano (a British constructor toy similar to the American <em>Erector Set</em>), which proved surprisingly useful. A copy of this machine was built at Cambridge under the direction of John Lennard-Jones, professor of theoretical chemistry, and Wilkes became an enthusiastic user. In early 1937, the University set up a Computing Laboratory under the direction of Lennard-Jones, and Wilkes was appointed assistant director from October 1937.</p>
<p>On the outbreak of war, the Computing Laboratory was taken over by the military. Wilkes joined the scientific war effort and worked on radar and operations research. This gave him an ideal background, and a network of contacts, for building computers after the war.</p>
<p>In October 1945, Wilkes returned to Cambridge to take full charge of what was now called the Mathematical Laboratory. In May 1946, he was visited by L. J. Comrie, a pioneer in mechanical computation, who brought with him a copy of the <em>First Draft of a Report on the EDVAC </em>written by John von Neumann, summarizing the deliberations of the computer group at the Moore School of Electrical Engineering, University of Pennsylvania. The Moore School had just completed the ENIAC, the world's first electronic computer for defense calculations, and the EDVAC was the design for a follow up machine. Wilkes had never seen the report before and stayed up late into the night reading it. He recognized it at once as “the real thing” and decided that the laboratory had to have one.</p>
<p>Later in 1946, Wilkes was invited to attend a summer school in computer design organized by the Moore School. Because of difficulties getting a transatlantic passage, he did not arrive on the course until mid-August, by which time he had missed more than half. Rarely short on confidence, Wilkes decided he had not missed much of consequence. Sailing home on the <em>Queen Mary </em>he began the design of a machine he called the Electronic Delay Storage Automatic Calculator -- EDSAC for short, an acronym consciously chosen as a tribute to the EDVAC.</p>
<p>Work started on building the EDSAC in early 1947. Almost everything had to be done from first principles—memory technology, electronic arithmetic and logic, and control circuits. Cambridge University was at the center of UK computing at this time, in part because of the fortnightly colloquia Wilkes established, which were attended by members of almost every computer project in the country.</p>
<p>The <a href=""/info/wilkes_1001395.cfm#link_2"">EDSAC</a>&nbsp;sprang into life on 6 May 1949, the world's first practical stored program electronic computer. Manchester University had got there first in June 1948 with an experimental machine, but the EDSAC was the first capable of running realistic programs. By the beginning of 1950 the Laboratory was offering a regular computing service.</p>
<p>Wilkes decided that the Laboratory would specialize in programming rather than building computers. He was perhaps the first person to recognize that what we now call software (a term not used until about 1960) would prove to be a worthwhile academic pursuit. He assigned the design of the EDSAC programming system to a research student David Wheeler (later a professor of computer science at Cambridge). The system that Wheeler created was a tour de force that was admired worldwide. In 1951 Wilkes got the techniques published as the first textbook on programming The Preparation of Programs for an Electronic Digital Computer, although the book was usually known for its three authors as “Wilkes, Wheeler and Gill” or WWG for short [<a href=""/bib/wilkes_1001395.cfm#link_1"">1</a>]. The third author, Stanley Gill, was another young researcher who later became a major figure in British computing until his untimely death in 1975.</p>
<p>EDSAC was soon loaded to capacity, and plans were laid for a successor, EDSAC 2. Wilkes came up with a new design principle -- which he called <a href=""/info/wilkes_1001395.cfm#link_1"">microprogramming</a>—that greatly simplified the logical design of the new computer. Microprogramming was Wilkes’ most important scientific contribution to computing, and had he done nothing else he would be famous for that [<a href=""/bib/wilkes_1001395.cfm#link_2"">2</a>]. In the early 1960s IBM based its world beating System/360 computers around the idea, and it remains a cornerstone of computer architecture.</p>
<p>Wilkes was appointed Professor of Computer Technology at Cambridge in 1965. He remained director of the Computer Laboratory (the name was changed from the Mathematical Laboratory in 1970) until he reached the statutory retirement age of 67 in 1980. His tenure had seen computers evolve from scientific instruments to information processing machines that were the basis of a worldwide industry. The Laboratory kept up with changing trends, primarily in computer engineering, developing time-sharing systems in the 1960s and computer networking in the 1970s. Wilkes was very good at keeping up with technology trends, and preventing either himself or the Laboratory getting locked into dying research fashions.</p>
<p>Cambridge’s prominence and Wilkes' confident manner led to a constant stream of invitations to give lectures and to participate on international committees. He played an influential role in promoting computing in Britain, being elected to the Royal Society in 1956, becoming inaugural president of the British Computer Society in 1957, and serving as the British representative for the International Federation of Information Processing Societies.</p>
<p>Wilkes received many awards and academic honors. In addition to the ACM Turing Award in 1967, he received the Harry H. Goode Memorial Award of the IEEE in 1968, the McDowell Award of the IEEE Computer Society in 1981, and the Faraday Medal of the Institution of Electrical Engineers also in 1981. In 1992 he was the first recipient of the Kyoto Prize. He received honorary doctorates from several universities, including his alma mater Cambridge University. He was knighted in 2000.</p>
<p>Following his retirement from Cambridge University in 1980, he took up a position as a consulting engineer with the Digital Equipment Corporation in Maynard, Massachusetts. In 1986 he returned to Cambridge, where he became a board member of Olivetti-AT&amp;T Research Laboratories. As an emeritus professor, he maintained a close association with the Computer Laboratory until the last months of this life. Wilkes was deeply interested in the history of his subject. He made a study of Charles Babbage, and wrote a number of important articles on the computer pioneer and his milieu [<a href=""/bib/wilkes_1001395.cfm#link_4"">4</a>].</p>
<p>Wilkes died in Cambridge, 29 November 2010. He married Nina Tyman in 1948, who predeceased him. They had one son and two daughters.</p>
<p style=""text-align: right;""><span class=""callout"">Author: Martin Campbell-Kelly</span><br>
&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/wilkes_1001395.cfm""><img src=""/images/lg_aw/1001395.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Maurice V. Wilkes""></a>
<br><br>
<h6 class=""label""><a href=""/photo/wilkes_1001395.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6 class=""label"">BIRTH:</h6>
<p>26 June 1913, Dudley, England</p>
<h6 class=""label"">DEATH:</h6>
<p>29 November 2010, Cambridge, England</p>
<h6 class=""label"">EDUCATION:</h6>
<p>King Edward VI Grammar School, Stourbridge, England; BA (1934 - mathematics), MA (1936), PhD (1937 - physics) St Johns College, Cambridge University, England; Honorary Degrees: Newcastle-upon-Tyne, Hull, Kent, City of London, Bath, Amsterdam, Munich, Linköping, Cambridge University, University of Pennsylvania</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Head of Computer Laboratory, Cambridge University, 1945-1980; Professor of Computer Technology, 1965-80; Fellow, St John's College, Cambridge, 1950 – 2010.</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Fellow, Royal Society, 1956; First President, British Computer Society, 1957-60, Distinguished Fellow, 1973; Foreign Honorary Member, American Academy of Arts and Sciences, 1974; Fellow, Royal Academy of Engineering, London, 1976; Foreign Associate, US National Academy of Engineering. 1977; Foreign Corresponding Menber, Royal Spanish Academy of Sciences, 1979; Foreign Associate, US National Academy of Sciences, 1980; Foreign Corresponding Member, Spanish Academy of Engineering, 1999; Honorary Freeman, The Worshipful Company of Scientific Instrument Makers, 2000; Turing Lecturer, Association for Computing Machinery, 1967; Harry Goode Memorial Award, American Federation for Information Processing Societies, 1968; Eckert-Mauchly Award, Association for Computing Machinery and IEEE Computer Society, 1980; IEEE Computer Society Pioneer Award (Charter Recipient), 1980; McDowell Award, IEEE Computer Society, 1981; Faraday Medal, IEE, London, 1981; Pender Award, University of Pennsylvania, 1982; C&amp;C Prize, Tokyo 1988; ITALGAS Prize for Computer Science, Turin, 1991; Kyoto Prize, Japan, 1992; Fellow of the Association for Computing Machinery, 1994; Von Neumann Medal, IEEE, 1997; Mountbatten Medal(with T. Kilburn), National Electronics Council, London, 1997; Knighted, 2000; Fellow, Computer History Museum, 2001.</p>","","https://dl.acm.org/author_page.cfm?id=81100297470","Maurice V. Wilkes","<li class=""bibliography""><a href=""/bib/wilkes_1001395.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283922&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/wilkes_1001395.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/wilkes_1001395.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179825-711","https://amturing.acm.org/award_winners/backus_0703524.cfm","For profound, influential, and lasting contributions to the design of practical
high-level programming systems, notably through his work on FORTRAN, and for seminal publication of formal
procedures for the specification of programming languages.","<p><strong>John Warner Backus was born on December 3, 1924, in Philadelphia, Pennsylvania, and grew up in Wilmington, Delaware, the son of a wealthy stockbroker.</strong> He attended the Hill School in Pottstown, Pennsylvania. He was not a good student, although he eventually graduated in 1942, whereupon he entered the University of Virginia and majored in chemistry. There he also had a difficult time at school, and was eventually expelled due to poor attendance after less than a year. He was then drafted into the US Army where, with the rank of corporal, he commanded an antiaircraft battery at Fort Stewart, Georgia and stayed in the United States for the remainder of World War II.</p>
<p>Due to excellent results on his military aptitude tests, Backus was first directed to the engineering program at the University of Pittsburgh and later to a premedical program at Haverford College outside of Philadelphia. During his medical studies, he was diagnosed with a cranial bone tumor, which was surgically removed and replaced with a metal plate. In March 1945, he attended the Flower and Fifth Avenue Medical School in New York City, but dropped out after nine months “because all you had to do was memorize stuff<a href=""#_ftn1"" name=""_ftnref1"" title="""">[1]</a>”. After one more operation to replace the plate in his head, this time with one he made himself, Backus left the army in 1946 with an honorable medical discharge.</p>
<p>Backus settled in New York City, but was undecided about his future. Although he had no skills in electronics, he entered a radio technician school to learn how to build a radio receiver. That work led him to study mathematics, and so he enrolled in a mathematical program at Columbia University. In the spring of 1949, just before his graduation, he happened to be walking by the IBM Computing Center on Madison Avenue where IBM had their <a href=""https://en.wikipedia.org/wiki/IBM_SSEC"" target=""_blank"">Selective Sequence Electronic Calculator</a> (SSEC), a one-of-a-kind relay and vacuum tube computer designed at the Watson Scientific Computing Laboratory at Columbia. He made a passing comment to his tour guide about his interest in working on the computer, was immediately taken upstairs to meet the SSEC project director, and hired on the spot as a programmer.</p>
<p>One of the main uses of the SSEC at that time was the calculation of <a href=""https://en.wikipedia.org/wiki/Ephemeris"" target=""_blank"">ephemeris tables</a>, a task on which Backus worked for three years. The techniques developed by Backus and his team were later used by NASA for the Apollo lunar missions of the 1960s.</p>
<p>Programming at that time meant writing instructions at the machine level. To facilitate the process, Backus invented a program called <a href=""https://en.wikipedia.org/wiki/Speedcoding"" target=""_blank"">Speedcoding</a> [<a href=""/bib/backus_0703524.cfm#bib_1"">1</a>, <a href=""/bib/backus_0703524.cfm#bib_2"">2</a>], which allowed operations on floating point numbers to be described in a more symbolic form. At that time IBM was developing the <a href=""https://en.wikipedia.org/wiki/IBM_704"" target=""_blank"">IBM 704</a>, a scientific computer built with vacuum tubes and core memory that was designed primarily for floating point operations. In 1953, based on his work with Speedcoding, Backus proposed the creation of a new language that would make it easier to program the 704.</p>
<p>IBM management accepted Backus’ proposal, and he eventually assembled a ten-person team that worked out of the IBM World Headquarters in Manhattan. About a year later, his team had enough confidence in their work to release the <em>Preliminary Report, Specifications for the IBM Mathematical FORmula TRANslating System, FORTRAN</em> (<a href=""http://archive.computerhistory.org/resources/text/Fortran/102679231.05.01.acc.pdf"" target=""_blank"">available here</a>).</p>
<p>The FORTRAN project took about two years from conception to first release, and the program consisted of over 25,000 lines of machine language. Eventually every IBM 704 sold included the FORTRAN program and its accompanying manual. For several more years, Backus and his team continued to refine the FORTRAN program—which we would today call a compiler—until it finally reached a reasonable degree of stability and correctness. FORTRAN gained considerable traction in the scientific community, and became the dominant programming language for scientific applications for many decades.</p>
<p>Efforts were soon underway to develop other high-level programming languages that attended to the needs of writing algorithms more clearly. Backus joined an international committee to design the ALGOrithmic programming Language, <a href=""https://en.wikipedia.org/wiki/ALGOL_58"" target=""_blank"">ALGOL 58</a>, and its successor, <a href=""https://en.wikipedia.org/wiki/ALGOL_60"" target=""_blank"">ALGOL 60</a>. The ALGOL language descriptions used a context-free grammar to formally describe its syntax, and Backus collaborated with <a href=""/award_winners/naur_1024454.cfm"">Peter Naur</a>, in developing the <a href=""https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_Form"" target=""_blank"">Backus-Naur Form</a> (BNF) notation that was used. BNF represented a significant milestone in the formalization of programming languages.</p>
<p>In 1963, Backus was made an IBM Fellow, the year the Fellow program was first established by Thomas Watson, Jr. In this position Backus was given considerable freedom to pursue whatever projects he desired, which included teaching assignments at the University of California in Santa Cruz and Berkeley.</p>
<p>Backus continued his work in programming languages in relative isolation. He eventually made contributions to functional programming with the creation of a new language, <a href=""https://en.wikipedia.org/wiki/FP_%28programming_language%29"" target=""_blank"">FP</a> (<u>F</u>unctional <u>P</u>rogramming). While the theoretical foundations of functional programming had been established earlier with the <a href=""https://en.wikipedia.org/wiki/Lambda_calculus"" target=""_blank"">lambda calculus</a> as manifested in <a href=""/award_winners/mccarthy_0239596.cfm"">John McCarthy’s</a> <a href=""https://en.wikipedia.org/wiki/Lisp_%28programming_language%29"" target=""_blank"">Lisp</a>, Backus’ work made functional programming more accessible and so launched a renaissance in research on the topic. Backus retired from IBM in 1991.</p>
<p>John Backus was married twice, first to Marjorie Jamison, whom he divorced in 1966, then to Barbara Una Stanard in 1968. Backus had two children, Karen and Paula. Barbara died in 2004, whereupon Backus moved to Ashland, Oregon, to live near Paula. Backus died on March 17, 2007 in Ashland.</p>
<p>Addition information can be found from the following links:</p>
<p><a href=""http://www.computerhistory.org/fellowawards/hall/bios/John,Backus/"">Computer History Museum</a><br>
<a href=""http://www.columbia.edu/cu/computinghistory/backus.html"" target=""_blank"">Columbia University</a><br>
<a href=""https://www.ibm.com/ibm/history/exhibits/builders/builders_backus.html"" target=""_blank"">IBM</a><br>
<a href=""http://www.ieeeghn.org/wiki/index.php/John_Backus"" target=""_blank"">IEEE History Center</a><br>
<a href=""http://archive.computerhistory.org/resources/text/Oral_History/Backus_John/Backus_John_1.oral_history.2006.102657970.pdf"" target=""_blank"">Oral history</a></p>
<p align=""right""><span class=""callout"">Author: Grady Booch</span></p>
<hr align=""left"" size=""1"" width=""33%"">
<div id=""ftn1"">
<p><a href=""#_ftnref1"" name=""_ftn1"" title="""">[1]</a> <a href=""http://www.computerhistory.org/collections/accession/102657970"" target=""_blank"">Computer History Museum oral history</a></p>
</div>
<p>&nbsp;</p>","<div class=""featured-photo"">
<a href=""/award_winners/backus_0703524.cfm""><img src=""/images/lg_aw/0703524.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""John Backus ""></a>
<br><br>
<h6 class=""label""><a href=""/photo/backus_0703524.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>December 3, 1924, Philadelphia, Pennsylvania, United States</p>
<h6><span class=""label"">DEATH: </span></h6>
<p>March 17, 2007, Ashland, Oregon, United States</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>BS mathematics (Columbia University, 1949); AM mathematics (Columbia University,1950); Honorary degree (Université Henre Poincaré, 1989).</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>US Army (antiaircraft battery crew; engineering and premed school, 1942-1946); IBM (programmer of Pure &amp; Advanced Science Departments, 1950–1954; manager of Programming Research Department, 1954–1959; research staff, 1959–1963; IBM Fellow, 1963–1991); University of California, Santa Cruz (adjunct professor of information sciences, 1974); University of California, Berkeley (visiting professor, 1980; visiting professor, 1985).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>IBM Fellow (1963); IEEE Computer Society W. W. McDowell Award (1967); National Medal of Science (1975); ACM Turing Award (1977); National Academy of Engineering Draper Prize (1993); Computer History Museum Fellow (1997);&nbsp;<span style=""line-height: normal;"">Asteroid&nbsp;6830 </span>Johnbackus<span style=""line-height: normal;"">&nbsp;named in his honor (June 1, 2007).</span></p>","","https://dl.acm.org/author_page.cfm?id=81100233661","John Backus","<li class=""bibliography""><a href=""/bib/backus_0703524.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283933&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/backus_0703524.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/backus_0703524.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/backus_0703524.cfm""><span></span>Video Interview</a></li>"
"1573179135-664","https://amturing.acm.org/award_winners/stonebraker_1172121.cfm","For fundamental contributions to the concepts and practices underlying modern database systems.","<p>Michael Stonebraker’s contributions to the refinement and spread of database management technology are hard to overstate. He began work in this area as a young assistant professor at the University of California—Berkeley. After reading Edgar F. Codd’s seminal papers on the relational model, Stonebraker started work with a colleague, Eugene Wong, to develop an efficient and practical implementation. The result was INGRES, a name that reflected the project’s original intention to produce a geographically-oriented system with graphical capabilities. This officially stood for “Interactive Graphic and Retrieval System” but echoed the name of a <a href=""https://en.wikipedia.org/wiki/Jean-Auguste-Dominique_Ingres"">celebrated French painter</a>.</p>
<p>A prototype of INGRES was working by 1974, but the project did not stop there. Over the next decade INGRES, and systems inspired by it, built a new commercial market of relational database systems. Today the relational database management system is one of computing’s most important and widely used technologies, having replaced filing cabinets as the standard way of storing and retrieving information.</p>
<p><strong>Development of INGRES</strong></p>
<p>Stonebraker led development of INGRES at Berkeley until 1985, supported by grant money and the labor of graduate and undergraduate students. Berkeley was particularly notable during this era as a place where theoretical research and system building came together with spectacular results. Further examples included the work on timesharing systems by <a href=""https://amturing.acm.org/award_winners/lampson_1142421.cfm"">Butler Lampson </a>(winner 1992) and others and the <a href=""https://en.wikipedia.org/wiki/Berkeley_Software_Distribution"">Berkeley Software Distribution (BSD)</a> of the Unix operating system, which gave rise to a commonly used form of open source licensing. These cultures and practices anticipated much of what we now associate with the open source software movement. Stonebraker remembers that “we would recruit the smartest freshmen and sophomores we could find, give them wonderful equipment, and they would basically die writing code for us.”</p>
<p>Stonebraker’s work built on, and complemented, that of three other Turing award winners. Academic research into database management technology has had an unusually direct connection to the widely used industrial-strength systems underlying the websites, business applications, scientific breakthroughs, social media systems, and “big data” projects of the modern world. <a href=""/award_winners/bachman_9385610.cfm"">Charles W. Bachman </a>(winner 1973) designed what is often called the first database management system in the early 1960s, and helped to define and popularize the concept of a database management system through his later work with the industry group <a href=""https://en.wikipedia.org/wiki/CODASYL"">CODASYL</a>. <a href=""/award_winners/codd_1000892.cfm"">Edgar F. Codd</a> (winner 1981) developed an elegant and flexible way of storing and retrieving data, the relational model, which gradually eclipsed the network data model over the course of the 1980s. <a href=""https://amturing.acm.org/award_winners/gray_3649936.cfm"">James Nicholas Gray</a> (winner 1988) contributed to IBM’s <a href=""https://en.wikipedia.org/wiki/IBM_System_R"">System R</a>, an influential experimental implementation of the relational model, and later pioneered robust, high performance methods for record locking and transaction processing.</p>
<p><strong>Legacy of INGRES</strong></p>
<p>INGRES and System R together helped to turn relational systems from a laboratory curiosity into the default choice for even the most demanding data processing applications. While the IBM prototype targeted the company’s multi-million dollar mainframes, INGRES was a Unix application suitable for relatively affordable minicomputers and was widely distributed to other universities where people used it, experimented with it, and extensively modified it.</p>
<p>INGRES brought a new kind of database technology to a new kind of computer. Database management systems were widely adopted by businesses from the early 1970s onwards as central hubs which managed the data used by many different application programs. These early commercial systems ran on mainframes and followed either Bachman’s network model or a more restrictive hierarchical approach favored by IBM. In the mainframe world these approaches remained dominant throughout the 1980s so that, for example, IBM first commercialized its work in the area as a niche product for “decision support” analytical applications rather than workaday operational systems.</p>
<p>During the 1970s, minicomputers became a cost-effective alternative to mainframes for an ever widening range of applications. Thanks to INGRES and its derivatives, relational technology became the default choice for minicomputer databases, as the new technology was widely applied to transaction processing applications (keeping routine records of things like address changes or account updates) as well as analytical work. The commercial database systems of the 1970s required their users to navigate through data structures at a relatively low level, making explicit decisions about how to index and link records when the database was <em>created</em> and navigating record by record through these structures when retrieving information. Relational database systems shifted to a more abstract and flexible view of data. Only when <em>querying</em> the database did users specify how data from different tables should be combined. This shifted much of the responsibility for efficiently organizing and retrieving data from the user to the database management software, pushing hard against the limits of affordable hardware.</p>
<p>INGRES was a feat of virtuoso software engineering, prioritizing performance and reliability so that new features were added only once a way of implementing them efficiently had been discovered. By 1976 INGRES was rapidly executing queries written in its <a href=""https://en.wikipedia.org/wiki/QUEL_query_languages"">QUEL query language</a> (roughly equivalent to the SEQUEL, later SQL, language introduced by IBM). It could be embedded in C programs or used interactively. Under the hood, INGRES implemented a variety of indexing and compression methods, automatically optimizing queries. The team had already begun to add support for transactions, so that related updates would occur together--or not at all--to enforce integrity constraints between related records in different tables, and to deal with the potential problems caused by simultaneous updates from different users. Additional features, such as crash recovery and efficient backup and restore capabilities, turned INGRES from a research project to an industrial-strength technology. This took a huge amount of additional work. As Stonebraker recalled, “We built an initial prototype, putting in the first 90% of the effort required to create a real system, and it more or less worked. I think that the thing that distinguished INGRES from the typical academic project, and in retrospect one of the smartest things we ever did, was to then put in the next 90% of the effort to make INGRES really work.”</p>
<p>Students trained on the INGRES project, and in many cases using the INGRES code itself as a starting point, produced most of the leading minicomputer database packages. These included Britton-Lee (an early supplier of specialized parallel processing database management systems), the NonStop SQL product offered by Tandem Computers, and Sybase (whose SQL Server was later licensed by Microsoft). In 1980 Stonebraker himself co-founded Relational Technology, Inc. to produce its own commercial version of INGRES. His involvement with the firm was primarily as a consultant, though he worked there full time for around six months. It was a significant player in the database software market over the next decade, making an initial public offering in 1988 before being acquired in 1990.</p>
<p><strong>Postgres</strong></p>
<p>By this point Stonebraker was already immersed in the development and commercialization of a successor system. <a href=""https://en.wikipedia.org/wiki/PostgreSQL"">Postgres </a>added many features missing from existing relational systems, including support for rules to maintain consistent relationships between tables, support for complex “object-relational” data types, the replication of data across servers, and procedural languages to embed code fragments within the database management system to be triggered when specified conditions occured. Postgres was also used to experiment with other features of interest to database researchers. Techniques pioneered in Postgres were widely implemented, and in 1992 Stonebraker cofounded Illustra Information Technologies to market a commercial version. It was acquired in 1997 by <a href=""https://en.wikipedia.org/wiki/Informix_Corporation"">Informix</a>, which rebuilt its product line around the code.</p>
<p><strong>Entrepreneurial Career</strong></p>
<p>Stonebraker retired from Berkeley in 1994, retaining a connection as a “Professor of the Graduate School.” In 1999 he moving to New Hampshire, soon taking up an adjunct appointment at MIT where he could focus on developing and commercializing new technologies without the obligation of regular faculty responsibilities. Since then he has cofounded a company every few years, focusing on the development of database management technologies specialized for particular areas such as data warehousing (Vertica), managing data streams captured by sensors (StreamBase Systems), and high-throughput transaction processing (VoltDB). However one of his latest ventures, SciDB, which focuses on handling massive arrays of scientific data, departs from the relational model as well as from traditional general purpose implementation techniques.</p>
<p>As an eloquent and authoritative commentator on trends in database technology, Stonebraker has <a href=""https://cacm.acm.org/blogs/blog-cacm/50678-the-nosql-discussion-has-nothing-to-do-with-sql/fulltext"">defended the enduring power of the relational model</a> against efforts by the “NoSQL” movement to promote the superiority of “post-relational” approaches. At the same time, he <a href=""https://dl.acm.org/citation.cfm?id=1054024"">has been critical</a> of the assumption that “one size fits all” when implementing relational database management systems and that dominant general purpose systems, such as Oracle, can serve the needs of all users.</p>
<p>Stonebraker is the only Turing award winner to have engaged in serial entrepreneurship on anything like this scale, giving him a distinctive perspective on the academic world. The connection of theory to practice has often been controversial in database research, despite the foundational contribution of mathematical logic to modern database management systems. Stonebraker has been critical of the insularity of some researchers, noting the attention given to such ideas as recursive querying or object-oriented databases suggests that “they are more interested in working on problems that are solvable, rather than problems that are important.” His “advice to theoreticians” was “go spend some time in the real world and work on problems that people want solved.” In contrast, “Knowing what I know now, I would never have started building INGRES, because it’s too hard…. So I think my advice to my younger self would be to suspend your disbelief and just do it anyway. The way you climb Mt. Everest is one step at a time…”</p>
<p>(Quotations from Stonebraker are taken from <a href=""http://www.sigmod.org/publications/interview/pdf/D1-DBP-stonebraker-final.pdf"">his interview with Marianne Winslett</a>, published in ACM SIGMOD Record, Vol.32, No. 2, June 2003 as ""Michael Stonebraker Speaks Out."")</p>
<p style=""text-align: right;""><em>Author: Thomas Haigh</em></p>","<div class=""featured-photo"">
<a href=""/award_winners/stonebraker_1172121.cfm""><img src=""/images/lg_aw/1172121.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Michael Stonebraker""></a>
</div>
<h6><a href=""/stonebraker_1172121.pdf"" target=""_blank"" title=""PDF format"">BIRTH:</a></h6>
<p>October 11, 1943 in Newburyport, Mass.</p>
<h6><a href=""/stonebraker_1172121.pdf"" target=""_blank"" title=""PDF format"">EDUCATION:</a></h6>
<p>Bachelor’s degree in Electrical Engineering (Princeton University, 1965); M.Sc. in Electrical Engineering (University of Michigan, Ann Arbor, 1967); Ph.D. in Computer Science &amp; Engineering (University of Michigan, Ann Arbor, 1971).</p>
<h6>EXPERIENCE</h6>
<p>Assistant Professor of Computer Science (University of California at Berkeley, 1971--1976), Associate Professor (1976-1982), Professor (1982-1993), Professor of the Graduate School (1994-1999); Senior Lecturer (Massachusetts Institute of Technology, 2001-2) Adjunct Professor (2002-Present). Concurrently co-founded and held executive or advisory roles with companies including Relational Technology, Inc. (founded 1980, later Ingres Corporation), Illustra Corporation (founded 1992, later acquired by Informix where Stonebraker was Chief Technology Officer 1996-2000), Cohera Corporation (1997, acquired by PeopleSoft), StreamBase Systems (2003, acquired by Tibco in 2013), Vertica Systems (2005, acquired by HP), Goby (2008, acquired by Telenauv in 2012), SciDB (2008), VoltDB (2009), and Tamr (2013).</p>
<h6>HONORS AND AWARDS:</h6>
<p>ACM System Software Award (1992); ACM SIGMOD Innovation Award (1994); National Academy of Engineering (elected 1998); IEEE John von Neumann Medal (2005); Alan M. Turing Award (2014).</p>","","https://dl.acm.org/author_page.cfm?id=81337493529","Michael Stonebraker","<li class=""bibliography""><a href=""/bib/stonebraker_1172121.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/stonebraker_1172121.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/stonebraker_1172121.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/stonebraker_1172121.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/stonebraker_1172121.cfm""><span></span>Video Interview</a></li>"
"1573179104-662","https://amturing.acm.org/award_winners/naur_1024454.cfm","For fundamental contributions to programming language design and the definition of Algol 60, to compiler design, and to the art and practice of computer programming.","<p><strong>Peter Naur was born in 1928 in Frederiksberg Denmark as the last of three children in an anti-religious family.</strong> His father, Albert, was a painter. His mother, Susanna Margarethe, had no particular profession but came from a wealthy commercial background. Both artistic and business-oriented people, such as actors, playwrights, and architects, visited the Naur home on a regular basis. Even though Peter’s parents divorced during the mid-1930s, which was quite unusual at the time, the divorce did not stop Peter from having a rich childhood.</p>
<p>Astronomy became Peter’s main passion. By the age of 12, he had already shown strong interest in his father’s collection of scientific books, including the works of the famous English astronomers <a href=""https://en.wikipedia.org/wiki/James_Jeans"" target=""_blank"">James Jeans</a> and <a href=""https://en.wikipedia.org/wiki/Arthur_Eddington"" target=""_blank"">Arthur Eddington</a>. The absence of city lights during the German occupation of Denmark allowed Peter to observe the stars while sitting on his mother’s balcony. Even as a teenager Peter sought regular contact with professional astronomers at the Copenhagen Observatory. A couple of days each week he would ride his bike to the observatory after school to be taught by professionals how to calculate the orbits of comets and planets. He learned to use logarithm tables and the mechanical calculating machines of the Observatory, and solved differential equations by numerical techniques that were practically unknown except to astronomers. By the age of 15 Peter had already written his first scientific paper, which was published after the war [<a href=""/info/naur_1024454.cfm#info_ref_10"">10</a>]. He gave talks in school about <a href=""https://en.wikipedia.org/wiki/Tycho_Brahe"" target=""_blank"">Tycho Brahe</a>, and about Danish physicist <a href=""https://en.wikipedia.org/wiki/Niels_Bohr"" target=""_blank"">Niels Bohr</a>, who he actually met at the time. In short, Peter was a prodigious child with a very promising future.</p>
<p>After high school, Peter went to gymnasium (1944-1947). He spent a lot of time in the public library reading widely, including books on psychology, a topic which would become increasingly important to Peter in later years. He studied astronomy at Copenhagen University and finished his degree in two years (1947-1949) instead of the regular five. After one year of military service, and on the recommendation of the great Danish astronomer <a href=""https://en.wikipedia.org/wiki/Bengt_Str%C3%B6mgren"" target=""_blank"">Bengt Strömgren</a>, Peter went to King’s College, Cambridge, to conduct research in both astronomy and the emerging field of computer programming (1950-1951).</p>
<p>The poor weather in Cambridge spoiled Peter’s plans for astronomical observation. Instead, he spent a lot of time programming the Electronic Delay Storage Automatic Calculator (<a href=""https://en.wikipedia.org/wiki/Electronic_Delay_Storage_Automatic_Calculator"" target=""_blank"">EDSAC</a>) in order to solve a perturbation problem in astronomy. The only documentation on how to use the EDSAC was a manual describing a library of subroutines, which was a precursor to the famous book, <em>The Preparation of Programs for an Electronic Digital Computer, </em>by <a href=""/award_winners/wilkes_1001395.cfm"">Maurice Wilkes</a>, <a href=""https://en.wikipedia.org/wiki/David_Wheeler_%28computer_scientist%29"" target=""_blank"">David Wheeler</a> and <a href=""https://en.wikipedia.org/wiki/Stanley_Gill"" target=""_blank"">Stanley Gill</a>.</p>
<p>As a boy, Peter had conducted hundreds of hours of computational work by hand at the Observatory of Copenhagen, so at Cambridge he already knew all the computational techniques. His energy therefore went into programming the EDSAC and, in particular, into dealing with the finite limitations of the machine, such as its limited number range. The reward for his efforts was that the machine was able to calculate in 20 seconds what would manually take two hours.</p>
<p>Peter’s stay at Cambridge was important for his later career in at least two ways. First, it provided an excellent tutorial on how to program and eliminate programming mistakes. Second, it was the place where he became acquainted with his life-long friend Peter Remnant, who introduced Naur to the works of several philosophers and psychologists [<a href=""/info/naur_1024454.cfm#info_ref_6"">6</a>] and, most notably, to William James’s <em>The Principles of Psychology</em> [<a href=""/info/naur_1024454.cfm#info_ref_8"">8</a>]. It would, however, take 20 more years before Peter would really start to study James’s work.</p>
<p>After his stay in Cambridge, Peter continued his astronomical research in the USA (1952-1953). There he met computing pioneers <a href=""https://en.wikipedia.org/wiki/Howard_H._Aiken"" target=""_blank"">Howard Aiken</a> at Harvard University and <a href=""https://en.wikipedia.org/wiki/John_von_Neumann"" target=""_blank"">John von Neumann</a> at Princeton, and learned the state of the art in computing [<a href=""/info/naur_1024454.cfm#info_ref_11"">11</a>]. After spending one more month in Cambridge in the summer of 1953, Peter returned to Denmark and married in 1954. He had three children before being divorced around 1969.</p>
<p>Even though Peter had become an astronomer of international standing, in the late 1950s he decided to leave that field behind for the non-academic occupation of computer programming. After joining Copenhagen’s computing center, <a href=""https://en.wikipedia.org/wiki/Regnecentralen"" target=""_blank"">Regnecentralen</a>, his new boss, Niels Ivar Bech, asked Peter to participate in the development of the programming language that would later be called <a href=""https://en.wikipedia.org/wiki/ALGOL_60"" target=""_blank"">ALGOL</a> (ALGorithmic Language). He first investigated the work of the Swiss-German ALCOR (ALgol COnverteR) group, which included <a href=""https://en.wikipedia.org/wiki/Heinz_Rutishauser"" target=""_blank"">Heinz Rutishauser</a> in Zurich and <a href=""https://en.wikipedia.org/wiki/Friedrich_L._Bauer"" target=""_blank"">Friedrich Bauer</a> and <a href=""https://en.wikipedia.org/wiki/Klaus_Samelson"" target=""_blank"">Klaus Samelson</a> in Munich. These three researchers were largely responsible for the 1958 <em>Zurich Report</em> which described the definition of the <a href=""https://en.wikipedia.org/wiki/ALGOL_58"" target=""_blank"">International Algebraic Language</a> (IAL), a precursor to ALGOL.</p>
<p>In the interest of building a <a href=""/info/naur_1024454.cfm#info_def_1"">compiler</a> for IAL, Peter and his colleague <a href=""https://en.wikipedia.org/wiki/J%C3%B8rn_Jensen"" target=""_blank"">Jørn Jensen</a> studied the Zurich Report in great detail. They found many ideas expressed in the report to be unclear, so Peter, Jensen, and Bech quickly organized a conference in Copenhagen in February 1959 at which 20 to 30 people from all over Europe discussed the <em>Zurich Report</em>. One of the main difficulties for the Danes lay in understanding the <em>Zurich Report’s</em> intended parameter passing mechanism for procedures. More specifically, the difficulty was that the ALCOR researchers viewed a procedure as a static mathematical object (i.e., as a closed function), while Peter and Jensen viewed it as a dynamic object and, hence, as something that typically has <a href=""/info/naur_1024454.cfm#info_def_3"">side effects</a>. Understanding, let alone consolidating, the varying points of view at the conference was far too difficult. Peter realized that oral discussions did not work, and concocted the idea of having all parties put down their views in writing. He initiated a discussion journal, called the <a href=""https://en.wikipedia.org/wiki/ALGOL_Bulletin"" target=""_blank""><em>ALGOL Bulletin</em></a>. By repeatedly sending out and receiving comments, the <em>ALGOL Bulletin</em> quickly became the official medium of communication, and Peter became, rather unintentionally, the leading European behind the ALGOL effort. He would later also become the sole editor of the very influential ALGOL 60 report [<a href=""/info/naur_1024454.cfm#info_ref_3"">3</a>].</p>
<p>In line with his later research, Peter’s main contribution to ALGOL lay in selecting the right forms of description to define the language. Contrary to most of his colleagues who were heavily defending (and quarreling about) specific language features, Peter put his efforts into meta considerations.</p>
<p>Two examples illustrate this. The first concerns the roundoff problems a programmer encounters when facing the finite limitations of his machine. Instead of describing every minute detail of, say, an addition or a multiplication, Peter insisted that all numbers in ALGOL were to be understood as approximations. Peter put ALGOL’s arithmetic operations on a solid basis by casting them in terms of the well-established field of numerical analysis.</p>
<p>The second example is about choosing an appropriate notation to define the syntax of the language. Backus’s now-famous 1959 paper [<a href=""/info/naur_1024454.cfm#info_ref_2"">2</a>] was one of many attempts to solve this vexing problem of the 1950s, and initially did not receive a lot of attention. It was Peter who, several months later, was able to apply it to ALGOL and demonstrate its practical usefulness. Peter modified Backus’s notation in slight but important ways [<a href=""/info/naur_1024454.cfm#info_ref_9"">9</a>] and used it to write a new report which was very different from the soon-obsolete <em>Zurich Report</em>.</p>
<p>The previously mentioned dichotomy between static solutions, as advocated by ALCOR, and dynamic solutions also played an important role in language implementation. Prior to Peter’s arrival, Regnecentralen had followed ALCOR’s static implementation approach with their <a href=""https://en.wikipedia.org/wiki/DASK"" target=""_blank"">DASK</a> machine. Peter played a pivotal role in getting Regnecentralen to switch its focus from compiler design to concentrating first and foremost on the <a href=""/info/naur_1024454.cfm#info_def_2"">runtime system</a> of an ALGOL implementation. This switch was much in line with what <a href=""/award_winners/dijkstra_1053701.cfm"">Edger Dijkstra</a> and <a href=""https://nl.wikipedia.org/wiki/Jaap_Zonneveld"" target=""_blank"">Jaap Zonneveld</a> were already doing in Amsterdam [<a href=""/info/naur_1024454.cfm#info_ref_5"">5</a>,<a href=""/info/naur_1024454.cfm#info_ref_7"">7</a>]. By 1962, the Danes furthered the state of the art: their <a href=""http://datamuseum.dk/site_dk/rc/giersimulator/"" target=""_blank"">GIER</a> computer system stood in sharp contrast to programming tradition in that it treated <em>both</em> data and programs during runtime as dynamically allocated storage. The Danish implementation automatically handled a backing store (drum) in addition to the working core store and, as a result, it could handle ALGOL programs even on the very small BESK machine [<a href=""/info/naur_1024454.cfm#info_ref_12"">12</a>].</p>
<p>During the rest of the 1960s, Peter played an increasingly important role in establishing computing as an academic field in Denmark. In 1966, he defined the courses he was teaching as <em>datalogi</em>; that is, as a science of data [<a href=""/info/naur_1024454.cfm#info_ref_13"">13</a>] and the term has been adopted into both Danish and Swedish to mean “computer science.” Besides teaching the basics of computing, Peter repeatedly stressed the importance of having students work on computer applications in other fields. By 1969, he was appointed professor at the Institute of Datalogi at Copenhagen University. He retired in 1999 at the age of 70.</p>
<p>During the 1960s, Peter played a pioneering role in research on program development. He was co-editor of the proceedings of the famous 1968 NATO Software Engineering conference that raised the spectre of a “software crisis” [<a href=""/info/naur_1024454.cfm#info_ref_14"">14</a>].</p>
<p>In 1970, Peter became a strong opponent of Edgar Dijkstra and <a href=""/award_winners/wirth_1025774.cfm"">Niklaus </a><a href=""/award_winners/wirth_1025774.cfm"">Wirth</a>’s Structured Programming agenda. While Dijkstra and Wirth focused on how programming <em>should </em>ideally be done, Peter conducted empirical investigations in order to find out how programming actually <em>is</em> conducted [<a href=""/info/naur_1024454.cfm#info_ref_15"">15</a>]. In subsequent decades, Peter published several papers in which he scrutinized the work of the formalists. For Peter, the programmer had to be able to choose the form of description most suitable to his needs. Being forced into the straightjacket of one <em>a priori</em> fixed formal notation is counter-productive and even harmful for program development. By the 1980s, and after having penetrated further into the philosophical literature, Peter wrote a paper entitled “Programming as Theory Building” [<a href=""/bib/naur_1024454.cfm#bib_8"">8</a>]. In hindsight, this paper was a starting point for the now-popular Agile Software Development movement (see e.g. Appendix B in Cockburn’s book [<a href=""/info/naur_1024454.cfm#info_ref_4"">4</a>]).</p>
<p>Though Peter does not want to be considered a philosopher, he acknowledges having been influenced by Popper, Quine, Russell, Ryle, and others. Peter’s writings of the 1970s and 1980s show how he borrowed concepts from philosophy to further his understanding of software engineering. In later years, he scrutinized work in philosophy and mathematical logic and rules. After studying the 1890 research of William James, Peter gradually developed his own theory of how mental processing works at the neural level of the nervous system. His 2006 Turing Award lecture [<a href=""/info/naur_1024454.cfm#info_ref_16"">16</a>] gives a glimpse of his work in this area.</p>
<p>During the 1990s and 2000s, after having studied William James’s work on psychology for decades, Peter wrote several books in which he scrutinizes various assumptions underlying western philosophy that many&nbsp; researchers in computing today take for granted.</p>
<p><strong>Acknowledgment</strong>: This profile borrows much from two interviews with Peter Naur [<a href=""/info/naur_1024454.cfm#info_ref_1"">1</a>,<a href=""/info/naur_1024454.cfm#info_ref_6"">6</a>].</p>
<p style=""text-align: right;""><span class=""callout"">Author: Edgar G. Daylight</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/naur_1024454.cfm""><img src=""/images/lg_aw/1024454.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Peter Naur""></a>
<br><br>
<h6 class=""label""><a href=""/photo/naur_1024454.cfm"" target=""_blank"">PHOTOGRAPHS</a></h6>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>25 October 1928, Frederiksberg, Denmark</p>
<h6><span class=""label"">DEATH: </span></h6>
<p>3 January 2016, Herlev, Denmark</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>Gymnasium in Copenhagen (1947); mag. scient. degree, Astronomy (Copenhagen University, 1949); PhD astronomy (Copenhagen University, 1957).</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Research Student at King’s College, Cambridge (1950–1951); researcher in Astronomy (U.S.A. 1952–1953); scientific assistant at Copenhagen Observatory (1953–1959); programmer at Regnecentralen (Copenhagen) and lecturer at Copenhagen University (1959–1969); Professor at Institute of Datalogi, Copenhagen University (1969–1999).</p>
<h6><span class=""label"">HONORS AND AWARDS: </span></h6>
<p>G. A. Hagemann Medal (1963); Jens Rosenkjaer Prize (1966); Computer Pioneer Award of the IEEE Computer Society (1986); ACM Alan M. Turing Award (2005).</p>","","https://dl.acm.org/author_page.cfm?id=81100158784","Peter Naur","<li class=""bibliography""><a href=""/bib/naur_1024454.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/naur_1024454.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/naur_1024454.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/naur_1024454.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179059-658","https://amturing.acm.org/award_winners/hinton_4791679.cfm","For conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.","","<div class=""featured-photo"">
<a href=""/award_winners/hinton_4791679.cfm""><img src=""/images/lg_aw/4791679.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Geoffrey E Hinton""></a>
</div>","","https://dl.acm.org/author_page.cfm?id=81100505762","Geoffrey E Hinton","<li class=""award-video""><a href=""/vp/hinton_4791679.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/hinton_4791679.cfm""><span></span>Research<br> Subjects</a></li>"
"1573179262-673","https://amturing.acm.org/award_winners/gray_3649936.cfm","For seminal contributions to database and transaction processing research and technical leadership in system implementation.","<div class=""bibliography"">
<div class=""callout2""><span>.</span><em>As a guide for those interested in learning more about Gray and his work, links have been provided below to relevant articles from the <a href=""http://www.sigmod.org/publications/sigmod-record/0806/index.html"" target=""_blank"">proceedings</a> of the Tribute held for him on May 31, 2008 at the University of California, Berkeley.</em></div>
<p><strong>James Nicholas Gray was born in San Francisco, California on 12 January 1944. </strong>He was raised by his mother, an English teacher, who encouraged her two children to read and make frequent visits to the aquarium or planetarium or to a museum. In 1961 Gray graduated from Westmoor High School in San Francisco.</p>
<p>Gray spent most of the next decade across the bay, at the University of California, Berkeley. His initial plan was to major in physics. Two stints in a co-op program at an aerospace company gave him a greater appreciation for the academic environment. He took graduate courses and carried out research work, graduating in 1966 with a bachelor’s degree in mathematics and engineering. After spending a year in New Jersey working at Bell Laboratories in Murray Hill and attending classes at the Courant Institute in New York City, he returned to Berkeley and enrolled in the newly-formed computer science department, earning a Ph.D. in 1969 for work on context-free grammars and formal language theory. He spent the next two years as a postdoctoral fellow, sponsored by IBM Corporation. During this time he served as the director of the CAL Timesharing System research project, a research project to build a secure and reliable operating system.</p>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p26.harrison.pdf"" target=""_blank"">“Jim Gray at Berkeley”</a> by Michael Harrison, who was Gray's advisor, discusses Gray's undergraduate, graduate, and postgraduate years.</em></div>
<p>In 1971 Gray became a research staff member of IBM’s Watson Research Center in Yorktown Heights, NY. Inspired by work he’d done during his postdoctoral fellowship on urban modeling, he joined the General Science Department and worked on land-use mapping. During this period he met <a href=""/award_winners/cocke_2083115.cfm"">John Cocke</a>, who suggested the challenge of scalable computing: finding a way to interconnect computers so that spending twice as much for computer hardware would allow a large computing problem to be solved twice as fast.</p>
<p>After a winter in New York, Gray decided he wanted to move back to California. But first he spent the summer of 1972 as a UNESCO Expert at the Polytechnic Institute of Bucharest in Romania. Back in California, he accepted an offer from IBM’s San Jose Research Laboratory (now IBM Almaden). The San Jose Research Laboratory was collocated with IBM’s General Products Division, which designed and manufactured computer disk drives. Database management was a research focus, and one of the lab members, <a href=""/award_winners/codd_1000892.cfm"">Edgar F. Codd</a>, had published an influential paper proposing a new way to organize database systems, called the relational model.</p>
<p>Several projects at IBM and elsewhere were begun with the goal of building practical systems based on Codd’s relational model. In 1973, IBM research management decided to combine people from the Watson and San Jose Laboratories into a single project located in San Jose. Gray soon joined this project, which became known as System R. The project continued for five years and — together with the Ingres project at the University of California, Berkeley — served as the foundation for the relational database industry. Ray Boyce and Don Chamberlin designed the widely-used SQL query language for System R.</p>
<p>Gray played a major role in System R, combining his experience with systems and theory to create a unified approach to the interrelated problems of concurrency control and crash recovery. He defined the <a href=""/info/gray_3649936.cfm#add_1""><em>transaction</em></a> as a unit of work, such as moving money from one bank account to another, that must leave the bank’s database in a consistent state whether or not the transaction succeeds: either the money moves, or it stays in the original account. Gray developed techniques that allowed concurrent execution of many transactions, as well as restart after crashes, while maintaining the consistency of the database. He proved the correctness of the approach. This work was the foundation for his Turing Award. As the research component of System R wound down, Gray helped transfer the technology to IBM product groups and began thinking about how to extend transactions to a distributed network of communicating computers.</p>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p33.stonebraker.pdf"" target=""_blank"">“Why Did Jim Gray Win the Turing Award?”</a> by Michael Stonebraker introduces the properties of transactions and explains the importance of Gray’s work.</em></div>
</div>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p38.lindsay.pdf"" target=""_blank"">“Jim Gray at IBM”</a> by Bruce Lindsay provides more detail on transaction properties and the implementation techniques pioneered by Gray.</em></div>
<p>In 1980, Gray made a career change, moving to Tandem Computers, where he spent the next decade. Tandem had pioneered the use of fault-tolerant hardware and software in commercial systems. Its approach of interconnecting isolated computers with a high-speed network promised the scalability that John Cocke had proposed a decade earlier. The scope of Gray’s work at Tandem moved beyond the research-to-production transfer he’d done at IBM, to extensive involvement in product development and activities involving Tandem customers and the&nbsp;entire field of&nbsp;data processing. An important example of his product development activity was the leadership role he played in the NonStop SQL relational database management system, which was tightly integrated with Tandem’s operating system and communication software, and featured fault tolerance, high availability, and scalable performance. Gray was involved from the initial conception: obtaining management approval, recruiting engineers, leading the architecture design, and participating in coding and tuning.</p>
<p>Gray believed that the relational database model and the SQL data access language were sound foundations for online applications, but he was concerned about the difficulty customers had comparing the offerings from various hardware and software vendors. He designed end user-oriented performance benchmarks, and helped establish a vendor-neutral organization, the Transaction Processing Performance Council, to oversee their impartial implementation. This led to more than a decade of strong competition between vendors to improve their products.</p>
<p>Gray’s interest in fault tolerance led him to work with Tandem customers to study the cause of system failures. He published one of the first papers with statistics from production fault tolerant systems, demonstrating that the most common sources of system failure were system administration and software bugs. As customer need for geographical distribution of computer systems and terminal networks increased, Gray studied how such distribution interacted with availability, consistency, and other desirable properties of the overall system. The many technical reports and papers he published while at Tandem helped customers plan their applications, helped Tandem engineers plan enhancements to their products, and contributed to the open literature on a wide variety of topics related to performance, reliability, availability, and ease of use.</p>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p41.nauman.pdf"" target=""_blank"">“Jim Gray's Tandem Contributions”</a> by John Nauman and Wendy Bartlett discusses Gray’s work on performance benchmarking, NonStop SQL, reliability, and scalability.</em></div>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p45.dewitt.pdf"" target=""_blank"">“Not just correct, but correct and fast”</a> by David J. DeWitt and Charles Levine provides more depth on Gray’s benchmarking work and its impact.</em></div>
<p>After a decade at Tandem, Gray moved to Digital Equipment Corporation in 1990, where he started a small laboratory in San Francisco. Over the next four years he consulted with product groups for the Rdb relational database management system and the ACMS transaction-processing monitor. In addition, he and his co-author Andreas Reuter completed the book <em>Transaction Processing: Concepts and Techniques</em>. They had begun the work in 1986 as preparation for a one-week seminar; it evolved into a 1000-page book published in 1992. Building on their combined experience designing and teaching algorithms and systems, the authors presented an integrated view of the overall architecture as well as many details faced by the implementers of transaction processing systems. Completing the book marked a turning point for Gray, ending his focus on transaction processing systems.</p>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p54.reuter.pdf"" target=""_blank"">“Is There Life Outside Transactions? Writing the Transaction Processing Book”</a> by Andreas Reuter explains how the book came to be.</em></div>
<p>In 1994 Gray resigned from Digital and accepted a Mackay Fellowship at the University of California, Berkeley. He participated in the Sequoia 2000 project, which was designing a Geographic Information System to support global change research.</p>
<p>During 1994-1995, Gray and Gordon Bell proposed to Microsoft that they establish an advanced development laboratory in San Francisco dedicated to servers and scalability. Microsoft agreed, and a small staff was hired. Over the next dozen years, Gray set the goal for himself “to put all the world’s scientific data online, along with tools to analyze the data.” He worked with colleagues at Microsoft and several universities to build a series of systems that applied the growing power of commodity hardware and software to a series of applications allowing access, search, and computation on large-scale scientific data. TerraServer allowed access to newly-available satellite imagery with resolution of 1.5 meters/pixel. SkyServer, a collaboration with Alexander Szalay and his colleagues at Johns Hopkins, allowed access to astronomical data from the Sloan Digital Sky Survey. SkyServer led to additional work with astronomical data, and Gray also worked with others to show how to apply the approach to other fields such as molecular biology, sensor networks for environmental science, and oceanography.</p>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p50.bell.pdf"" target=""_blank"">“Scalability and Immortality”</a> by Gordon Bell discusses how he and Gray came to establish a laboratory at Microsoft, and provides an overview of Gray’s work with Bell and others.</em></div>
<div class=""callout2""><span>.</span><em>Additional articles provide more detail about Gray’s work with large-scale scientific data: <a href=""http://www.sigmod.org/publications/sigmod-record/0806/p59.barclay.pdf"" target=""_blank"">“TerraServer and the Russian Adventure”</a> by Tom Barclay, <a href=""http://www.sigmod.org/publications/sigmod-record/0806/p61.szalay.pdf"" target=""_blank"">“The Sloan Digital Sky Survey and Beyond”</a> by Alexander S. Szalay, <a href=""http://www.sigmod.org/publications/sigmod-record/0806/p67.wong.pdf"" target=""_blank"">“Building the WorldWide Telescope”</a> by Curtis Wong, and <a href=""http://www.sigmod.org/publications/sigmod-record/0806/p78.bellingham.pdf"" target=""_blank"">“Exploring Ocean Data”</a> by James G. Bellingham and Mike Godin.</em></div>
<p>Gray had a lifelong interest in teaching and mentoring others. He taught formal and informal courses at Stanford University, gave lectures at universities around the world, and served on a wide range of program committees, editorial boards, and advisory boards. Taken together, his research, system building, mentoring, writing, teaching, and speaking had a large positive impact on almost everyone involved commercially or academically in the field of online transaction processing. Our modern society depends on online transaction processing for banking, ecommerce, and a host of other applications.</p>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p28.helland.pdf"" target=""_blank"">“Knowledge and Wisdom”</a> by Pat Helland, <a href=""http://www.sigmod.org/publications/sigmod-record/0806/p30.lazowska.pdf"" target=""_blank"">“500 Special Relationships: Jim as a Mentor to Faculty and Students”</a> by Ed Lazowska, <a href=""http://www.sigmod.org/publications/sigmod-record/0806/p35.vaskevitch.pdf"" target=""_blank"">“Jim Gray: His Contribution to Industry”</a> by David Vaskevitch, and <a href=""http://www.sigmod.org/publications/sigmod-record/0806/p36.rashid.pdf"" target=""_blank"">“A ‘Gap Bridger’”</a> by Richard Rashid focus on Gray’s impact as a mentor, advisor, communicator, and senior statesman of the scientific world.</em></div>
<p>On January 28, 2007, Gray failed to return from sailing his 40-foot sloop <em>Tenacious</em> around the Farallon Islands. The Coast Guard conducted a comprehensive search but found no signs of the boat. Friends and colleagues of Gray conducted an innovative search using satellite imagery and cloud computing, but were also unsuccessful. A four-month long search of the seabed covering approximately 1000 square kilometers and using a state-of-the-art technology (including multibeam echosounders and remotely operated vehicles) was equally unsuccessful. After the legally-mandated waiting period, a court granted a petition to have him declared dead as of January 28, 2012.</p>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p16.carnes.pdf"" target=""_blank"">“Ode to a Sailor”</a> by Donna Carnes, Gray’s wife, describes the circumstances of his disappearance and the subsequent searches, the impact of ambiguous loss on loved ones, and the organization of the Tribute itself.</em></div>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p21.olson.pdf"" target=""_blank"">“The Amateur Search”</a> by Michael Olson describes the search effort initiated by friends and colleagues.</em></div>
<div class=""callout2""><span>.</span><em><a href=""http://www.sigmod.org/publications/sigmod-record/0806/p70.saade.pdf"" target=""_blank"">“Search Survey for S/V Tenacious”</a> by Ed Saade describes the four-month long seabed search.</em></div>
<div class=""callout2""><span>.</span><em>Additional Tribute articles by <a href=""http://www.sigmod.org/publications/sigmod-record/0806/p19.boss.pdf"" target=""_blank"">Pauline Boss</a> and <a href=""http://www.sigmod.org/publications/sigmod-record/0806/p25.hawthorn.pdf"" target=""_blank"">Paula Hawthorn</a> explain more about ambiguous loss and describe the charitable entities that were established to accept donations to honor Jim Gray.</em></div>
<p>&nbsp;</p>
<p align=""right""><span class=""callout"">Author: Paul McJones<br>
Jim Gray's sister Gail Gordon and his wife Donna Carnes<br>
provided encouragement and suggestions<br>
that improved this article.</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/gray_3649936.cfm""><img src=""/images/lg_aw/3649936.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Jim Gray ""></a>
</div>
<h5><a href=""/photo/gray_3649936.cfm""><img alt="""" src=""/images/misc/bhlight.jpg"" style=""float: left;""></a>&nbsp; <a href=""/photo/gray_3649936.cfm"">Photo-Essay</a><br>
&nbsp;</h5>
<h6 class=""label"">BIRTH:</h6>
<p>12 January 1944, San Francisco, California, USA</p>
<h6 class=""label"">DEATH:</h6>
<p>Disappeared at sea 28 January 2007; declared legally dead as of 28 January 2012.</p>
<h6 class=""label"">EDUCATION:</h6>
<p>Westmoor High School, San Francisco, California; Departments of Mathematics and Engineering, University of California, Berkeley, BS (Mathematics and Engineering, 1966); Courant Institute, New York University (1966); Department of Computer Science, UC Berkeley, PhD (Computer Science, 1969).</p>
<h6 class=""label"">EXPERIENCE:</h6>
<p>Co-op Student, General Dynamics Astronautics, San Diego, CA (1962-1963); Reader, Mathematics Department, University of California, Berkeley (1964); Research Assistant, Electronics Research Laboratory, UC Berkeley (1965–1966); Member of Technical Staff, Bell Telephone Laboratory, Whippany, NJ (1966–1967); Research Assistant, Department of Computer Science, UC Berkeley (1967–1969); IBM Post Doctoral Fellow, Dept. of Computer Science, UC Berkeley (1969–1971); Research Staff Member, General Science Department, IBM Research, Yorktown Heights, NY (1971–1972); UNESCO Expert, Polytechnic Institute of Bucharest, Romania (1972); Research Staff Member, Computer Science Department, IBM Research, San Jose, CA (1972–1980); Software Designer, Tandem Computers, Cupertino, CA (1980–1990); Department of Computer Science, Stanford University (spring 1988); Corporate Consulting Engineer, Digital Equipment Corporation (1990–1994); McKay Fellow, Computer Science Department, University of California, Berkeley (1994–1995); Senior Researcher, Microsoft Research, San Francisco, CA (1995–2000); Distinguished Engineer (2000–2007).</p>
<h6 class=""label"">HONORS AND AWARDS:</h6>
<p>Honorary Doctorate of Natural Science, University of Stuttgart (1990); ACM Turing Award (1998); IEEE Charles Babbage Award (1998); Microsoft’s Jim Gray eScience Award named after him (1998); John Wesley Powell Award (with Tom Barclay for TerraServer) (2000); Honorary Doctorate, University of Paris Dauphine (2004); Member of National Academy of Engineering, National Academy of Science, American Academy of Arts And Sciences, and the European Academy of Science. For a complete list of Gray’s awards and affiliations, see the Vita section of his <a href=""http://research.microsoft.com/en-us/um/people/gray/"">professional web site at Microsoft</a>.</p>","","https://dl.acm.org/author_page.cfm?id=81100403088","James (""Jim"") Nicholas Gray","<li class=""bibliography""><a href=""/bib/gray_3649936.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=2159561&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/gray_3649936.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/gray_3649936.cfm""><span></span>Additional<br> Materials</a></li>"
"1573179075-659","https://amturing.acm.org/award_winners/wirth_1025774.cfm","For developing a sequence of innovative computer languages, EULER,
ALGOL-W, MODULA and PASCAL. PASCAL has become pedagogically significant and has provided a
foundation for future computer language, systems, and architectural research.","<p><strong>Niklaus Wirth grew up in Switzerland, and he spent most of his professional life at the Swiss Federal Institute of Technology (ETH) in Zürich.</strong> After earning his first degree there in 1959, he left for graduate study in North America and earned his Ph.D. in 1963 from the University of California, Berkeley. After a stint on the Stanford faculty, he returned to ETH in 1968. He was a driving force behind the creation in 1981 of what quickly became one of the world’s leading computer science departments, serving as its head for much of the 1980s. Wirth retired in 1999, but remained a frequent presence on its beautiful hilltop campus for many more years.</p>
<p>Wirth began his rise to prominence in computer science by creating two languages: Euler (based on his dissertation work), and the systems programming language PL360 (for the IBM System/360 series of computers). This early work broke new ground in formal separation of syntax and semantics, in novel implementation techniques, and in careful language design for efficient implementation with specific parsing methods.</p>
<p>Both languages were heavily influenced by <a href=""https://en.wikipedia.org/wiki/ALGOL"" target=""_blank"">Algol</a>. Wirth was part of the elite international group centered on the Algol project, which collaborated to define and implement a series of language standards. The first high level languages, such as FORTRAN, were popular because they made it much easier to write and maintain application programs. But they were hard for computer companies to implement because the hardware was so limited, compiler techniques were poorly understood, and the languages themselves were clumsy or overly complex. Algol 60, the most important creation of the Algol group, introduced <a href=""https://en.wikipedia.org/wiki/Recursion_%28computer_science%29"" target=""_blank"">recursive functions</a>, structured code blocks, and local variables. It also pioneered the formal description of programming language syntax.</p>
<table width=""580"">
<tbody>
<tr>
<td width=""80""><em>Wirth discusses his introduction to compiler design as a graduate student.</em></td>
<td width=""500""><iframe frameborder=""0"" height=""281"" scrolling=""no"" src=""https://www.youtube.com/embed/SUgrS_KbSI8?controls=0&amp;showinfo=0&amp;start=1081&amp;end=1211&amp;rel=0"" width=""500""></iframe></td>
</tr>
</tbody>
</table>
<p>Starting in 1957, when academic departments and regular conferences for computer scientists did not yet exist, the Algol project laid a vital foundation for the emerging discipline. Through Algol, Wirth collaborated with other future Turing Award winners including <a href=""/award_winners/hoare_4622167.cfm"">C.A.R. (Tony) Hoare</a>,&nbsp;<a href=""/award_winners/dijkstra_1053701.cfm"">Edsger Dijkstra</a>, and <a href=""/award_winners/naur_1024454.cfm"">Peter Naur</a>. Like Wirth, all had joined the group after showing early promise as designers of compilers and other systems software, production of which was the major practical challenge facing computer scientists during this era.</p>
<p>In 1966 the Algol group voted against a proposal by Wirth to make its next language an extension and improvement of Algol 60 influenced by his own language <a href=""https://en.wikipedia.org/wiki/Euler_%28programming_language%29"" target=""_blank"">EULER</a>. Instead, it chose for the Algol 68 proposal a rival design of great complexity, full of novel and hard-to-implement features. Wirth resigned from the group in 1968. He worked instead with Tony Hoare to turn the rejected proposal into a new unofficial Algol version, <a href=""https://en.wikipedia.org/wiki/Algol-W"" target=""_blank"">Algol-W</a> [<a href=""/bib/wirth_1025774.cfm#bib_1"">1</a>].</p>
<p>Wirth used Algol-W as the basis for what would prove his most influential creation, the language <a href=""https://en.wikipedia.org/wiki/Pascal_%28programming_language%29"" target=""_blank"">Pascal</a>. Following his personal aesthetic, Pascal was simple, flexible and designed for rapid compilation into efficient code. It retained Algol’s code structures, logical completeness, and support for recursion, but stripped away some of its complexity and added support for complex and user-defined data types. Wirth later wrote that the “single most important guideline” was “to include features that were well understood, in particular by implementors, and to leave out those that were still untried and unimplemented.” [<a href=""/bib/wirth_1025774.cfm#bib_10"">10</a>]</p>
<p>Pascal was adopted in 1971 for teaching at ETH, and it spread rapidly to other universities. To help implement Pascal on computers of all kinds, Wirth created a new kind of compiler which was written for, and generated code to run on, a virtual machine. Simulating this virtual machine on a new computer made it easy to port his compiler. Pascal’s great leap into mainstream use came a few years later, with the spread of personal computers. The simplicity and efficiency of Pascal made it a natural fit to their limited memory and disk space. Borland’s cheap and fast <a href=""https://en.wikipedia.org/wiki/Turbo_Pascal"" target=""_blank"">Turbo Pascal</a> compiler cemented Pascal’s position as the leading high level computer language of the 1980s for serious personal computer software development.</p>
<table width=""580"">
<tbody>
<tr>
<td width=""80""><em>Wirth discusses the implementation and spread of Pascal.</em></td>
<td width=""500""><iframe frameborder=""0"" height=""281"" scrolling=""no"" src=""https://www.youtube.com/embed/SUgrS_KbSI8?controls=0&amp;showinfo=0&amp;start=1945&amp;end=2115&amp;rel=0"" width=""500""></iframe></td>
</tr>
</tbody>
</table>
<p>During the 1970s Wirth shared the interest of other veterans of the Algol project, such as Edsger Dijksta and Tony Hoare, in programming methodologies and formal methods. He participated in the IFIP Working Group on Programming Methodology, proposing the idea of stepwise refinement of code as a complement to the various visions of “structured programming” they put forward. His books <em>Systematic Programming</em> [<a href=""/bib/wirth_1025774.cfm#bib_3"">3</a>] and <em>Algorithms + Data Structures = Programs</em> [<a href=""/bib/wirth_1025774.cfm#bib_5"">5</a>] are among the most influential contributions to the literature on programming methods and concepts.</p>
<p>Unlike many of his colleagues, Wirth remained a generalized hands-on system builder. He struggled, with remarkable success, against narrow niches for academic work, and the resulting separation of language design, operating systems, hardware, graphics, and networking into distinct specializations.</p>
<p>During the 1950s and early 1960s, many influential software systems had been created by small academic or industrial groups dealing with practical problems. But by the 1970s and 1980s, the scale of industrial software development increased, and computer science in universities and corporate labs focused more on theory and basic research. It was increasingly unusual for a top computer scientist, particularly within a university, to attempt to create entire production quality systems as Wirth did.</p>
<p>In 1976, influenced by his exposure to the new workstation technology he used while on a sabbatical leave at Xerox’s Palo Alto Research Center (PARC), he led a project at ETH to build a new graphical workstation. Named <a href=""https://en.wikipedia.org/wiki/Lilith_%28computer%29"" target=""_blank"">Lilith</a>, it was a complete computing environment, with an operating system (Medos), high speed local area networking, applications such as text and graphics editors, and laser printers. Its new programming language, <a href=""https://en.wikipedia.org/wiki/Modula_2"" target=""_blank"">Modula- 2</a>, extended Pascal with support for concurrency and greater modularity of code. The first Lilith systems were working by 1980, making these capabilities standard at ETH (and several other universities) years before they were matched by commercial products.</p>
<table width=""”580”"">
<tbody>
<tr>
<td width=""80""><em>Wirth discusses the roots of Lilith in his time at Xerox PARC.</em></td>
<td width=""500""><iframe frameborder=""0"" height=""281"" src=""https://www.youtube.com/embed/SUgrS_KbSI8?controls=0&amp;showinfo=0&amp;start=2341&amp;end=2575&amp;rel=0"" width=""500""></iframe></td>
</tr>
</tbody>
</table>
<p>A few years later he repeated the trick, leading development of the <a href=""https://en.wikipedia.org/wiki/Ceres_%28workstation%29"" target=""_blank"">Ceres workstation</a>, its operating system, and the new object-oriented <a href=""https://en.wikipedia.org/wiki/Oberon_%28programming_language%29"" target=""_blank"">Oberon</a> programming language and operating system. These were used, in several revisions, at ETH from the mid-1980s well into the 1990s. Modula-2 and Oberon were transferred to commercial machines and were used for computer science teaching applications, though neither had the widespread impact of Pascal. Oberon was implemented for quite different machines, achieving the kind of program portability later made famous by <a href=""https://en.wikipedia.org/wiki/Java_%28programming_language%29"" target=""_blank"">Java</a>.</p>
<p>Wirth’s involvement in hardware design deepened during the 1980s. Computer scientists know that hardware and software design are similarly challenging, but most tend to focus on one or the other. Wirth did both. He seized on the new <a href=""https://en.wikipedia.org/wiki/Field-programmable_gate_array"" target=""_blank"">Field Programmable Gate Array</a> (FPGA), a special chip that can be reprogrammed for a particular application, and developed languages and tools to configure them efficiently from a high level specification.</p>
<p>Decades after pioneering computer science groups at MIT and the Universities of Cambridge and Manchester stopped building their own computers and operating systems, Wirth made ETH a place where computer science students and faculty used internally produced hardware, operating systems, and programming tools. He believed that students should read and understand the code of real systems before trying to write their own. He had a life-long drive for simple, elegant, and efficient systems as part of a broader commitment to the integration of theory and practice.</p>
<table width=""580"">
<tbody>
<tr>
<td width=""80""><em>Wirth discusses the importance of abstraction to language design.</em></td>
<td width=""500""><iframe frameborder=""0"" height=""281"" scrolling=""no"" src=""https://www.youtube.com/embed/SUgrS_KbSI8?controls=0&amp;showinfo=0&amp;start=5686&amp;end=5915&amp;rel=0"" width=""500""></iframe></td>
</tr>
</tbody>
</table>
<p>As Wirth put it in his 1984 Turing Award lecture [<a href=""/bib/wirth_1025774.cfm#bib_9"">9</a>], one must “distinguish early between what is essential and what ephemeral” and ensure that “the ephemeral never impinge on the systematic, structured design of the central facilities.” He observed that:</p>
<p style=""margin-left:.5in;"">…every single project was primarily a learning experiment. One learns best by inventing. Only by actually doing a development project can I gain enough familiarity with the intrinsic difficulties and enough confidence that the inherent details can be mastered.</p>
<p>The effectiveness of his systems, and his ability to build complex systems with small teams, relied on his constant search for elegant simplicity—for what could be left out. Over time his language designs and compiler techniques became, in some respects, simpler and more efficient rather than, as is almost universal, slower and more complex. In 1995 he warned that “The plague of software explosion is not a ‘law of nature.’ It is avoidable, and it is the software engineer's task to curtail it.”</p>
<p align=""right""><span class=""callout"">Author: Thomas Haigh</span></p>","<div class=""featured-photo"">
<a href=""/award_winners/wirth_1025774.cfm""><img src=""/images/lg_aw/1025774.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Niklaus E. Wirth""></a>
</div>
<h6><span class=""label"">BIRTH: </span></h6>
<p>Winterthur, Switzerland, February 15 1934.</p>
<h6><span class=""label"">EDUCATION: </span></h6>
<p>Bachelor’s degree in Electronics Engineering (Swiss Federal Institute of Technology Zürich—ETH Zürich, 1959); M.Sc. (Université Laval, Canada, 1960); Ph.D. in Electrical Engineering and Computer Science (EECS) (University of California, Berkeley, 1963).</p>
<h6><span class=""label"">EXPERIENCE: </span></h6>
<p>Assistant Professor of Computer Science (Stanford University, 1963—1967); Assistant Professor (University of Zurich, 1967—1968). Professor of Informatics (ETH Zürich, 1968—1999) (one-year sabbaticals at Xerox PARC 1976–1977 and 1984–1985).</p>
<h6><span class=""label"">HONORS AND AWARDS:</span></h6>
<p>ACM Turing Award (1984); IEEE Computer Society, Computer Pioneer Award (1988); IBM Europe Science and Technology Prize 1988 (1989); Member, Swiss Academy of Engineering (1992); Foreign Associate, US Academy of Engineering (1994); Orden Pour le merite (1996); ACM SIGSOFT, Outstanding Research Award in Software Engineering (1999); Leonardo da Vinci Medal, Societe Europeenne pour la Formation des Ingenieurs (1999); Eduard-Rhein Technology-Prize, München (2002); Fellow of the Computer History Museum (2004). Ten honorary doctorates (&nbsp; University of York, England, 1978;&nbsp; Ecole Polytechnique Federale, Lausanne, Switzerland, 1978; Université Laval, Quebec, Canada, 1987;&nbsp; Johannes Kepler Universitaet Linz, Austria, 1993; University of Novosibirsk, Russia, 1996; The Open University, England, 1997; University of Pretoria, South Africa, 1998; Masaryk University, Brno, Czech Republic, 1999;&nbsp; Saint Petersburg State University of Information Technologies, Mechanics and Optics, Russia, 2005; State University of Ekaterinburg, Russia, 2005).</p>","","https://dl.acm.org/author_page.cfm?id=81332536058","Niklaus E. Wirth","<li class=""bibliography""><a href=""/bib/wirth_1025774.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-lecture""><a href=""https://dl.acm.org/ft_gateway.cfm?id=1283941&amp;type=pdf"" target=""_new""><span></span>ACM Turing Award<br> Lecture</a></li>
<li class=""key-words""><a href=""/keywords/wirth_1025774.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/wirth_1025774.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/wirth_1025774.cfm""><span></span>Video Interview</a></li>"
"1573179534-690","https://amturing.acm.org/award_winners/valiant_2612174.cfm","","<p>Les Valiant has had an extraordinarily productive career in theoretical computer science producing results of great beauty and originality. His research has opened new frontiers and has resulted in a transformation of many areas. His work includes the study of both natural and artificial phenomena. The natural studies encompass the algorithms used by computing objects such as the human brain while the artificial include computers and their capabilities. In the case of computers the limitations of these devices are only beginning to be understood while for natural objects, such as the human brain, the questions of how they operate remain to be answered.</p>
<p>In 2003, in the 50<sup>th</sup> anniversary volume of the <em>Journal of the ACM</em>, he published a paper [<a href=""/bib/valiant_2612174.cfm#bib_1"">1</a>] which advocates this view that computer science describes both natural and artificial phenomena, in terms of three general problem areas for further investigation:</p>
<ol>
<li>Characterizing the power of computation, i.e., to fully characterize what can be computed in practice in the physical world.</li>
<li>Characterizing a semantics for cognitive computation, i.e., to seek out a semantics or description of knowledge that can computationally support the basic phenomena of intelligent behavior.</li>
<li>Characterizing cortical computation, i.e., to describe how knowledge is represented in the brain and to determine the algorithms that are used for computing the most basic behavioral tasks.</li>
</ol>
<p>In 1983 he published an important paper [<a href=""/bib/valiant_2612174.cfm#bib_4"">4</a>] in the area of semantics for cognitive computation. In it he devised a model of learning that offers a quantitative criterion on when a computing device can be considered to be able to learn. This <a href=""https://en.wikipedia.org/wiki/Probably_approximately_correct"" target=""_blank"">probably approximately correct</a> (PAC) model has given rise to a fruitful research area now known as <a href=""https://en.wikipedia.org/wiki/Computational_learning_theory"" target=""_blank"">computational learning theory</a>. The PAC model considers a learning algorithm that takes experience from the past to create a hypothesis that can be used to make a decision in the future with controlled error. The model has been intensively studied and extended by other researchers into an important tool in practical applications.</p>
<p>In 1994 he broadened the PAC concept in his book <em>Circuits of the Mind</em> [<a href=""/bib/valiant_2612174.cfm#bib_5"">5</a>] to investigate how the brain accesses and computes on the large amount of information it needs when reasoning. This investigation uncovered some important quantitative constraints on neural computation, such as the strength of interconnections, and offers a range of questions for experimentalists. The book also provides a model in a computational language and framework that can be used in future investigations into memory, learning and reasoning.</p>
<p>The power of computation is the subject of computational complexity theory.In the early 1970’s, computational complexity generally dealt with the difficulty of decision problems, such as whether a graph has a perfect matching or whether a traveling salesman can find a route of at most a certain length. This difficulty was characterized by complexity classes, such as P (tractable problems) and NP (problems for which a solution can easily be checked once it has been produced, but perhaps not easily found in the first place). Of course, many important practical problems are not simply decision problems: instead of asking whether there is a route of at most 1000 miles, one could ask for the length of the shortest possible route. However, these more general problems can often be reduced to a sequence of decision problems, for example by using binary search to narrow in on the length of the shortest route.</p>
<p>One of Valiant’s most noteworthy discoveries is that counting problems are much more subtle than previous experience suggested. A counting problem asks for the number of some combinatorial objects: for example, how many perfect matchings are there in a graph? Now we are not just asking the decision problem of whether that number is positive, but also how large it is. If the decision problem is difficult, then the counting problem must be as well, but Valiant’s surprising realization was that the converse fails. In his paper “<a href=""https://en.wikipedia.org/wiki/Permanent_is_sharp-P-complete"" target=""_blank"">The complexity of computing the permanent</a>” [<a href=""/bib/valiant_2612174.cfm#bib_2"">2</a>], he showed that although there is an efficient algorithm to tell whether a graph has a perfect matching, counting perfect matchings is as hard as any counting problem, including for any NP-complete problem. This came as a shock to the computational complexity community, which had grown accustomed to the idea that decision problems would easily capture the key features of a problem. Instead, Valiant extended the theory of complexity classes to include the new counting class <a href=""https://en.wikipedia.org/wiki/Sharp_P"" target=""_blank"">#P</a>.</p>
<p>If counting problems were limited to esoteric mathematical problems, Valiant’s theory would still have been conceptually fascinating but it would have had limited impact. However, it turns out that these problems pervade much of computer science. For example, estimating any probability amounts to an approximate counting problem, and random sampling is closely related. Approximate counting and sampling are both important goals in their own right (for example, in statistics) and valuable tools for solving other problems; entire subfields of computer science, such as Markov Chain Monte Carlo algorithms, are by now devoted to these topics. The ideas Valiant introduced have grown into a thriving field, which has united computer science, probability and statistics, combinatorics, and even aspects of statistical physics.</p>
<p>In the area of artificial computation, his work has analyzed of how to control vary large parallel computing systems. These range from the multi-core processors that are commonly found in personal computers to large, widely distributed, clusters of computing machines. He centered his work on investigating the role played by communication between parallel processors. In 1990 he published a paper [<a href=""/bib/valiant_2612174.cfm#bib_3"">3</a>] describing his <a href=""https://en.wikipedia.org/wiki/Bulk_synchronous_parallel"" target=""_blank"">bulk synchronous parallel</a> (BSP) model. This model has had wide influence on the way parallel computation is conceptualized and carried out.</p>
<p>In describing Valiant’s contributions, the ACM Turing Award Committee pointed out: &nbsp;</p>
<div class=""callout2"" style=""margin-left: 0.5in; ""><strong>Rarely does one see such a striking combination of depth and breadth as in Valiant’s work. His is truly a heroic figure in theoretical computer science and a role model for&nbsp; his courage and creativity in addressing some of the deepest unsolved problems in science.</strong></div>","<div class=""featured-photo"">
<a href=""/award_winners/valiant_2612174.cfm""><img src=""/images/lg_aw/2612174.jpg"" class=""img-rounded"" width=""219"" height=""208"" alt=""Leslie G Valiant""></a>
</div>
<h6 class=""label""><strong>BIRTH:</strong></h6>
<p>28 March 1949, Budapest, Hungary</p>
<h6 class=""label""><strong>EDUCATION:</strong></h6>
<p>Latymer Upper School, London England; King’s College, Cambridge, England (BA, Mathematics, 1970); Imperial College, London, England (DIC in Computing Science); University of Warwick, England (PhD, Computer Science, 1974)</p>
<h6 class=""label""><strong>EXPERIENCE:</strong></h6>
<p>Carnegie Mellon University (Visiting Assistant Professor, 1973-1974); Leeds University, England (Lecturer, 1974-1976); University of Edinburgh, Scotland (Lecturer, later Reader, 1977-1982); Harvard University (Gordon McKay Professor of Computer Science and Applied Mathematics, from 1982, T. Jefferson Coolidge Professor of Computer Science and Applied Mathematics (2001 forward).</p>
<h6 class=""label""><strong>HONORS &amp; AWARDS:</strong></h6>
<p>International Mathematical Union Nevanlinna Prize (1986);&nbsp;<span style=""line-height: 20.8px;"">Fellow of the Royal Society (1991);</span><span style=""line-height: 20.8px;"">&nbsp;</span><span style=""line-height: 1.3;"">ACM Special Interest Group on Algorithms and Computation Theory and the IEEE Technical Committee on the Mathematical Foundations of Computation Theory Knuth Prize (1997);&nbsp;</span><span style=""line-height: 20.8px;"">member of the USA National Academy of Sciences (2001);&nbsp;</span><span style=""line-height: 1.3;"">European Association for Theoretical Computer Science Distinguished Achievement Award (2008); Turing Award (2010); Fellow of the American Association for artificial Intelligence.&nbsp;</span></p>","","https://dl.acm.org/author_page.cfm?id=81100502250","Leslie Gabriel Valiant","<li class=""bibliography""><a href=""/bib/valiant_2612174.cfm""><span></span>Short Annotated<br> Bibliography</a></li>
<li class=""award-video""><a href=""/vp/valiant_2612174.cfm""><span></span>ACM Turing Award<br> Lecture Video</a></li>
<li class=""key-words""><a href=""/keywords/valiant_2612174.cfm""><span></span>Research<br> Subjects</a></li>
<li class=""additional""><a href=""/info/valiant_2612174.cfm""><span></span>Additional<br> Materials</a></li>
<li class=""additional""><a href=""/interviews/valiant_2612174.cfm""><span></span>Video Interview</a></li>"
